[
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "Added\n\nMigrated website framework from Squarespace to Quarto\nImplemented responsive design with light/dark theme support\nCreated spotlight section (MIT-inspired) for homepage showcase\nSet up GitHub Pages hosting infrastructure with automated deployment\nAdded modern CSS with smooth transitions and animations\nCreated Works portfolio with hierarchical structure (Experiences, Projects, Consulting, Maps & Infographics)\nAdded placeholder pages for all main content sections\nImplemented GitHub Actions workflow for automatic deployment\nCreated comprehensive documentation suite\n\nChanged\n\nRenamed image folder from hero-images to spotlight (MIT-inspired naming)\nRestructured content organization for better maintainability\nImproved page load performance with static site generation\nEnhanced mobile responsiveness across all breakpoints\nUpdated navigation with dropdown menu for Works section\n\nTechnical\n\nFramework: Quarto\nHosting: GitHub Pages (testing at /site/ subdirectory)\nRepository: github.com/bennyistanto/site\nThemes: Custom light (Cosmo-based) and dark (Darkly-based) variants\nPrimary Color: Orange (#D35400 light, #E67E22 dark)\nDeployment: Automated via GitHub Actions\nCustom Domain: Configured for future migration to benny.istan.to\n\nDocumentation\n\nQUICK-START.md - Quick start guide\nDEPLOYMENT.md - Testing and deployment instructions\nFINAL-MIGRATION.md - Production migration guide\nMIGRATION-SUMMARY.md - Complete migration overview\nSETUP.md - Detailed setup instructions\nREADME.md - Project overview\n\nMigration Status\n\n‚úÖ Framework setup complete\n‚úÖ Placeholder pages created\n‚úÖ Works portfolio structure ready\n‚úÖ Deployment workflow configured\n‚è≥ Asset migration pending (logo, favicon, spotlight images)\n‚è≥ Content migration in progress\nüîú Final deployment to benny.istan.to (when ready)"
  },
  {
    "objectID": "changelog.html#version-history",
    "href": "changelog.html#version-history",
    "title": "Changelog",
    "section": "",
    "text": "Added\n\nMigrated website framework from Squarespace to Quarto\nImplemented responsive design with light/dark theme support\nCreated spotlight section (MIT-inspired) for homepage showcase\nSet up GitHub Pages hosting infrastructure with automated deployment\nAdded modern CSS with smooth transitions and animations\nCreated Works portfolio with hierarchical structure (Experiences, Projects, Consulting, Maps & Infographics)\nAdded placeholder pages for all main content sections\nImplemented GitHub Actions workflow for automatic deployment\nCreated comprehensive documentation suite\n\nChanged\n\nRenamed image folder from hero-images to spotlight (MIT-inspired naming)\nRestructured content organization for better maintainability\nImproved page load performance with static site generation\nEnhanced mobile responsiveness across all breakpoints\nUpdated navigation with dropdown menu for Works section\n\nTechnical\n\nFramework: Quarto\nHosting: GitHub Pages (testing at /site/ subdirectory)\nRepository: github.com/bennyistanto/site\nThemes: Custom light (Cosmo-based) and dark (Darkly-based) variants\nPrimary Color: Orange (#D35400 light, #E67E22 dark)\nDeployment: Automated via GitHub Actions\nCustom Domain: Configured for future migration to benny.istan.to\n\nDocumentation\n\nQUICK-START.md - Quick start guide\nDEPLOYMENT.md - Testing and deployment instructions\nFINAL-MIGRATION.md - Production migration guide\nMIGRATION-SUMMARY.md - Complete migration overview\nSETUP.md - Detailed setup instructions\nREADME.md - Project overview\n\nMigration Status\n\n‚úÖ Framework setup complete\n‚úÖ Placeholder pages created\n‚úÖ Works portfolio structure ready\n‚úÖ Deployment workflow configured\n‚è≥ Asset migration pending (logo, favicon, spotlight images)\n‚è≥ Content migration in progress\nüîú Final deployment to benny.istan.to (when ready)"
  },
  {
    "objectID": "changelog.html#about-this-changelog",
    "href": "changelog.html#about-this-changelog",
    "title": "Changelog",
    "section": "About This Changelog",
    "text": "About This Changelog\nThis changelog follows semantic versioning principles and documents all notable changes to the website. Updates are organized by date and categorized as:\n\nAdded: New features or content\nChanged: Updates to existing functionality\nFixed: Bug fixes\nRemoved: Deprecated features\nTechnical: Infrastructure or development changes\n\nFor detailed commit history, visit the GitHub repository."
  },
  {
    "objectID": "blog/20130911-menginstall-gdalogr-for-python-di-windows.html",
    "href": "blog/20130911-menginstall-gdalogr-for-python-di-windows.html",
    "title": "Menginstall ‚ÄúGDAL/OGR for Python‚Äù di Windows",
    "section": "",
    "text": "Beberapa bulan terakhir saya banyak menggunakan GDAL/OGR untuk melakukan clip dan translate data satelit. Sebelumnya saya menggunakan GDAL/OGR yang sudah terintegrasi di dalam QGIS plugins (FW tools).\nTernyata menggunakan GDAL/OGR secara langsung juga mudah dan jauh lebih enak, apalagi ketika saya hanya membutuhkan operasi clip untuk memotong data satelit menjadi wilayah Indonesia saja, dan translate untuk mengubah format HDF menjadi format GeoTIFF.\nDengan menggunakan 1-baris perintah GDAL/OGR, proses clip dan translate untuk 1000 file bisa dilakukan dengan cepat dan hanya memerlukan waktu beberapa detik saja, tentunya syarat dan ketentuan berlaku (tergantung ukuran filenya, shapefile yang digunakan untuk memotong, luasan wilayah, dll).\nTetapi banyak orang enggan menggunakan GDAL/OGR secara langsung, ini disebabkan oleh beberapa faktor seperti:\nSetelah mencoba beberapa metode, saya menemukan cara mudah menginstall software GDAL/OGR terbaru di sistem operasi Windows. (Untuk sistem operasi lain, tidak perlu saya beri contoh, karena jauh lebih mudah dibanding Windows).\nUntuk melakukan instalasi ini, kamu harus mempunyai akses sebagai Administrator di Windows."
  },
  {
    "objectID": "blog/20130911-menginstall-gdalogr-for-python-di-windows.html#prosedur-instalasi",
    "href": "blog/20130911-menginstall-gdalogr-for-python-di-windows.html#prosedur-instalasi",
    "title": "Menginstall ‚ÄúGDAL/OGR for Python‚Äù di Windows",
    "section": "Prosedur instalasi:",
    "text": "Prosedur instalasi:\n\nMenentukan versi Python yang diinginkan\nMendapatkan file instalasi GDAL\nMendapatkan Python bindings\nMenginstal GDAL\nMengedit Environment Variable pada Windows\nMenginstal Python bindings\nInformasi dan solusi untuk instalasi GDAL yang sudah ada\nMasalah umum\n\n\n1. Menentukan versi Python yang diinginkan\nJika kamu menginstal Python secara manual, itu bisa berada di mana saja lokasi tempat menginstalnya, tetapi lokasi default-nya adalah seperti C:, di mana XX adalah versi utama; misalnya, 27 adalah versi utama Python 2.7.5. (Versi 2.7.5 adalah versi python yang terinstall di Windows saya dan lokasinya berada di C:)\nJika di komputer kita telah terinstall ArcGIS misalnya, ArcGIS 10.2 biasanya menginstal Python 2.7 ke C:, meskipun sebenarnya dapat juga diinstal ke drive lokal lain seperti D:.\nJika kamu telah menginstal Background Geoprocessing 64-bit (dijelaskan di sini), maka itu akan menginstal Python tambahan, dengan lokasi default C:\n\nKamu juga dapat mencoba mencari python.exe di hard-drive kamu, tetapi perlu diketahui bahwa beberapa aplikasi mungkin berisi instalasi internal Python mereka sendiri (yang mungkin tidak cocok/ingin kamu gunakan).\nSetelah kamu mengetahui Python mana yang akan kamu gunakan, buka interpreter Python dengan cara biasa (atau klik dua kali pada python.exe-nya). Ini akan mencetak informasi tentang versi di atas ketika terbuka, misalnya:\nPython 2.7.5 (default, 15 May 2013, 08:10:18) [MSC v.1500 32 bit (Intel)] on win32\nInformasi yang penting dari versi Python di atas: versi di komputer saya adalah Python 2.7 (informasi tambahan .5 tidak penting) dan 32 bit.\n\n\n2. Mendapatkan file instalasi GDAL\nSitus ini: https://www.gisinternals.com/release.php adalah tempat dimana kamu dapat mengunduh file instalasi GDAL. Gulir ke bawah ke bagian bawah tabel dan klik tautan yang relevan di ‚ÄôDownloads‚Äô - pada kolom Arch. kolom menunjukkan apakah 32 bit (win32) atau 64 bit (x64), pilih salah satu yang sesuai dengan Python kamu. Misal: release-1500-gdal-1-10-1-mapserver-6-4-0\nDari halaman baru, download file GDAL Core: gdal-[version]-[build]-core.msi (Contoh: gdal-110-1500-core.msi)\n\n\n3. Mendapatkan Python Bindings\nUntungnya, Python bindings juga tersedia di sini, nama filenya GDAL-[gdalVersion.architecture]-py[pythonVersion].[exe/msi] (misalnya GDAL-1.10.1.win32-py2.7.msi), pastikan saja untuk mengunduh versi yang cocok dengan versi Python yang kamu instal.\n\n\n4. Menginstall GDAL\nJalankan file instalasi GDAL Core yang diunduh pada langkah 2, dan ingat/catat lokasi folder instalasi jika kamu mengubahnya (tidak sesuai dengan default).\n\n\n5. Edit Environmental Variables\n\nPERINGATAN\nJangan membuat kesalahan apa pun disini (seperti menghapus sesuatu), atau akan berakibat sistem kamu menjadi tidak berfungsi‚Ä¶\n\nUntuk memunculkan kotak dialog Environmental Variables:\n\nBuka Control Panel\nMasuk ke System\nKlik pada Advanced di sebelah kiri\nLalu klik Environmental Variables dibagian bawah\n\nPertama, kamu perlu mengidentifikasi apakah akan ada konflik dengan versi GDAL yang mungkin telah diinstal sebelumnya. Sebagai contoh, saya akan memodifikasi dua Environmental Variables, yang satu disebut GDAL_DATA dan yang lainnya sistem Path.\nUntuk memeriksa versi GDAL sebelumnya:\n\nGulir ke bawah di panel System Variables (di agian bawah) dan cari variabel yang disebut GDAL_DATA\nGulir ke bawah lebih jauh di panel System Variables dan temukan variabel Path, klik Edit dan salin konten ke editor teks (Notepad atau Wordpad), lalu cek isinya dengan cermat dan lihat apakah ada referensi ke GDAL.\nPeriksa panel variabel User untuk hal yang sama (kecil kemungkinannya ada di sini, tetapi perlu diperiksa)\n\nJika GDAL_DATA tidak ada dan tidak ada referensi ke GDAL di variabel Path, seharusnya tidak ada konflik dan kamu dapat melanjutkan dengan yang di bawah ini. Jika tidak, periksa informasi di langkah 7 sebelum melanjutkan.\nDi panel System Variables, gulir kotak ke bawah, pilih variabel Path, lalu ke Edit, tekan End untuk mendapatkan kursor di bagian akhir, tambahkan titik koma (;) dan kemudian masukkan path instalasi GDAL kamu. Sebagai contoh dalam instalasi di komputer saya, saya menambahkan ini:\n;C:Files (x86)\n\nJika sistem operasi kamu menggunakan 64 bit, dan kamu juga menggunakan file instalasi GDAL 64 bit, maka yang harus kamu tambahkan adalah:\n;C:Files\nPastikan untuk tidak memasukkan spasi tambahan sebelum atau sesudah path!\n\nKlik OK, lalu klik New dan masukkan yang berikut ini:\nNama variabel: GDAL_DATA\nNilai variabel adalah path ke direktori data GDAL. Sebagai contoh dalam instalasi di komputer saya, saya menambahkan ini:\nC:Files (x86)-data\n\nJika sistem operasi kamu menggunakan 64 bit, dan kamu juga menggunakan file instalasi GDAL 64 bit, maka yang harus kamu tambahkan adalah:\nC:Files-data\n\nUntuk menguji hasil instalasi, buka command prompt kamu (ketik cmd di dialog Run), ketik ogr2ogr dan tekan Enter. Ini adalah salah satu program yang disertakan dengan GDAL; jika Path kamu sudah ditulis dengan benar, itu akan dapat diakses dari mana saja.\nJika beberapa informasi tentang penggunaan ogr2ogr dan opsinya tercetak di layar (lihat gambar di bawah), maka GDAL sudah terpasang dengan benar dan Environmental Variables berfungsi.\n\nJika sebaliknya dikatakan: ‚Äòogr2ogr‚Äô is not recognized as an internal or external command, operable program or batch file., Berarti ada masalah. Pertama, coba restart komputer kemudian jalankan perintah ogr2ogr lagi, jika ogr2ogr masih tidak berfungsi, periksa Environmental Variables kamu untuk memastikan tidak ada kesalahan ejaan atau kesalahan ketik.\n\n\n6. Menginstall Python bindings\nJalankan file instalasi Python bindings yang diunduh pada langkah 2. Jika kamu memiliki banyak instalasi Python atau pengaturan non-standar, pastikan untuk memeriksa ke versi Python mana ini akan terinstall.\nSetelah instalasi dilakukan dan untuk menguji seluruh pengaturan, buka prompt Python yang relevan dan masukkan:\nimport ogr\nTekan Enter, lalu:\nimport gdal\nJika keduanya berjalan tanpa ada respon apa pun, berarti kamu telah berhasil menginstall GDAL/OGR! (lihat gambar di bawah ini)\n\n\n\n7. Masalah/solusi untuk instalasi GDAL yang sudah ada\nJika program/aplikasi lain telah menginstall GDAL pada komputermu, ini mungkin dapat menyebabkan masalah jika kamu mengganti variabel Path dan GDAL_DATA karena versi library nya mungkin tidak cocok.\nIdentifikasi dan buatlah list terkait program/aplikasi mana saja yang menggunakan atau menginstall GDAL, dan pikirkan apakah kamu masih memerlukannya. Idealnya kamu dapat menghapus versi yang ada dan melanjutkan hanya dengan satu versi, jika kamu melakukan ini pastikan untuk membersihkan variabel Path dan GDAL_DATA sehingga mengarah ke lokasi yang benar dan lanjutkan dengan langkah 5 dan 6 di atas. Sebagai alternatif, kamu mungkin bisa mendapatkan Python bindings untuk versi yang telah diinstal, dan mengizinkan Python untuk mengaksesnya.\n\n\n8. Masalah umum\nSaat mencoba melakukan ‚Äúimport gdal‚Äù atau ‚Äúimport ogr‚Äù pada prompt Python, pesan kesalahan seperti ImportError: DLL load failed: The specified module could not be found. adalah paling sering disebabkan oleh kesalahan saat menambahkan folder pemasangan GDAL ke variabel sistem Path - ketika menemukan kesalahan ini, kembali ke langkah 5 dan cek dengan teliti."
  },
  {
    "objectID": "blog/20040522-pendugaan-deret-hari-kering.html",
    "href": "blog/20040522-pendugaan-deret-hari-kering.html",
    "title": "Pendugaan deret hari kering",
    "section": "",
    "text": "Makalah berikut merupakan tugas dari mata kuliah Klimatologi Pertanian di semester 6, dibimbing oleh Rizaldi Boer."
  },
  {
    "objectID": "blog/20040522-pendugaan-deret-hari-kering.html#penentuan-musim-tanam-berdasarkan-kejadian-deret-hari-kering",
    "href": "blog/20040522-pendugaan-deret-hari-kering.html#penentuan-musim-tanam-berdasarkan-kejadian-deret-hari-kering",
    "title": "Pendugaan deret hari kering",
    "section": "Penentuan musim tanam berdasarkan kejadian deret hari kering",
    "text": "Penentuan musim tanam berdasarkan kejadian deret hari kering\nKejadian deret hari kering merupakan salah satu indikator yang dapat digunakan untuk mengetahui apakah suatu tanaman mengalami cekaman kekeringan atau tidak (McCaskill dan Kariada, 1992; Niewolt,1989). Untuk mendukung produksi tanaman yang baik, diusahakan agar syarat-syarat yang dibutuhkan tanaman terpenuhi. Salah satu syarat yang dibutuhkan tanaman adalah cuaca dan iklim. Curah hujan sebagai salah satu unsur iklim sangat besar peranannya dalam mendukung ketersediaan air.\nKegagalan panen sering disebabkan karena pembagian curah hujan di daerah tersebut tidak merata sehingga pada waktu tanaman betul-betul sedang membutuhkan air, hujan tidak ada. Untuk mengatasi keadaan ini, diperlukan analisis data hujan dalam menentukan musim tanam tanaman agar fase kritis tidak jatuh pada waktu curah hujan kurang.\nTujuan\n\nMenentukan peluang terjadinya deret hari kering (DHK) pada masing-masing alternative waktu tanam.\nMenentukan waktu tanam optimum dengan menggunakan nilai rata-rata peluang kejadian deret hari kering (DHK).\n\nData\n\nData curah hujan bulanan rata-rata Kabupaten Kebumen.\n\nMetode\n\nMengkelompokkan beberapa wilayah berdasarkan pola dan tinggi hujan rata-rata dari hasil analisis gerombol dengan algoritma centroid.\nMencari nilai p(DHK ‚â• 10) dan p(DHK ‚â• 15) untuk masing-masing kelompok.\nMemilih peluang kritis yang digunakan sebagai acuan dalam penentuan waktu tanam.\nMentukan waktu tanam optimal dari suatu daerah.\n\nHasil dan Pembahasan\nAnalisis ini menggunakan data curah hujan Kabupaten Kebumen yang terdiri dari 24 wilayah. Berdasarkan hasil analisis gerombol dengan algoritma centroid diperoleh dendogram dan mengelompokkan wilayah tersebut, menjadi 6.\n\nAnalisis yang dilakukan yang dilakukan selanjutnya pada praktikum ini diasumsikan mempunyai nilai persamaan-persamaan yang sama dengan persamaan-persamaan dari panduan praktikum ‚ÄúPenentuan Waktu Tanam Tembakau di Tamanggung Jawa Tengah Berdasar Sifat Curah Hujan dan Model Regresi Akhir (Sumber : Supriyanto,1997).\nJika diketahui p(DHK‚â•10)= 1/[1+exp(-0.2688 + 0.00745X)] dan p(DHK ‚â• 15) = 1/[1+exp(0,22913+0,00831 X)] maka diperoleh:\n\nKarena peluang kritis yang digunakan adalah 0,2 artinya waktu penanaman harus diatur agar pada bulan dimana fase pertumbuhan sensitif terhadap kekeringan tercapai ‚â§ 0,2. Sehingga waktu tanam optimal adalah daerah yang diarsir pada tabel 3.\n\n\n\nCatatan:¬†Tabel¬†yang diarsir menunjukkan waktu tanam optimum (peluang kekeringan tercapai ‚â§ 0,2)\n\n\nCatatan:¬†Tabel¬†yang diarsir menunjukkan waktu tanam optimum (peluang kekeringan tercapai ‚â§ 0,2)\nBerdasarkan tabel 3, waktu tanam tembakau optimum di Kebumen rata-rata pada bulan Oktober- Mei kecuali ada beberapa yang tidak optimum yaitu pada kel.4 di bulan mei, kel.5 di bulan Pebruari, April, dan Mei.\nDengan waktu tanam yang berbeda dan selang 10 hari (diambil contoh mulai tanggal 1 Januari ‚Äì 30 Mei) maka nilai rata-rata peluang kejadian deret hari kering yang merupakan rataan dari keenam kelompok. Maka diperoleh hasil seperti:\n\nJika diketahui bahwa umur tanaman tembakau pada fase vegetatif awal 20 hari, fase vegetatif cepat 40 hari, dan fase pemasakan 30 hari. Tembakau ditanam selang 10 hari berdasar julian day, dan sebaran curah hujan menjadi dasarian dengan perbandingan 5:3:2 maka dapat diperoleh sebaran curah hujan menurut fase tumbuhnya (hasil tidak dilampirkan).\nDari hasil perhitungan tersebut, diambil contoh kelompok 1. Berdasarkan kondisi kejadian deret hari kering yang diinginkan oleh masing-masing fase tumbuh, maka nilai peluang dapat diperoleh (Tabel 5).\nSetelah diketahui nilai peluang DHK masing-masing fase, maka akan didapatkan nilai nilai rata-rata peluang kejadian DHK (Prataan), yang merupakan acuan penentuan waktu tanam. Dengan persamaan\n\\[P_{\\text{rerata}} = 0.051 \\cdot p(DHK \\leq 10) + 0.041 \\cdot p(DHK \\geq 15) + 0.97 \\cdot p(DHK \\geq 10)\\]\n\nNilai peluang rataan menentukan waktu tanam. Waktu tanam paling optimum adalah saat peluang rataannya terbesar. Pada tabel 5 menunjukkan bahwa waktu tanam optimum dari 6 kelommpok adalah pada tanggal 1 Mei, karena memiliki peluang terbesar yaitu 0,54. Ini berarti potensi produksi optimal tanaman tembakau akan lebih besar jika ditanam pada tanggal 1 Mei untuk daerah yang termasuk dalam kelomppok 1 yaitu Kedungringin dan Sadang. Dengan catatan kualitas tembakau belum diperhitungkan.\nKeragaman produksi (Y) ditentukan oleh panjang deret hari kering maksimum fase vegetatif awal (X1), P (DHK ‚â• 15) fase vegetatif cepat (X2) dan P (DHK ‚â• 10) fase pemasakan, dengan persamaan:\n\\[Y = 0.407 - 0.00259 X_1 - 0.009X_2 + 0.0426X_3\\] maka diperoleh hasil:\n\nDari perhitungan yang telah dilakukan nilai Y terbesar adalah 0,42. Pada saat Prerata maksimum, nilai Y juga 0,42 meskipun nilai Y tidak terlalu beragam. Mulai tanggal 10 April ‚Äì 20 Mei waktu penananam, nilai Y = 0,42 tatapi nilai Prerata tanggal 1 Mei yang paling tinggi. Nilai peluang terjadinya deret hari kering ‚â• 10 (X3) pada tanggal ini juga mempunyai nilai paling tinggi yaitu 0,48.\nHal ini menunjukkan pada tanggal 1 Mei di daerah kelompok satu, bila ditanam tanaman tembakau akan akan memperoleh keragaman produksi sebesar 0,42. Ini berarti saat masa tanam optimum maka keragaman prosuksi juga akan tinggi.\nKesimpulan\nPada analisis gerombol diperoleh bahwa jumlah gerombol untuk Kebumen adalah 6 kelompok.\nDari analisis peluang deret hari kering untuk kelompok 1 (daerah Kedungringin dan Sadang) menunjukkan waktu yang paling optimum untuk menanam tembakau adalah pada tanggal 1 Mei karena memiliki peluang rataan terbesar yaitu 0,54 dan keragaman produksi 0,42.\nPustaka\nBoer, R et al.1996. Penentuan Waktu Tanam Tembakau di Temanggung Jawa Tengah Berdasarkan Sifat Hujan dan Model Regresi Hasil.\nRachmat, Marsudi. 1998. Analisis Data Hujan Untuk Menentukan Musim Tanam Padi dan Palawija Pada Lahan Tadah Hujan. Jurusan Geofisika dan Meteorologi. FAMIPA. IPB. Bogor."
  },
  {
    "objectID": "blog/20190727-technical-engagement-workshop-in-beijing.html",
    "href": "blog/20190727-technical-engagement-workshop-in-beijing.html",
    "title": "Technical engagement workshop in Beijing",
    "section": "",
    "text": "Last week I wen to Beijing for a technical engagement workshop on emergency preparedness and response at National Disaster Reduction Centre of China (NDRCC) office. The purpose of this workshop is:\n\nTo build stronger capacity for both NDRCC and WFP itself on the data and technology for emergency preparedness and response The potential use cases may fall into analysis and tools to detect risks, produce alerts and assess humanitarian impact; collaboration for information sharing at technical levels (satellites, GIS, Early Warning, monitoring) and building systems for preparedness and response, with specific focus on natural hazards.\nTo build strong capacity for other countries, through leveraging China‚Äôs knowledge, expertise and experience in emergency management. The areas include, but are not limited to, disaster risk reduction, rapid impact assessments. This is in line with South-South cooperation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the workshop, we go around enjoying the city of Beijing, trying local food at Huguosi St - Xicheng District, walked in Houhai Park, take the metro during peak hours from Tiananmen Square station, and finally visit Great Wall on my way to airport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20091121-webmap-interface.html",
    "href": "blog/20091121-webmap-interface.html",
    "title": "Web Map Interface",
    "section": "",
    "text": "This training was part of ADB‚Äôs ETESP Package 39 Part 2 project and was carried out last month, which was attended by staff from Aceh‚Äôs Forestry and Plantation Office.\nThe training course is built around the concept of data severs, map servers and application servers and the open source software required at each tier to establish a scalable architecture capable of delivering spatial web services and web applications.\nOutcomes\nParticipants will be up to date with the latest geospatial data standards and exchange formats which enable interoperability.\nOn completion of the training course, participants will have developed a map viewer framework using open source software which distributes spatial web services and web mapping applications.\nLength\nThe length of the course is approximately 5 days\nTopics\nAn overview of the training course material is summarised below;\nModule 1, focuses on spatial web services, the standards, protocols and data exchange formats as well as practical applications including the creation, debugging and accessing of spatial web services.\nMS4W\n\nIntroduction to Map Server 4 Windows (MS4W)\nMap Server 4 Windows installation\n\nWMS\n\nIntroduction to the Web Mapping Service (WMS) standard\nMapServer, creating a WMS data service, debugging and troubleshooting\nWMS and uDig\n\nDownload tools and materials\n\nData (Download here)\nMS4W (http://www.maptools.org/ms4w/index.phtml?page=downloads.html), symbol (Download here) and fonts (Download here)\nFirefox (http://www.mozilla.com/en-US/firefox/fx/)\nNotepad++ (http://notepad-plus-plus.org/download/v5.9.3.html)\nuDig (http://udig.refractions.net/download/)\n\nModule 2, introduces the basic technologies that can be used to begin creating web applications, including client side javascript libraries and data formats.\nHTML and CSS\n\nIntroduction to Hyper-Text Markup Language (HTML)\nIntroduction to Cascading Style Sheets (CSS)\nCreating a web site\n\nJavaScript\n\nIntroduction to JavaScript\nUsing JavaScript to create Dynamic HTML (DHTML)\n\nExt JS\n\nIntroduction to Ext JS and the Ext JS API\nIncluding Ext JS in your website\n\nDownload tools and materials\n\nFirebug (http://getfirebug.com/downloads)\nExt JS (http://www.sencha.com/products/extjs/download/)\nimgs (Download here)\nReference (Download here)\n\nModule 3, introduces OpenLayers client side JavaScript libraries that are used to display web services and vector data formats, to enable the creation of dynamic maps in web applications.\nOpenLayers\n\nIntroduction to OpenLayers and the OpenLayers API\nA basic web map\nVector Styles and Formats in Open Layers (KML, GML, WKT)\n\nDownload tools and materials\n\nimgs (Download here)\nMapFish (http://mapfish.org/downloads/)\nOpenLayers (http://openlayers.org/download/)\n\nModule 4, investigates spatial database systems, in particular the object-relational PostgreSQL with PostGIS spatial extension including database and table management, loading SHP files, basic and spatial queries and database formats\nPostGIS\n\nOverview of PostGIS\nLoading SHP files into PostGIS\n\nDB Queries\n\nIntroduction to basic queries and results\nIntroduction to spatial queries and formats\nOpenLayers and PostgreSQL/PostGIS\n\nDownload tools and materials\n\nPostGIS (http://postgis.refractions.net/download/)\nPostgreSQL (http://www.postgresql.org/download/)\n\nModule 5, database access, spatial web services and JavaScript combine to produce a user friendly graphical web mapping application which enables geo-processing.\nWebApp\n\nCreating a toolbar and customised tools\nSpatial database processing from the web client\n\nAnnex A provides a background to geographic standards and software and the web sites that provide support and documentation.\nReference sites to geographic standards, software downloads and supporting documentation\nDownload Web Map Interface Training module here (52 MB)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210413-how-to-get-daily-rainfall-forecast-data-from-gfs-part1.html",
    "href": "blog/20210413-how-to-get-daily-rainfall-forecast-data-from-gfs-part1.html",
    "title": "How to get daily rainfall forecast data from GFS? (Part 1)",
    "section": "",
    "text": "Last week, Tropical Cyclone Seroja hit eastern part of Indonesia and Timor-Leste. It brought flood and landslide displaced more than 8,000 people and led to the deaths of 42 people in total in Timor-Leste. The storm also caused power outages in the entire country. In Indonesia, Seroja caused widespread rainfall and thunderstorms in Nusa Tenggara Barat and Nusa Tenggara Timur provinces, and trigger a flood and landslide in various areas.\nMany questions arise on that day: What will happen tomorrow? How is the weather will be in the afternoon? Where is Seroja now?\nCurrently, it is quite easy to get weather and forecast information from internet or mobile apps. Lot of 3rd party company provide the forecast based on public data for free. You named it: Ventusky, Windy, Zoom, DarkSky, the most popular and influential one is earth.nullschool.net, a famous project by Cameron Beccario.\n\nGIS-ready data\nSometimes you are looking for a daily rainfall forecast data in the area where the cyclone will pass. But you don‚Äôt understand on how to get the data and what data are available for public?\n\nGFS\nOne of the best data related to weather forecast is Global Forecast System (GFS) data from NOAA. Below are some detail about GFS:\n\nThe Global Forecast System (GFS) is a weather forecast model produced by the National Centers for Environmental Prediction (NCEP).\nThe GFS dataset consists of selected model outputs as gridded forecast variables. The 384-hour forecasts, with 3-hour forecast interval, are made at 6-hour temporal resolution (i.e.¬†updated four times daily).\nTEMPORAL COVERAGE: from 15 Jan 2015 to nowadays (new version of GFS)\nTEMPORAL RESOLUTION: 3h, 6h, 12h, 18h, 24h upto 16 days\nSPATIAL COVERAGE: 90¬∞ N ‚Äì 90¬∞ S\nSPATIAL RESOLUTION: 0,25¬∞ x 0,25¬∞\nVARIABLE: 743 variable are available, full list: https://www.nco.ncep.noaa.gov/pmb/products/gfs/gfs.t00z.pgrb2.0p25.f003.shtml\nFORMAT: GRIB2 (*.grib2), Reference: https://wmoomm.sharepoint.com/:b:/s/wmocpdb/EUmnLNAM9WdMr1S7GRMl_G8BFqp-B1Qie-k-vMwmrG22GQ?e=cEd2Vk\n\n\n\n\nHow to acquire the data?\nIn this case, I am interested to get 1 - 5 days Total Precipitation (APCP - code for total precipitation in GFS) data. There are several ways to get this data.\n\n1. GRIB filter\nGFS forecast data only available for the last 10-days and updated every 6-hours period. Accessible via this GRIB filter:https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p25.pl. If today is 13 April 2021, then GFS only available from 4 - 13 April 2021.\nWe will try to get forecast data issued on 13 April 2021, forecast hour 00.\n\nAccess GRIB filter link above: choose sub-directory gfs.20210413, 00, atmos\nIn combo-box, choose gfs.t00z.pgrb2.0p25.f006 (551063361); levels: surface; variables: APCP; sub-region: left longitude 94, right longitude 142, top latitude 7, bottom latitude -12.\nClick Start Download, you will get grb file with name: gfs.t00z.pgrb2.0p25.f006\n\n\n\nIf you choose option Show the URL only for web programming, you will get URL information: https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p25.pl?file=gfs.t00z.pgrb2.0p25.f006&lev_surface=on&var_ACPCP=on&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgfs.20210413%2F00%2Fatmos\nUsing above URL, we can modified and make automation script to download all data that we required.\n\n\n\nScript for batch download\nExample parameters\n\nForecast available every 6-hours, we will focus from f006 (start of data) until f120 (day-5)\nRelease hour (00, 06, 12, 18): geavg.t00z\nForecast hour (f006, f012, f018, f024, f030, f036, f042, f048, f054, f060, f066, f072, f078, f084, f090, f096, f102, f108, f114, f120): f006\nLevel: surface\nVariable: APCP\nSubset: Indonesia (94,142,7,-12)\nDate (YYYYMMDD): 20210413\nRelease hour (00, 06, 12, 18): 00\nScript: https://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f006&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\n\nDownload process\n\nEach time we run the download process procedure (Example: Date 3 Nov 2020, release hour 00), it means we need to download all the forecast hour from f006 to f120\n\nhttps://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f006&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\nhttps://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f012&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\nhttps://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f018&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.\nhttps://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f114&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\nhttps://nomads.ncep.noaa.gov/cgi-bin/filter_gefs_atmos_0p25s.pl?file=geavg.t00z.pgrb2s.0p25.f120&lev_surface=on&var_APCP=on&subregion=&leftlon=94&rightlon=142&toplat=7&bottomlat=-12&dir=%2Fgefs.20210413%2F00%2Fatmos%2Fpgrb2sp25\n\nWhen new forecast released (for example release hour: 06, 12 and 18), we need to repeat all download procedure above and adjust the script with current release hour.\n\n\n\n2. NCAR/UCAR Research Data Archive\nGFS data also available from NCAR/UCAR Research Data Archive portal: https://rda.ucar.edu/datasets/ds084.1/. If you don‚Äôt have account, you need to register in order to download the data.\n\nGo to Data Access tab, Get a Subset\nChoose temporal selection: 2021-01-01 00:00 to 2021-04-28 12:00\nParameter selection: Total Precipitation, Continue\nOutput format: I prefer to have data in netCDF\nIn the gridded product, I choose 24-hour, 48-hour, 72-hour, 96-hour and 120-hour Accumulation\nSpatial Selection: you can leave as it is, or choose within a bounding box. Continue with push Click here button\nIt‚Äôs optional to choose data compression option.\nSubmit request.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the data ready to download, you will get an email.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20120612-menggunakan-fungsi-arcpy-melalui-python-non-arcgis.html",
    "href": "blog/20120612-menggunakan-fungsi-arcpy-melalui-python-non-arcgis.html",
    "title": "Menggunakan fungsi ArcPy melalui Python non-ArcGIS",
    "section": "",
    "text": "ArcGIS menginstal versi Python-nya sendiri yang tidak terdaftar secara resmi di sistem Windows tetapi pada kenyataannya dapat digunakan seperti layaknya Python melalui instalasi normal. Saya pribadi lebih suka menggunakan ini sebagai Python utama saya, tetapi itu tidak selalu lancar karena kadang kita membutuhkan library yang mungkin tidak sesuai dengan versi yang ada di Python ArcGIS.\nPetunjuk di bawah ini dapat membantu kamu mengizinkan akses instalasi Python yang berbeda dan menggunakan Arcpy, misalnya Python yang diinstal oleh Python (x, y). Perhatikan bahwa:\n\nVersi utama dari Python harus sesuai (2.6 untuk ArcGIS 10.0 dan 2.7 untuk ArcGIS 10.1 dan setelahnya)\nArsitektur (32 bit atau 64 bit) harus sesuai\nJika kamu menulis skrip dan menggunakannya sebagai tools dalam lingkungan Arc, script tersebut akan menggunakan Python yang diinstal oleh Arc; ini berarti bahwa library apa pun yang telah kamu instal ke Python lain, tetapi tidak diinstal ke ArcGIS Python, tidak akan dapat diakses (menyebabkan ImportError)\n\nPython mencari library di folder tertentu (yaitu folder Lib-packages* dari instalasi), dan ketika kamu mengetik import somelibrary, itu akan mencari file dengan nama somelibrary.py* di folder ini. File Arcpy sebenarnya terletak di folder instalasi ArcGIS, di Program Files, tetapi terdapat file Path yang ditempatkan di folder Lib-packages* yang memberi tahu Python untuk menelusuri juga folder ArcGIS yang relevan untuk file saat kamu melakukan impor library*.\nUntuk mengizinkan instalasi Python lain mengakses Arcpy, file Path ini hanya perlu disalin dari folder Lib-packages* dalam instalasi Arc Python* (Saya menggunakan ArcGIS 10.2, maka lokasinya adalah sebagai berikut: C:-packages) dan ditempatkan di folder yang sesuai dari non-Arc Python (lokasinya adalah sebagai berikut: C:-packages). Jika kamu belum menginstal 64-bit background geoprocessing file tersebut adalah Desktop10.2.pth; jika kamu sudah menginstalnya, file tersebut adalah DTBGGP64.pth.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20110501-ilo-basic-operational-gis-for-road-assessment-part-2.html",
    "href": "blog/20110501-ilo-basic-operational-gis-for-road-assessment-part-2.html",
    "title": "ILO Basic Operational GIS for Road Assessment (Part 2)",
    "section": "",
    "text": "This the second part of 3 series post on ILO Basic Operational GIS for Road Assessment.\nYou can read first part here.\nObjective of the training course is to provide support to personnel working with GPS and GIS in road infrastructure in order strengthen the capacity for decision making and analysis through standardized data processing and dissemination of information.\nMethodology\nTraining: The training used Classroom and On-the-job training (OJT) method. Training was started by a general lecture on the methods and concepts of GIS, followed by a practical session and work in a case study. During the training, participants were expected to be active with any questions or problems previously experienced during the work.\nField Training: For each survey, there were two to three teams that carry out field survey. Each team consists of staff of the ILO, BAPPEDA, DBMCK and drivers. This survey updated the district roads data using the real-time tracking of GPS.\nTraining Participants\nMost of participants was the field supervisor of DBMKCK and BAPPEDA in Bireuen and Pidie.\nBireuen\n\nDBMCK (10 people)\nBAPPEDA (4 people)\nDPKKD (3 peoples)\nKPPTSP (3 people)\nDBMCK consultant (7 people)\nStudent (6 people)\n\nPidie\n\nDBMCK (13 people)\nBAPPEDA did not send any representative, as its staffs are already trained and familiar with the GPS. They confirm to participate in the upcoming training on GIS.\n\nPrerequisite of this training was to have a basic computer skill and comfortable working in the Windows environment, the open-source interface, have some Internet exposure, and have some experience working with data sets. Sending institutions were also encouraged to nominate participant from IT/Information management related division.\nBireuen\n\nDBMCK (8 people)\nBAPPEDA (4 people)\nStudent (1 person)\n\nPidie\n\nDBMCK (10 people)\nBAPPEDA (8 people)\nDPKKD (2 people)\n\nEach participant was asked to bring his or her own laptop, because the training was also including software installation process, and the committee provided the software. Software packages used in the training were FOSS and non open-source software and downloadable from internet i.e.¬†ArcGIS 9.3.1 student license (http://www.esri.com/software/arcgis/arcview/eval/evaluate.html), GPS Utility (http://www.gpsu.co.uk/download.html), Map Window (http://www.mapwindow.org) and DNR Garmin (http://www.dnr.state.mn.us/mis/gis/tools/arcview/extensions/DNRGarmin/DNRGarmin.html).\nSummary of the Training Activities\nTraining on the use of GPS and GIS application was carried out in April and May. The following materials (slide presentation) are provided in the training:\n\nIntroduction to Survey Instrument\nMap Reading and Navigation Techniques\nIntroduction to Navigation System Using GPS\nIntroduction of GPS Survey Method\nGPS Survey Planning and Preparation\nGPS Survey Data Collection\nGPS Survey Data Processing\nGPS and GIS\nField Training: The purpose of this field survey is for the preparation of Bireuen and or Pidie district road map that is valid and accurate for the legalization of the status of road. Implementation of field survey is conducted as a part of the GPS training. With the field survey, the participants are expected to acquire knowledge about data survey using GPS and producing district road network map.\nWhat is GIS\nIntroduction to ArcGIS\nWorking with external data source\n\nThe materials was developed as simple as possible in order to make sure the beginner can follow. First material is about the introduction to survey instrument, reading map and manual navigation technique. At the same time, participants are expected to share their experience in using maps and conducting survey by both manual and automatic equipment.\nNext material is introducing system of navigation by GPS and participants were guided to understand the modern concept of survey using satellite navigation in GPS. Also about complete method of GPS survey, including the preparation and planning, data collection, data processing and integration of GPS data to GIS (presenting survey data result of analysis by GIS). Main objective addressed in the module is to give an overview and guidance so participants would be able to prepare, plan and conduct a survey using GPS.\nLast part of the material is about GIS. The presentation discusses about the definition of GIS and geodata and Typical questions a GIS can answer. Introduce participants to the basic use of ArcGIS. ArcGIS is a large program with many extensions and many uses. We will concentrate on ArcMap and ArcCatalog. And the strength of GIS is that we can use many different information sources to bring together relevant information.\n4 ‚Äì 5 April 2011, Week 1: GPS\nBireuen;\nTraining was conducted in BAPPEDA with only 1 participant. The training was covering topics on Introduction to Survey Instrument, Map Reading and Navigation Techniques, and Introduction to Navigation System Using GPS.\nPidie:\nScheduled\n11 ‚Äì 14 April 2011, Week 2: GPS\nBireuen:\nTraining was participated by 25 people as follows: 5 DBMCK staffs, 3 BAPPEDA staffs, 3 DPKKD staffs, 3 KPPTSP staffs, 5 consultants in DBMCK, and 6 students from local university. The training was covering topics on Introduction to Survey Instrument, Map Reading and Navigation Techniques, Introduction to Navigation System Using GPS, Introduction of GPS Survey Method, GPS Survey Planning and Preparation, GPS Survey Data Collection, GPS Survey Data Processing, GPS and GIS. In addition to that basic theory related to GPS, participants also got a number of GPS modules on GPS terminology, Do We Need GPS? and GPS and Garmin POI. Participants were also introduced to various type of maps and read the map correctly.\n\nPidie:\nTraining was participated by 13 people and all were coming from DBMCK. The training delivered same module and presentation as the training in Bireuen.\n\n\n\n\n\n\n\n\n\n\n\n18 ‚Äì 21 April 2011, Week 3: GPS\nBireuen:\nTraining was participated by 21 people as follows: 9 DBMCK staffs, 2 BAPPEDA staffs, 1 DPKKD staff, 6 consultants in DBMCK and 3 students from local university. The module and presentation were covering topics on using GPS for field survey (complete module of GPS Garmin eTrex and 76CSx were distributed for guidance). Practical session in the area surrounding DBMCK involved the exercise to take waypoints and fill the information in GPS survey form. Additional information (if available) can be added to the form as well.\n\nFurther exercise was download and upload the data from GPS. This guidance is beneficial for participants who have GIS interface/facilities in their home institution, so they can further analyze the survey data. Learning outcomes from this part is that the participants would be able to connect the GPS to PC, configure GPS software, download and upload waypoints, tracks and routes. Complete module on data uploading and downloading in GPS was distributed.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú527‚Äù] Bireuen: GPS point plotted into a map [/caption]\nPidie:\nTraining was participated by 9 people and all were coming from DBMCK. The training delivered same module and presentation as the training in Bireuen.\n\n\n\n\n\n\n\n\n\n\n\nLast part of the training was on presenting GPS data in MapWindow - open source GIS software. This software has comprehensive features, available for free and open source in the internet. Participants were expected to be able to display GPS data on the map, edit the attributes data, use various plug-in in MapWindow that supports GPS application and basemap of Aceh was used for overlay and excercises. Complete module on using GIS-open source software for displaying GPS data was distributed.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú891‚Äù] Pidie: GPS point plotted into a map [/caption]\n25 ‚Äì 26 April 2011, Week 5: GPS Field Training\nBireuen:\nAfter getting the theory and practical session of the use of GPS, we then conducted the field survey. We split the the participants into two teams; first team consisted of 1 ILO staff, 1 BAPPEDA staff and 3 DBMCK staff with area survey in Kutablang, Makmur and Peusangan sub-district. The second team consisted of 1 ILO staff, 2 DBMCK staff and 2 BAPPEDA staff with area survey in Juli and Kota Juang sub-district. The following picture shows the sub-district area in Juli, which had been surveyed by the second team. The red line shows the existing road network, the yellow point shows the point of road section identifier, and the green line shows the track which is traversed by a survey team that can later be converted into a road map.\n \nPidie:\nScheduled\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20110428-ilo-basic-operational-gis-for-road-assessment-part-1.html",
    "href": "blog/20110428-ilo-basic-operational-gis-for-road-assessment-part-1.html",
    "title": "ILO Basic Operational GIS for Road Assessment (Part 1)",
    "section": "",
    "text": "This the first part of 3 series post on ILO Basic Operational GIS for Road Assessment.\nAfter spent 2 months to do an assessment of the currently available GIS system in 2 districts (Pidie and Bireuen), I would like to start writing a technical documentation on how to use a GPS and GIS for road survey and monitoring and prepare some supporting material.\nBelow is the outline:\n\nGPS Training\nThis document contains information about the material that was given to participants in the GPS training.\nSlide presentation\nThe most important element during the presentation was that they understand: Survey Instrument, Navigation Concept, How the GPS system works, the importance of doing this measurement for the survey (important to be careful to take the measurement), The basic of the Garmin GPS receiver. It is important that the participants play with the GPS receiver trying themselves to follow the steps mentioned in presentation slide. This allowed them to be more familiar with the device and the data collection protocol. Some slide presentations that have been made are the following:\n\nIntroduction to Survey Instrument\nMap Reading and Navigation Technique\nMap Reading and Navigation Technique material are equipped with several examples of maps with a completely different layout to give understanding to the training participants that maps as the output of the GIS, created for different purposes by different presentation as well. Some examples of these maps include:\n\nContour Map - Semeru, Indonesia\nDisaster Situation Map ‚Äì Merapi, Indonesia\nForest Landuse Map ‚Äì Aceh, Indonesia\nGeology Map, Aceh - Indonesia\nHumanitarian Situation Map - Libya\nIDP - West Sumatra, Indonesia\nReference Map - Indonesia\nTopographic AMS Map ‚Äì Lhokseumawe, Indonesia\nTopographic Map - Indonesia\n\nIntroduction to Navigation System using GPS\nIntroduction to GPS Survey Method, Survey Planning and Preparation, Data Collection, and Data Processing.\n\nAt the end of the lecture of the GPS survey method the participants was expected to be able to test the GPS device and make a practical exercise. This practical part was essential and certainly the most important part of the training.\nAt the end of this practical exercise, the participants should be able to take the measurement of the latitude and longitude of any landmark (road section position registration) within the district and enter the correct information in the survey form.\nModule\n\n76csx Quick Guide: Handheld Garmin GPS 76csx is a common type of GPS is owned by government officials in Bireuen and Pidie.\neTrex Quick Guide: Handheld Garmin GPS eTrex H is suitable for the beginner, as it is easy to use, small, lightweight, low cost and able to make quick and reliable satellite fixes.\nDownload and Upload GPS: The module discusses about the installation of GPS software, Connecting GPS with PC, Configure GPS software, Download waypoints-track- route, Upload waypoints-tracks-routes, Save a file from the GPS. DNR Garmin and GPS Utility software used in this module.\nDisplay GPS Data using GIS OpenSource Software: The module discusses about the installation of GIS software, Displays GPS Data in MapWindow, Viewing attribute information, Using a plug-in MapWindow. MAPWindow is software used in this module.\nGPS and Google Earth: Google Earth is a software that is quite popular today, everyone can download it for free. Google Earth is also can be used together with GPS. This module will discuss how to Importing data from a GPS device and Viewing GPS track in Google Earth\n\nSoftware\nSoftware packages used in the training were free and open-sources software (FOSS) and downloadable from internet i.e.¬†GPS Utility (http://www.gpsu.co.uk/download.html), Google Earth (http://earth.google.com) and MapWindow (http://www.mapwindow.org), DNR Garmin (http://www.dnr.state.mn.us/mis/gis/tools/arcview/extensions/DNRGarmin/DNRGarmin.html).\nGPS Maps\nTo complete a map that has been available in the GPS, Indonesia maps that can be downloaded free through from Navigasi.net (http://navigasi.net/). Navigasi.net is a site and forum of a group of GPS users in Indonesia.\nSupporting Documents\n\nList of District Road Network in Bireuen was used as supporting data during field training\nAbout GPS: This document discussed concept and the use of GPS Technology in various Industries\nGPS Data Collection Form for field training: Form was used to fill the additional information obtained during the survey.\nGPS Terminology: This document contained a list of terms commonly used in GPS.\nDo We Need GPS? ‚Äì Navigasi.net: This article discussed about the advantages use of GPS devices, what can be done using the GPS and also gives other illustration related to the GPS which is about the tracking and routing (determining the direction of travel).\nGPS and Garmin POI ‚Äì Navigasi.net: This article discussed the types of Point of Interest (POI) found in Garmin devices.\n\n\n\nGIS Training\nThis document contained information about the material that was delivered to participants in the GIS training.\nSlide presentation\n\nWhat is GIS: The presentation discussed about the definition of GIS and geodata and Typical questions a GIS can answer.\nIntroduction to ArcGIS: This presentation was aimed to introduce participants to the basic use of ArcGIS. ArcGIS is a large program with many extensions and many uses. We will concentrate on ArcMap and ArcCatalog.\nWorking with external data source: The strength of GIS is that we can use many different information sources to bring together relevant information.\n\nModules\n\nInstallation: The step-by-step installation of ArcGIS Desktop.\nIntroduction to ArcMap: The purpose of this exercise is to get familiar with the application and learn how to use the basic map tools in order to create a map using a data set over Indonesia in ArcMap.\nCreating Map Objects: The purpose of this exercise is to learn how to create different geographic features represented as line, points and polygons.\nWorking with Buffers: The purpose with this exercise is to get familiar with the ArcToolbox and the two different buffer tools used for geoprocessing.\nJoining Tables: The purpose of this exercise is to use different databases to combine data and visualize the result as unique values symbology.\nExercise - Working with External Geodata\n\nSoftware\nSoftware packages used in the training class were from ESRI, ArcGIS 9.3.1 Student licenses and downloadable from internet (http://www.esri.com/software/arcgis/arcview/eval/evaluate.html).\nSpatial and Non-spatial data used in this module\n\nAdministrative boundaries, Road network, Capital city of Aceh\nJakarta image map\nEarthquake epicenter\nPlace code and PNPM data of Aceh\nHealth facilities and Shelter assessment data during Emergency situation in Jogjakarta\nIndonesia villages boundaries\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200710-calculate-spi-using-chirps-data.html",
    "href": "blog/20200710-calculate-spi-using-chirps-data.html",
    "title": "Calculate SPI using CHIRPS data",
    "section": "",
    "text": "NOTES - 1 Mar 2022\nI have write a new and comprehensive guideline on SPI, you can access via this link https://bennyistanto.github.io/spi/\n\nI explained¬†in the previous blog post on how to calculate monthly SPI using daily IMERG data. This time I use CHIRPS, because I want to get higher resolution, more frequent monitoring (updated every dekad ~ 10days), and long-term historical data from 1981 ‚Äì now.\n\nCHIRPS data acquisition for SPI analysis\n\n\nExample output\nSPI 1, 2, 3, 6, 9, 12, 24, 36, 48, 60 and 72-month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSPI 3-month for Indonesia, March 2020\n\nWFP‚Äôs calculated SPI, based on IMERG\nWFP‚Äôs calculated SPI, based on CHIRPS\nBMKG, based on weather station. Link: https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg?p=the-standardized-precipitation-index-maret-2020&tag=spi&lang=ID\nClimate Engine, based on CHIRPS. Link: https://climengine.page.link/nTyi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\nThanks to Yanmarshus Bachtiar for CDO‚Äôs tips.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200423-vulnerable-groups-in-covid-19.html",
    "href": "blog/20200423-vulnerable-groups-in-covid-19.html",
    "title": "Vulnerable groups in COVID-19",
    "section": "",
    "text": "[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. Elderly population (60+) and access to hospital [/caption]\nGood access (to markets, health centers, education) means:\n\nContributes to the diversification of household economies\nStrengthen households‚Äô resilience against shocks and coping capacity\nImproves health status and increases standards of living\n\nAccess to health facilities.\nAccessibility calculations are based on a quantification of time of travel. The approach is to obtain a gridded layer of time of travel ‚Äì in short, each point in a regular grid has the value of the time it takes to travel to the nearest (pre-defined) health facilities. Once we have this time of travel output, we can derive catchment areas or quantify population living more than a given travel time away from the target locations.\nRather than deriving time of travel from a plain geographic (or Euclidean) distance and assuming a fixed travel speed, the method described here allows us to account for the different speeds of travel that various surfaces allow (mountain forest versus flat open ground, a highway versus single track) as well as any barriers that might encounter (national borders, rivers, mountains) and derive the time it would take to travel along the most economical path. Economical here in the sense of the one that takes the shortest time to complete.\nThe modelling of travel time starts by deriving a general travel speed, over both roads and terrain. The calculation of this travel speed accounts for factors such as the road quality, the type of land cover; the topographical slope acts as a speed-reducing factor while obstructions (rivers, borders, road closures) that prevent or delay travel can also be introduced.\nThis is converted to a cost or friction surface (cost in terms of the time it takes to move from one grid cell to the next) which is input to a cost-distance function that derives travel time to the target locations from the accumulation of these travel costs.\nThe underlying assumption here is that we are dealing with mixed modes of transportation. Given any arbitrary location away from a road, travel proceeds at walking speed (modulated by land cover and slope) until a road is reached, at which time travel proceeds at vehicle speed set as a function of road quality. In any case the model is trivially adapted to walking travel only by adopting suitable travel speeds.\nReference: http://technicalconsortium.org/wp-content/uploads/2014/05/Accessibility-Mapping_RuralPoverty.pdf\nAbove analysis is based on raster data with a pixel size of 30m, an area with no color means no population. The map symbology inspired from Bivariate Choropleth Map from NASA‚Äôs cartographer Joshua Stevens.\nIf we define high vulnerability as an area with a high number of elderly people but difficult access to health facilities (in this case Category B3, C2, and C3), then around 250,000 elderly or equal to 7% of the total elderly in West Java are belong to this group.\nElderly population and demographic estimates data used in this analysis came from Facebook Population Density Maps and available free for public at Humanitarian Data Exchange platform.\nIf you are curious about how to analyze it, please read my old post on accessibility models.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210611-ninth-year-and-the-2020-nobel-peace-prize-laureate.html",
    "href": "blog/20210611-ninth-year-and-the-2020-nobel-peace-prize-laureate.html",
    "title": "Ninth year and the 2020 Nobel Peace Prize Laureate",
    "section": "",
    "text": "[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] ‚Äúuntil the day we have a vaccine, food is the best vaccine against chaos‚Äù [/caption]\nToday marks my ninth year working with the UN World Food Programme, I feel honoured because this has been beyond a profession, but I found my definition of ikigai (A Reason for Being).\n\nI LOVE drawing and coloring, especially map since I was a kid\nI am GOOD AT turning data into maps for actionable and live-saving insight WFP are delivering\nas one of crisis mapper at WFP, I GET PAID for it, of course\nwith WFP, I share the ideas of helping others and contribute to humanitarian NEEDS is as simple as I do\n\nLast year, WFP was awarded the Nobel Peace Prize! This completes my achievement at WFP: as the best GIS in the 1st WFP GIS Community Award in 2015, and with VAM team won 2017 WFP Innovation Challenge, for VAMPIRE works.\nThank you WFP!\nLooking forward to doing more meaningful work in climate analytics and geospatial technology for greater impact.\nCredit: thumbnail image - https://www.instagram.com/p/CGHfltyAvlF/\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20141216-osm-use-case-market-accessibility.html",
    "href": "blog/20141216-osm-use-case-market-accessibility.html",
    "title": "OSM use case: accessibility mapping",
    "section": "",
    "text": "OpenStreetMap (OSM) is a collaborative project to create a free, editable map of the world. The core of OSM is a collection of map data which can be used for many different purposes. You can get a first idea browsing the world map on the main page of OSM. While browsing, try the Layers button on the right to view different map formats. It illustrates how OSM‚Äôs data can be used in diverse manners.\nI will explain on how OSM data can be applied in the humanitarian context to support WFP‚Äôs program activities in the field."
  },
  {
    "objectID": "blog/20141216-osm-use-case-market-accessibility.html#market-access-in-sumba-island-indonesia",
    "href": "blog/20141216-osm-use-case-market-accessibility.html#market-access-in-sumba-island-indonesia",
    "title": "OSM use case: accessibility mapping",
    "section": "Market access in Sumba Island, Indonesia",
    "text": "Market access in Sumba Island, Indonesia\n\nDEFINITION AND RELEVANCE FOR FOOD SECURITY\nThe term ‚Äúaccessibility‚Äù refers to the distance to a location of interest and ease with which each destination is reached (Goodall, 1987).\nAccessibility in terms of ease of physical access to markets and social infrastructure has an impact in terms of food security and poverty of (rural) populations. Access to markets can contribute to the diversification of household economies by exposing them to channels for the sale of their labour or products. Well-functioning markets go a long way to insulate households from shocks to crop production such as those from droughts or floods. Good access to health and educational resources is an essential condition for development and the increasing in standards of living.\nHence the interest in the modelling and quantification of accessibility for institutions involved in humanitarian assistance is high. Maps of travel time to some target location (market, administrative capital) can flag isolated areas more prone to chronic poverty and food insecurity. From them you can derive the catchment area of target locations, i.e.¬†areas where populations are closer to a given target than to any other: Catchment areas of markets can highlight the reach of particular markets or enable you to characterize the population served by a given market in terms of its wealth, food security or livelihood. You can also broadly characterize a country by mapping propotions of the population that are more than a certain travel time away from a given amenity (such as health centres, water points, education resources).\nBeyond these operational applications, being able to model travel time and catchment area of markets (or health centres or any other location of interest) offers the possibility to integrate accessibility data into household surveys as additional explanatory variables.\nThis publication adapts an existing GIS model of accessibility, and implements it as a self study training guidebook to a study of market access in Sumba Island, East Nusa Tenggara, Indonesia. It covers the preparation of travel time and market catchment data layers. It also shows how to integrate these datasets into household survey databases and how to use the travel time for estimating the percentage of population having access to a market in a one day round trip.\n\n\nREFERENCES\nThis accessibility model used in this training guidebook has been developed by Pozzi and Robinson in the scope of the FAO ‚Äì IGAD Livestock Policy Initiative. The main reference to this exercise is:\nAccessibility Mapping in the Horn of Africa: Applications for Livestock Policy, Francesca Pozzi and Tim Robinson, Dec 2008, which can be found here: https://cgspace.cgiar.org/handle/10568/24984\nYou can also see a practical application in the context of rural population access to health centres in Niger in:\nJustine Blanford, Supriya Kumar, Wei Luo and Alan M MacEachren, ‚ÄúIt‚Äôs a long, long walk: accessibility to hospitals, maternity and integrated health centers in Niger‚Äù, International Journal of Health Geographics, 2012, 11:24. http://www.ij-healthgeographics.com/content/11/1/24\n\n\nTHE MODELLING APPROACH\nAccessibility calculations are based on a quantification of time of travel. The approach is to obtain a gridded layer of time of travel ‚Äì in short, each point in a regular grid has the value of the time it takes to travel to the nearest (pre-defined) target location. Target locations in this exercise are markets, but it could be any point of interest, form settlements with more than a given number of inhabitants, to provincial capitals, to health centres, banks, etc. Once you have this time of travel output, you can derive catchment areas or quantify population living more than a given travel time away from the target locations.\nRather than deriving time of travel from a plain geographic (or Euclidean) distance and assuming a fixed travel speed, the method described here allows you to account for the different speeds of travel that various surfaces allow (mountain forest versus flat open ground, a highway versus single track) as well as any barriers that you might encounter (national borders, rivers, mountains) and derive the time it would take to travel along the most economical path. Economical here in the sense of the one that takes the shortest time to complete.\nThe modelling of travel time starts by deriving a general travel speed, over both roads and terrain. The calculation of this travel speed accounts for factors such as the road quality, the type of land cover; the topographical slope acts as a speed-reducing factor while obstructions (rivers, borders, road closures) that prevent or delay travel can also be introduced.\nThis is converted to a cost or friction surface (cost in terms of the time it takes to move from one grid cell to the next) which is input to a cost-distance function that derives travel time to the target locations from the accumulation of these travel costs.\nThe underlying assumption here is that we are dealing with mixed modes of transportation. Given any arbitrary location away from a road, travel proceeds at walking speed (modulated by land cover and slope) until a road is reached, at which time travel proceeds at vehicle speed set as a function of road quality. In any case the model is trivially adapted to walking travel only by adopting suitable travel speeds.\nThe steps below will be followed:\n\nAssign average speeds to roads of different class.\nAssign a variety of walking speeds to a number of land cover units.\nConsider National Boundaries as barrier to limit the speed.\nRivers are specified as an untransposable barrier to travel.\nCalculation of overall travel speed and cost distance\nSlope is used as a factor that increases travel time.\nDerivation of the travel time data layer\nDerivation of the market catchments\nEstimating the population with poor access\nIntegrating accessibility variables into household surveys\n\n\nThe following are the datasets used in this study and their sources:\n\nDigital Elevation Model, SRTM 3 arc-second (about 90 m) resolution.\nRoad network layer from OSM, downloaded from Geofabrik\nHydrographic network, extracted from OSM and BIG‚Äôs RBI\nLand Cover database extracted from Global Land Cover (Globcover 2009) at 250 m scale resolution (dataset and legend to classify classes)\nA set of market locations, extracted from OSM and Indonesia‚Äôs Geospatial Information Agency (BIG) topographic map (RBI) database\nLandscan population density dataset at 30 arc-second (about 1 km) resolution\nNational boundaries downloaded from BIG\n\n\n\n\nAPPLICATION\nCharacterizing markets catchments\n\nArea of influence of markets is delineated\nCharacterized by land cover, livelihoods, poverty, food insecurity\n\nIncorporating accessibility into analysis of household surveys\n\nExtract travel time values at HH survey sampling locations\nIncorporate in HH survey DBs\n\nDeriving a general measure of accessibility\n\nDefine threshold T of travel time for ‚Äúpoor access‚Äù\nCalculate population over admin\nCalculate population within admin*travel time &lt; T\n(Ratio) Proportion of both"
  },
  {
    "objectID": "blog/20141216-osm-use-case-market-accessibility.html#exercise",
    "href": "blog/20141216-osm-use-case-market-accessibility.html#exercise",
    "title": "OSM use case: accessibility mapping",
    "section": "Exercise",
    "text": "Exercise\n\n1. ASSIGNING TRAVEL SPEED TO THE ROAD NETWORK\nAdd a new field to the Road layer attribute table and assign a value for the average speed (m/min) of travel over each road type according to the table below:\n\nThis is a crucial stage in the model: needs ADAPTATION to country context\n\nOpen Attribute Table, Create New Field, call it AvgSpeed:\nSelect all fields of ‚Äúprimary‚Äù, set AvgSpeed to 1000\n\nRepeat for other road types, close table to save.\n\n\n2. CONVERTING A NATIONAL BORDER OR ISLAND BOUNDARY TO A TRAVEL BARRIER\nAdd a new field to the Road layer attribute table\nConfirm the file is a polygon feature, convert to a line if NOT:\n\nProperties -&gt; Source tab -&gt; Geometry Type: Polygon\nArcToolBox | DataManagement Tools | Features | Feature To Line\n\nAssign a speed to the border. Same procedure as for Roads, but you only need one single value. Set to 17 (17 meters/min = 1 km/h)\n\n\n3. ASSIGNING TRAVEL SPEED TO THE RIVER DATA\nAssign a travel speed to the river data using the same method\n\nFill the new field ‚ÄúAvgSpeed‚Äù with the value of 0 (default fill)\n\nThis implies that you cannot cross rivers, except through roads\n\n\n4. ASSIGNING TRAVEL SPEED TO THE LAND COVER DATA\nClassification for Land Cover classes\n\nRe-classify the land cover data into travel speeds‚Ä¶\n\nOpen ArcToolBox, go to Spatial Analyst | Reclass | Reclassify.\n\nInput Raster: globcover2009\nReclass Field: VALUE (the pixel values)\n\nClick on Unique in order to list all values\nRefer to the Land Cover class table: enter in the New Value column the travel speeds corresponding to the Old Values (pixel values)\nSave the output as LandSpeed. It will be added to the Table of Contents and displayed.\n\n\n\n5. SETTING THE ARCGIS ENVIRONMENT FOR THE NEXT STEPS\nSetting the correct pixel alignment and the correct spatial extent using DEM data raster extent:\n\nClick-right in the ArcToolBox (or click the Geoprocessing menu) and select Environments.\nSelect Processing Extent (in ArcGIS 10.x)\n\nSelect an existing raster to snap the new rasters to:\n\nIn the Snap Raster field, specify the Elevation raster name, idn_phy_elevation_90m_srtm.tif.\n\nTo define the correct spatial extent:\n\nIn the Extent field, use the explore button to browse to and select the Elevation data. Or set to Union of Inputs.\n\n\n\n\n6. CONVERTING ROADS, RIVERS, AND BORDER LAYERS TO RASTER\nStart with the roads layer: A field in the Attribute Table is used in the conversion to raster. The values of this field will be the pixel values. E.g. AvgSpeed for Roads\nSpatial Analyst Tools | Convert | Features to Raster\n\nInput Features: road layer\nField: AvgSpeed as the field to use in the conversion\nOutput raster: RoadSpeed\nOutput cell size: 0.000833333 (should be there already) or filled with Elevation file name and location.\n\nThen proceed to the others: Rivers and Borders, and save as RiverSpeed and BorderSpeed.\n\n\n\n7. MERGING ALL THE TRAVEL SPEED RASTERS TOGETHER\nThe travel speed data for each component have to be combined into a single travel speed raster.\nCombination of BorderSpeed, RoadSpeed, RiverSpeed and LandSpeed has to follow a precise order. Order determines is VERY important: keep to this precise order!\nIn ArcGIS 10.x, it‚Äôs done through Mosaic to New Raster.\n\nArc Toolbox: DataManagement | Raster | Raster Dataset | Mosaic to new Raster\n\nIn Mosaic to new Raster interface:\n\nInput Rasters: Drop or Add travel speed rasters in the mentioned ORDER\nOutput Location: /path/to/dir\nRaster Name: TravelSpeed\nPixel Type: 16_BIT_UNSIGNED\nMosaic Operator: FIRST\n\n\n\n\n8. CONVERTING TRAVEL SPEED TO COST DISTANCE\nConvert travel speed to cost of travel.\nTravel speed pixel values: speed, meters/min.\nTravel cost pixels values: time, time to cross the pixel.\nCalculate the inverse of the travel speed and apply a numeric factor for converting meters to degrees\nIn Raster Calculator enter the following expression:\n111321 / ‚ÄúTravelSpeed‚Äú\n\n\nOutput name: TravelCost0 9. CALCULATING THE SLOPE FACTOR\nThe slope factor is derived from a Digital Elevation Model (DEM, SRTM 30 arc-second or about 1 km). This involves two stages:\n\nDeriving slope from a DEM\nReclassifying the slope into travel penalties or weights\n\nDERIVING SLOPE FROM DEM\nImport the DEM (yem_dem) into your project\n\nArcToolBox | Spatial Analyst Tools | Surface | Slope\nOutput Measurement: Percent\nZ factor (Sumba): 0.00000912\nOutput: temp_slope\n\n\nEnvironment Settings | Raster Analysis ‚Äì select the dem raster for the Cell Size field.\nZ factor for Sumba Island: 0.00000912\n(The latitude in the middle of the country is about 9.7S, so we can take the value for 10, or 0.00000912)\nRECLASSIFY SLOPE\nSlope factor is derived from re-classification of Slope into weights that penalize the travel speed.\n\nE.g. Under slope factor of 80%, speed is 80% of flat terrain speed.\nOutput = SlopeWeight\n\nSpatial Analyst | Reclass | Reclassify\n\n\n\n10. DERIVE FINAL TRAVEL COST LAYER\nApply the slope weight through a simple expression‚Ä¶\n\nTravelCost = 100*TravelCost0/(SlopeWeight)\n\nThis done in Spatial Analyst Tools | Map Algebra | Raster Calculator\nThis travel cost is the cost (in time) of crossing each grid cell.\nThis is the key input for the next step which will take this cost layer and derive the total cost (i.e.¬†the travel time) to the nearest target location.\n\n\n11. DERIVE TRAVEL TIME OUTPUT\nTravel time is derived using the CostDistance function:\nSpatial Analyst Tools | Distance | Cost Distance\nInput parameters:\n\nInput Source Data: Markets (from OSM data)\nInput Cost Raster: TravelCost\nOutput: TravelTime0\n\nConvert to integer. In RasterCalculator enter:\nInt(TravelTime0)\nSave As TravelTime\nPREPARE A TRAVEL TIME MAP\nDisplay Travel time with proper¬†symbology¬†(Classified) using values in table\n \n\n\nAPPLICATION 1: CHARACTERIZE MARKET POPULATION\nRelevance: Knowing the socio-economic status of the population within a given market catchment (Cash and Vouchers)\nSteps:\n\nJoin HH Survey Locations with Market Catchment\nCreating a new feature class (point) , e.g YemHHMkt\nExport to Excel\n\nFollowing Work:\n\nTake Excel to SPSS, populating database, so that each HH is labelled with the catchment it belongs to.\nThen aggregate over catchment - % Food Insecure, Poverty, etc,\n\n\n\nAPPLICATION 2: ACCESS IN HOUSEHOLD FOOD SECURITY ANALYSIS\nRelevance: Knowing how far each household is from the next market could provide additional explanatory power in analysis of Food Insecurity patterns (additional variable to account for in analysis).\nSteps:\n\nDo an Extraction Values to Points ‚Äì TravelTime (raster) to HHSurveyXY (points)\nOutput is a new feature class (point) , e.g HHMktDist\nExport to Excel\n\nFollowing Work:\n\nTake Excel to SPSS, populating database, so that each HH is labelled with the distance to the nearest market.\nUse in analysis as additional variable as relevant and required\n\n\n\nAPPLICATION 3: MAPPING POPULATION WITH POOR ACCESS\nRelevance: Knowing what proportion of a given district population is more than X hours from a market or urban centre offers a way to quantify the degree of poor accessibility. X? E.g. 4 hours.\nSteps:\n\nTravelTime raster reclassified to mask (time &lt;= X, 1, else 0)\nMultiply mask by Landscan\nTotal population in district through Zonal Statistics on Landscan\nTotal population in district through Zonal Statistics on masked Landscan\nDo a ratio of the two with Raster Calculator\n\n\n\nEXAMPLE\nUsing additional data: sub district boundary, population density, and additional analysis to calculate market service area using Cost Allocation tool, new product could be derived.\n\nProportion of population less than 3 hours from a market\nAt a glance quickly identify the more isolated population in Sumba\nThis was one of the key variables used to prioritise populations for multi-agency food and nutrition security interventions"
  },
  {
    "objectID": "blog/20230824-monthly-mosaic-of-modified-radar-vegetation-index.html",
    "href": "blog/20230824-monthly-mosaic-of-modified-radar-vegetation-index.html",
    "title": "Monthly mosaic of modified Radar Vegetation Index",
    "section": "",
    "text": "Few months ago, I wrote a post about how to calculate The modified Radar Vegetation Index (mRVI) using Sentinel-1 satellite. It was try to extract the mRVI every Dekad with study case is Ukraine.\nFor areas in Europe, getting the S1 data every dekad is doable, but it‚Äôs bit tricky for area outside Europe. Currently, for my work, I would like to extract the mRVI for Mpumalanga province in South Africa. The location is near -25.5 S, then according to below picture the revisit time is every 12 days.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3579‚Äù][](https://sentinel.esa.int/documents/247904/4748961/Sentinel-1-Repeat-Coverage-Frequency-Geometry-2021.jpg) Source: https://sentinel.esa.int/documents/247904/4748961/Sentinel-1-Repeat-Coverage-Frequency-Geometry-2021.jpg [/caption]\nGetting monthly mosaic mRVI seems possible for case in South Africa, as if I keep the dekad, some of them will return an empty collection.\nSo, I need to modify the existing code to get the monthly list, calculate monthly mosaics, calculate monthly mean and the ratio anomaly\nFull GEE code is here: https://code.earthengine.google.com/aea00cb8f3f1ccc921d5f6698b5c0c5a\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20150801-2015-esri-user-conference.html",
    "href": "blog/20150801-2015-esri-user-conference.html",
    "title": "2015 ESRI User Conference",
    "section": "",
    "text": "San Diego Convention Center\n20 ‚Äì 24 July 2015, San Diego, CA, USA\n\n\nThe Conference\n\nPlenary Session and Keynote Address\n\nHere‚Äôs a quick overview of the opening Plenary Session, Applying Geography Everywhere ‚Äì Welcome and GIS Vision:\n\nEnter the Era of Geographic Enlightenment. Jack Dangermond introduced the term geoenlightenment during introductory remarks at the Esri UC Plenary Session. What is geoenlightenment? Dangermond said it means understanding how things on our planet are interconnected and applying that knowledge to make the world a better place. ‚ÄúWhat we do here, affects there,‚Äù\nNew Development in ArcGIS\n\n\nEsri just launched the R - ArcGIS Community on GitHub. The new initiative‚Äôs goal is to build a collaborative community for R and ArcGIS users. R is an open source programming language for statistical analysis. R users can directly access all their organization‚Äôs GIS data, and ArcGIS users can directly integrate R into their geoprocessing workflows.\nAppStudio for ArcGIS. Esri demonstrated how to create native apps for multiple platforms in a snap using the configurable application templates and other functionality in the new AppStudio for ArcGIS. There‚Äôs no coding required.\nAnalyze Very Big Data with ArcGIS.\nEsri App for Drone Imagery.\nVector Tiles\nArcGIS Ready-to-Use Apps\n\n\nWhat‚Äôs Trending in Web GIS, including the establishment of portals for geospatial information, the growing amount of authoritative content that‚Äôs available, and easy-to-use analysis tools.\nOpen and Interopable\nArcGIS Pro and Environmental Justice\n\nPlenary-Geography Around the World:\n\nBruce Aylward of the World Health Organization talked about his work to stop the Ebola outbreak in western Africa and the success brought about by mapping the movement of patients and the people who were exposed to them.\nGary Knell from National Geographic Society, called for greater geographic literacy and said educators must take into account the technological savvy of kids who will ‚Äúnever know the world before tablets and smart phones.‚Äù\nMartin O‚ÄôMalley from State of Maryland, used GIS in his administrations to identify problem areas that required government services.\nDangermond honored many organizations with awards for their fine work implementing GIS.\n\n\nSession (Technical Workshop)\n\nHere the list of technical workshop that I attended\n\nTue, Jul 21\n\n8.30 ‚Äì 9.45 ArcGIS Online: Living Atlas of the World. ArcGIS Online provides access to a diverse and growing set of content to enrich the ArcGIS user experience. This content represents a living atlas of the world with beautiful and authoritative maps on hundreds of topics.\n10.15 ‚Äì 11.30 Leveraging GIS for Climate Change and Sustainable Development. The presentation describes how the system integrates remote sensing, soil, crop, climate and other data to produce GHG estimates and support tools to address development challenges.\n11.30 ‚Äì 11.45 Story Maps ‚Äì Simple Yet Compelling.\n1.30 ‚Äì 2.45 Creation of a Composite Drought Index using ArcGIS and Satellite Data. ArcGIS and its extensions were used to resample and extract indicators values from the massive raw data sets and then to calculate the Composite Drought Index.\n3.15 ‚Äì 4.30 ArcGIS Online: The Mapping Platform for Your Organization. This session talking about how the organization can create interactive web maps and apps that we can share with anyone and discover what‚Äôs included such as ready-to-use content, apps, and templates.\n4.30 ‚Äì 6.00 Map Gallery\n\nWed, Jul 22\n\n8.30 ‚Äì 9.45 Python Map Automation: Intriduction to arcpy.mapping / arcpy.mp. It provide an overview of the mapping module along with demonstrations of how it can be used to process the contents of map documents and layer files and how to automate exporting, printing, and the generation of PDF map books.\n11.00 ‚Äì 11.30 How to make a Story Map Journal\n12.00 ‚Äì 12.30 How to make a Story Map Tour\n1.30 ‚Äì 2.45 ArcGIS Online: Smart Mapping ‚Äì Make brilliant maps quickly and with confidence. Smart Mapping allows novices and pros alike to work faster centered around the idea of data-driven workflows, smart defaults, and automatic cartographic expertise.\n3.15 ‚Äì 4.30 Volunteers can do it!: Using Collector for Natural Resources Management. This is an example the used of Collector for ArcGIS app for NGO with limited resources.\n4.30 ‚Äì 6.00 Map Gallery\n\nThu, Jul 23\n\n8.30 ‚Äì 9.45 Python ‚Äì Raster Analysis.This session introduced Spatial Analyst arcpy module, best practices for using the raster object and classes to expand the modelling capability, optimized performance and identify how to use NumPy.\n10.15 ‚Äì 11.30 Administering your Microsoft SQL Server Geodatabase. This session address a variety SQL Server specific configuration and optimization techniques for experienced enterprise geodatabase administrators.\n12.00 ‚Äì 12.30 How to make a Story Map Tour\n1.30 ‚Äì 2.45 ArcGIS Online: Managing Data. This session talking about how to store the data online and, once stored, how to manage it effectively.\n3.15 ‚Äì 4.30 A GIS model to identify Flood Affected Areas using Landsat Images. This session give an example on how conduct an automated multi-temporal analysis of Landsat images, the model would provide valuable information to assess damage and mitigate losses.\n5.30 ‚Äì 8.00 Thursday Night Party\n\nFri, Jul 24\n\n9.00 ‚Äì 10.15 Collector for ArcGIS: an Overview. This session explain how to use this app and configure it to satisfy a specific data collection workflow.\n10.30 ‚Äì 12.00 Closing Session\n\nESRI Showcase\n\nBesides attending the technical workshop, I am also visited ESRI showcase on Humanitarian, Nonprofit, Apps, Online, Mapping and Visualization, Spatial Analysis. I also took the opportunity to discuss with ESRI developer and explain some of my problem using ESRI product.\nLesson-learned\n\nTell Story Using a Map\n\nThe Story Map is ideal when we want to combine narrative text with maps and other embedded content. A Map Journal and other contains entries, or sections, that users simply scroll through. Each section in a Map Journal has an associated map, image, video or web page.\nDuring ESRI UC, all staffs of ESRI as well as most of participants made use of Story Map for presenting their products. Story Map is likely to replace other presentation slide applications because it has more feature. Users are able to modify ESRI templates via the following link https://github.com/Esri/map-journal-storytelling-template-js that provides guideline for customizing the templates. However Story Map has some shortages such as standard chart is difficult to be edited. This problem can be solved by using cedar.js chart library here http://esri.github.io/cedar/. Chart customization in Story Map needs advance programming skill and will change a whole appearance. Staff with limited programming skill is likely to only use default templates. WFP Indonesia has used Story Map for presenting Food Security and Vulnerability Atlas 2015 http://arcg.is/1yThC40\nStandard feature on Story Map does not allow too much editing to tables and graphs. WFP Indonesia applies ArcGIS online maps as basemaps for alternate version of FSVA online http://fsva.wfp.or.id\nThe alternate version of FSVA online uses D3, Highchart and CoffeeScript and other library and a set of data separately stored. Tables and graphs of FSVA online are interactive and user friendly.\nDuring UC I discussed a number of issues with ESRI Story Map developer including the options for template customization, tables&chart modification. Unfortunately event the easiest option still need quite advance programming skill. It is possible to present interactive tables and graphs on http://fsva.wfp.or.id in Story Map by embedding it to http://arcg.is/1yThC40 as per suggestion from ESRI Story Map team.\nESRI staff also suggest to me to submit the FSVA story map to the gallery so it can be appear in ESRI website. But they also make sure that FSVA story map should be referenced on a WFP website page or blog.\n\nThe Power of Apps\n\nArcGIS includes a suite of apps that are ready to go and free to use.\n\nExplore maps\n\nMapping apps like ArcGIS Explorer for the Mac, provide a way to manage a collection of data. We can find, use, and share maps from Mac, Android, or iOS device using ArcGIS Explorer.\n\nCollect data using Collector for ArcGIS and Survey123\n\nWe can use smartphone or tablet to collect and update information in the field, whether connected or disconnected. The power of Collector for ArcGIS enables organizations to use maps to gather data in the field and to synchronize the results with their enterprise GIS data. With Collector we can update data in the field, log the location, and put the capture data back into central GIS database directly from phone or mobile device with iOS/Android.\nBelow pictures are the example of Collector for ArcGIS running in my iPhone device and used for collecting IDP and flooded area in Jakarta.\nWe can download maps to our device to work offline; use GPS to create and update map data, points, lines, and area features; fill out easy-to-use map-driven forms; find places and get directions; track and report areas visited.\nFor gathering data in the field, we can combine Collector for ArcGIS with Survey 123 app. We can design the surveys in a spreadsheet and use Survey123 Connect to upload the surveys to ArcGIS. Get the Survey123 for ArcGIS mobile app from the Google Play and Apple App Store, download surveys and start collecting data. That‚Äôs it.\nNext month I will test Survey123 to collect data for Local School Meal program in Papua, and will displaying the result through Rael-Time Operation Dashboard.\n\nOffice for Maps\n\nESRI Maps for Office helps us integrate information from Microsoft Office product like Excel dan Powerpoints with our maps. Our Excel spreadsheet data mapped within the Excel environment and updated automatically as oour work on the spreadsheet.\nThis app still have limitation like we can‚Äôt create custom color for our maps and custom classification using natural breaks renderer or equal interval value. Here‚Äôs the example of the used ESRI Maps for Office for displaying the food security and vulnerability atlas data.\n\nReal-Time Dashboard\n\nUsing Operation Dashboard for ArcGIS, we can monitor activities and events, track our field workforce, and assess the status and performance of WFP daily operations.\nOperation Dashboard for ArcGIS have many features like: Bring together a common view of the systems and resources you manage. Monitor real-time data feeds for large-scale events or day-to-day operations on your desktop or tablet device. Create focused executive dashboards that integrate maps, charts, and graphs. Use maps with dynamic data sources to provide real-time views. Configure charts, gauges, histograms, and more, to provide statistical context. Create and share operation views that are focused on a specific need. Design views for use on multiple monitors or single-display devices. Create custom widgets and tools to suit your needs.\nHere‚Äôs the example from ESRI Netherland on the used of Operation Dashboard to monitor the election.\nWFP Indonesia will create similar application and currently I am still configuring the dashboard, maps, data and survey from Local School Meal program in Papua.\n\nPortal for ArcGIS\n\nFrom the demo at ESRI showcase, Portal for ArcGIS ‚Äì an extention for ArcGIS Server which is ArcGISOnline-like software that we can install on-premises in case we don‚Äôt want to expose our resources to the Internet or for any other reason. Portal will let us organize our resources within a company and manage the services from one place. In general, Portal have similar feature with ArcGIS Online like: Create, save, and share web maps, Create and host web mapping apps, Search for GIS content within organization, Create groups for sharing GIS information with coworkers, Share links to GIS applications, Share map and layer packages to use in ArcGIS for Desktop.\nSince 10.2, Portal is an extension to ArcGIS Server software (which means it is not available directly). Earlier, it was a separate product that was accessible only via Esri Professional Services. I can‚Äôt explore this app more details because WFP Enterprise Level Agreement didn‚Äôt include this product.\nBut it would be very advantageous if we have this extension, considering the current WFP data are hosted on ArcGIS Online which is at ESRI cloud, and after the subscription is ended, the data also will be deleted and we can‚Äôt recover it. So, by using the Portal and ArcGIS Server, and WFP already implemented the SDI, we can utilize all this product as a substitute for ArcGIS Online with several advantages, our data is safe and all the apps are hosted in WFP server.\n\nOpenData Portal and Geoportal Server\n\nAs part of our ArcGIS Online subscription, we can use ArcGIS Open Data to share our live authoritative open data. Esri-hosted ArcGIS Open Data gives us a quick way to set up public-facing websites where people can easily find and download your open data in a variety of open formats. During the UC, there is a good example of OpenData portal from WHO displaying all the data related to Ebola response.\nESRI also have an open-source product for metadata catalogue called Geoportal https://github.com/Esri/geoportal-server/ that we can physically install in a server.\nArcGIS Open Data uses the ArcGIS Online groups to identify open data, allowing us to quickly publish or remove our open data. Our open datasets automatically sync with the latest version of our sources. It can even integrate with other open data platforms, such as CKAN and ESRI Geoportal.\nIt seems that both of product (OpenData and Geoportal) are competitor of our Geonode portal. I will request to Filippo to activated the OpenData feature for my ArcGIS Online account, so I can test that platform in near future.\nConclusion\nSome applications of ArcGIS Online platform can be used/implemented in the Country Office or maybe Field Office. I‚Äôve tried several apps and it looks pretty good. The main problem is the lack of good internet connections, it will also block the access to ArcGIS Online. To customize the application (interface, feature, etc.) required a programming skill.\nNext Step\nIn the near future I am planning develop some examples of ArcGIS online application for WFP Indonesia activities such as the use of Story Map for online version of FSVA, Collector for ArcGIS and Survey 123 for collecting field survey data, monitoring progress of Local School Meal program in Papua using Operation Dashboard and use of ESRI Maps for Excel for distributing tabular data and FSVA maps via Microsoft Excel.\nEnough time for preparation is necessary for future winner of GIS community award who get a chance to join ESRI UC. I had only short preparation. I was notified about the award shortly before the event, difficulty to find accommodation and flights, WFP ArcGIS Online account was not valid for registration which I was not officially registered until a day before ESRI UC started. I did not get a chance to participate map design competition (Map Gallery) due to this issue of registration because deadline for submission of the map was about 2 weeks before the conference.\nESRI UC Asia Pacific in Bangkok and FOSS4G 2015 in Seoul are potential chances to present GIS products of WFP including implementation of SDI during the period of 2014 - 2015\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20161205-standing-crowd-density-2-desember-2016.html",
    "href": "blog/20161205-standing-crowd-density-2-desember-2016.html",
    "title": "Standing crowd density, 2 Desember 2016",
    "section": "",
    "text": "Lagi rame tentang angka 7 juta :)\nMasih terkait dengan metode perhitungan yang cocok untuk standing crowd density pada 2 Desember di Monas dan sekitarnya, saya mencoba menghitung ulang dengan memanfaatkan foss4g. Kebetulan saya mempunyai shapefile jalan di Jakarta dalam bentuk polygon, sehingga memudahkan saya untuk segera memulai perhitungan.\nHasil dari perhitungan berikut menggunakan beberapa asumsi\n\nMomen yang mau diambil ini adalah ketika menjelang sholat Jum‚Äôat dan sesudahnya, jadi peserta aksi dalam keadaan diam (sedikit bergerak).\nWilayah yang dihitung berdasarkan beberapa foto dari atas yang beredar di social media, jadi jika ada wilayah diluar perhitungan, berarti tidak/belum ditemukan foto dari atas yang bisa dijadikan referensi lokasi peserta aksi.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWilayah yang dipakai untuk menghitung jumlah peserta aksi SUDAH mempertimbangkan lebar jalan dan trotoar TETAPI mengabaikan jumlah peserta yang ada di jembatan penyeberangan ataupun peserta yang mengendong anaknya. Peta dasarnya menggunakan data hasil digitasi citra IKONOS (Sumber citra: tidak diketahui)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelain itu, perhitungan juga TIDAK mempertimbangkan separator Transjakarta, bus dan kendaraan lainnya yang parkir, gerobak penjual di sepanjang jalan yang digunakan oleh peserta aksi.\n\nAda dua nilai luaran yang dihasilkan dari perhitungan ini.\n\nPertama dengan mengasumsikan ketika peserta sedang melaksanakan sholat Jum‚Äôat, dengan area 1 meter persegi idealnya ada 2 orang yang menempati area tersebut.\nKedua ada 4 orang yang menempati area 1 meter persegi. Dalam keadaan berdiri dan tentunya peserta aksi sama sekali tidak membawa ‚Äúgembolan‚Äù (meski kecil kemungkinannya), maka 4 orang masih terbilang ideal. Tetapi mungkin tidak dalam melaksanakan sholat, karena ruangnya cukup sempit dan dipastikan ketika sujud akan nyundul bokong (males banget kan) peserta didepannya.\n\nBerdasarkan beberapa asumsi diatas, peserta aksi 2 Desember kemarin menempati area seluas 363,202 meter persegi. Kira-kira seperti gambar di bawah ini, massa menempati wilayah yang diberi warna merah.\n\n\nJika menggunakan asumsi 1 meter persegi ditempati oleh 2 orang, maka jumlah peserta aksi adalah sekitar 725,000 orang\nJika menggunakan asumsi 1 meter persegi ditempati oleh 4 orang, maka jumlah peserta aksi adalah sekitar 1,450,000 orang\nBerdasarkan https://www.gkstill.com/Support/crowd-density/CrowdDensity-1.html saya mencoba menggunakan asumsi 1 meter persegi ditempati oleh 5 orang - ini menjadi batas ATAS untuk ruang orang berdiri yang nyaman). Maka jumlah peserta aksi adalah sekitar 1,800,000 orang.\n\nBanyak yang nanya: Kamu ga ngitung orang2 yang ada di jembatan penyebarangan?\nYa tinggal itung sajalah jumlah jembatan penyebrangan di sekitar lokasi, anggap saja jumlahnya 100 jembatan (biar keliatan banyak, meski tidak mungkin kan), dan setiap jembatan ditempati 1000 orang (makin tidak mungkin kan?), dengan asumsi tersebut, jumlah jamaah di jembatan itu maksimal cuman 100,000 orang. Ga bisa dipake ngatrol menjadi tiba-tiba 7 juta.\nUnit perhitungan ini sudah mencapai ratusan ribu hingga jutaan. Penambahan massa di lokasi seperti tempat parkir tidak akan serta merta mendongkrak nilainya menjadi 7 juta.\nSupaya 7 juta, sebarannya harus seperti apa sih?\nDengan asumsi per m2 digunakan oleh 2 orang (kondisi ideal untuk sholat jamaah), massa harus memenuhi setidaknya area dalam radius 2.2 km dari titik tengah Monas, seperti gambar di bawah ini.\n\nDengan asumsi per m2 di gunakan oleh 5 orang (kondisi sangat tidak ideal), maka massa harus memenuhi setidaknya area dalam radius 1.5 km dari titik tengah monas, seperti gambar di bawah ini.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20170516-list-of-free-satellite-based-products-and-geospatial-data-on-internet.html",
    "href": "blog/20170516-list-of-free-satellite-based-products-and-geospatial-data-on-internet.html",
    "title": "List of free satellite-based products and geospatial data on internet",
    "section": "",
    "text": "This post updated regularly, so if you couldn‚Äôt find what you are looking for, comeback later or send me an email!\nLatest update: 3 Nov 2020\nDecision making to response different type of hazards requires different information. So does datasets necessary for the analysis. The data and information may not be available in one single map. This list is intended to provide data options and its use for different contexts.\nThe data vary in accuracy, resolution, frequency of update, and geographic coverage. Data summary below mainly highlights the format, methods and technicalities (links to the sources provided). Full download of the datasets is available for most sources.\n\nBaseline\nAn effective disaster management system requires collection of baseline data that is comprehensive, accurate, timely, and accessible. Without these characteristics, an effective, economical, and task- oriented disaster management system cannot be achieved. This handbook focuses on baseline data collection and impact calculation for specific disasters, which can be defined as the scope and nature of the overall informational resources needed to prepare for and respond to a variety of disasters. What is presently required is a clear understanding of the characteristics of current information resources, their functional capabilities, and their use for disaster management.\n\nBoundaries\n\nGADM\n\nReference: https://gadm.org/index.html\nFormat: Geopackage and Shapefile\nCoverage: Global, admin level 0 - 4 (availability is different for each country)\nDownload link: By country https://gadm.org/download_country_v3.html, Indonesia Shapefile: https://biogeo.ucdavis.edu/data/gadm3.6/shp/gadm36_IDN_shp.zip Geopackage: https://biogeo.ucdavis.edu/data/gadm3.6/gpkg/gadm36_IDN_gpkg.zip\nThe coordinate reference system is longitude/latitude and the WGS84 datum.\n\nCOD - Common Operational Datasets\n\nReference: https://data.humdata.org/dataset?vocab_Topics=common+operational+dataset+-+cod\nFormat: Geopackage and Shapefile\nCoverage: Global, admin level 0 - 4 (availability is different for each country)\nDownload link: By country¬† https://data.humdata.org/dataset?vocab_Topics=common+operational+dataset+-+cod, Indonesia https://data.humdata.org/dataset/indonesia-administrative-boundary-polygons-lines-and-places-levels-0-4b\n\nNatural Earth Data\n\nReference: https://www.naturalearthdata.com\nDownload link: https://www.naturalearthdata.com/downloads/10m-cultural-vectors/\n\nMarine Boundaries\n\nCoverage: World, marine boundaries and Economic Exclusive Zone\nReference: http://www.marineregions.org/sources.php#marbound\nDownload link: http://www.marineregions.org/downloads.php#marbound\nMethodology: http://www.marineregions.org/eezmethodology.php\n\n\nPopulation\nGlobal mapping of population is rapidly growing in recent years. They are available at detailed spatial scales. The analysis is based on satellite or other geospatial data layers. The specifications of each map (data) widely differ. In order to get accurate population distributions, each data should be reconstructed and synchronized. Population data are necessary for the analysis of impacts of population growth, monitor population changes, and plan interventions.\n\nLandscan\nReference: https://landscan.ornl.gov\nFormat and resolution: GeoTIFF, 1km spatial resolution\nCoverage: Global, annual from 2000 - 2018 estimates of number of people per pixel (ppp)\nDownload link: https://landscan.ornl.gov/landscan-datasets\nWorldPop\nReference: https://www.worldpop.org\nFeature: API, GIS plugin, R package, and modelling script\nFormat and resolution: GeoTIFF, 100m spatial resolution\nData coverage: Global, annual from 2000 - 2018 estimates of number of people per pixel (ppp)\nLink: The GeoTIFF images can be downloaded like so:¬†ftp://ftp.worldpop.org.uk/GIS/Population/Global_2000_2020/YYYY/ISO3/iso3_ppp_yyyy.tif Example for Indonesia, year 2020, FTP download 0.99 GB - ftp://ftp.worldpop.org.uk/GIS/Population/Global_2000_2020/2020/IDN/idn_ppp_2020.tif\nOther data product besides population: Births, Pregnancies, Urban Change, Age and sex structures, Development and Health Indicators, Dependency Ratio, Internal Migration Flow and others.\nFacebook\nReference: https://dataforgood.fb.com/tools/population-density-maps/\nFormat and resolution: GeoTIFF, 30m spatial resolution\nData coverage: Global, 2019 estimates of number of people per pixel (ppp)\nDownload link: Humanitarian Data Exchange (HDX) - https://data.humdata.org/organization/facebook\nDownload link for Indonesia:\nIDN_children_under_five_2019-06-01_csv.zip, zipped csv (111.2M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/94e808ad-9a61-426a-9fa3-60db15ad9eee/download/idn_children_under_five_2019-06-01_csv.zip\nIDN_children_under_five_2019-06-01_geotiff.zip, zipped geotiff (77.8M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/358825e9-bdac-4fa0-ae0e-07ec3f2fd1b4/download/idn_children_under_five_2019-06-01_geotiff.zip\nIDN_elderly_60_plus_2019-06-01_csv.zip, zipped csv (111.1M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/90fecfaf-8861-4715-af8f-941a1cd3ef90/download/idn_elderly_60_plus_2019-06-01_csv.zip\nIDN_elderly_60_plus_2019-06-01_geotiff.zip, zipped geotiff (77.8M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/a64ca5fb-7f43-47dd-a238-a2ff13b8e912/download/idn_elderly_60_plus_2019-06-01_geotiff.zip\nIDN_men_2019-06-01_csv.zip, zipped csv (111.2M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/2ed67ff5-5f22-44b5-89cd-1bc8133b7440/download/idn_men_2019-06-01_csv.zip\nIDN_men_2019-06-01_geotiff.zip, zipped geotiff (77.7M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/b1bf4b94-771c-44d2-8484-bad8c475621a/download/idn_men_2019-06-01_geotiff.zip\nIDN_women_2019-06-01_csv.zip, zipped csv (111.2M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/fb4f26e7-6f15-4f1e-ae14-8800890c5340/download/idn_women_2019-06-01_csv.zip\nIDN_women_2019-06-01_geotiff.zip, zipped geotiff (77.7M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/4258beb2-1579-475a-8fc2-2028289585dd/download/idn_women_2019-06-01_geotiff.zip\nIDN_women_of_reproductive_age_15_49_2019-06-01_csv.zip, zipped csv (111.2M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/5c97ebbe-e1b5-4991-9e32-154f9d1e4942/download/idn_women_of_reproductive_age_15_49_2019-06-01_csv.zip\nIDN_women_of_reproductive_age_15_49_2019-06-01_geotiff.zip, zipped geotiff (77.7M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/6620dbe5-24fd-4820-ba8b-6e5dd568b3be/download/idn_women_of_reproductive_age_15_49_2019-06-01_geotiff.zip\nIDN_youth_15_24_2019-06-01_csv.zip, zipped csv (111.1M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/64907f8a-a9d3-4763-b7cf-6c41c926cefc/download/idn_youth_15_24_2019-06-01_csv.zip\nIDN_youth_15_24_2019-06-01_geotiff.zip, zipped geotiff (77.8M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/776d4ce4-8490-4d96-8cc8-05980c7eadde/download/idn_youth_15_24_2019-06-01_geotiff.zip\npopulation_idn_2018-10-01_geotiff.zip, zipped geotiff (77.7M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/2b5f1310-ef98-44cb-b8b6-0d314add751c/download/population_idn_2018-10-01_geotiff.zip\npopulation_idn_2018-10-01.csv.zip, zipped csv (122.2M), https://data.humdata.org/dataset/0474df44-62b5-4a4c-a4fd-fd733979e2cc/resource/1951bdfe-df73-45a9-b1bb-14d72966666c/download/population_idn_2018-10-01.csv.zip\nJRC GHSL\nReference: https://ghsl.jrc.ec.europa.eu\nFormat and resolution: GeoTIFF, 38m for footprint and 250m for population grids spatial resolution\nData coverage: 2015 estimates of number of people per pixel (ppp)\nDownload link: Population grids - https://ghsl.jrc.ec.europa.eu/ghs_pop.php and Footprint - https://ghsl.jrc.ec.europa.eu/ghs_bu.php\n\n\n\nSettlement\n\nWorld Settlement Footprint (WSF) 2015\n\nAOI: Global\nReference:\n\nhttps://www.nature.com/articles/s41597-020-00580-5\nhttps://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-9628/16557_read-40454/\n\nSource: DLR\nFormat and resolution: GeoTIFF, 10m spatial resolution\nDownload Link: WSF2015 can be downloaded from a dedicated repository\n\n10m https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015/10048412\n100m https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015_-_Percent_Settlement_Area_-_100m/100484750\n250m https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015_-_Percent_Settlement_Area_-_250m/10048514\n500m https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015_-_Percent_Settlement_Area_-_500m/10048520\n1km https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015_-_Percent_Settlement_Area_-_1km/10048535\n10km https://figshare.com/articles/dataset/World_Settlement_Footprint_WSF_2015_-_Percent_Settlement_Area_-_10km/10048541\n\n\n\n\nClimate\n\nPrecipitation\nMany research institution are producing global satellite precipitation estimate, and mostly available for public. Most notably products are CHIRPS, TRMM, CMORPH, GPM and GSMaP.\n\nCHIRPS\n\nCoverage: Global\nReference: https://www.chc.ucsb.edu/data/chirps\nFormat and resolution: GeoTIFF and NetCDF, 0.05 deg ~ 5.6 km spatial resolution\nData range: 1 Jan 1981 - present\nDownload link:\nDaily - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/\nPentad - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_pentad/\nDekad - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_dekad/\nMonthly 1 - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/\nMonthly 2 - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_2-monthly/\nMonthly 3 - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_3-monthly/\nAnnual - https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_annual/\n10-days Forecast - https://data.chc.ucsb.edu/products/EWX/data/forecasts/CHIRPS-GEFS_precip/10day/\nPreliminary data - https://data.chc.ucsb.edu/products/CHIRPS-2.0/prelim/\n\nIMERG\n\nCoverage: Global\nSource: NASA IMERG (https://pmm.nasa.gov/data-access/downloads/gpm)\nFormat and resolution: NetCDF4, 0.1 deg ~ 10 km spatial resolution\nData range: 12 Mar 2000 - present\nLink:\n30-min Final Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGHH.06/\n30-min Early Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGHHE.06/\n30-min Late Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGHHL.06/\nDaily Final Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGDF.06/\nDaily Early Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGDE.06/\nDaily Late Released https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGDL.06/\n\n\n\nHydrographic\nRiver\n\nHydroshed River\nCoverage: Global\nSource: https://hydrosheds.org\n\nFormat and resolution: Shepfile, 15 sec ~ 500 m spatial resolution\nLink: https://hydrosheds.org/downloads\nLake\n\nHydroshed Lake\nCoverage: Global\nSource: https://hydrosheds.org/pages/hydrolakes\n\nFormat and resolution: Shepfile and GDB\nLink:\nPolygon: shp (782 MB zip-file) - https://97dc600d3ccc765f840c-d5a4231de41cd7a15e06ac00b0bcc552.ssl.cf5.rackcdn.com/HydroLAKES_polys_v10_shp.zip and gdb (727 MB zip-file) - https://97dc600d3ccc765f840c-d5a4231de41cd7a15e06ac00b0bcc552.ssl.cf5.rackcdn.com/HydroLAKES_polys_v10.gdb.zip\nPour points: shp (782 MB zip-file) - https://97dc600d3ccc765f840c-d5a4231de41cd7a15e06ac00b0bcc552.ssl.cf5.rackcdn.com/HydroLAKES_points_v10_shp.zip and gdb (78 MB zip-file) - https://97dc600d3ccc765f840c-d5a4231de41cd7a15e06ac00b0bcc552.ssl.cf5.rackcdn.com/HydroLAKES_points_v10.gdb.zip\nWater Body\n\nSRTM Water Body\n\nCoverage: Global\nSource: SRTM Water Body (http://dds.cr.usgs.gov/srtm/version2_1/SWBD/SWBD_Documentation/Readme_SRTM_Water_Body_Data.pdf)\nFormat: Shapefile in zip-file\nLink: https://dds.cr.usgs.gov/srtm/version2_1/SWBD/\n\nJRC Global Surface Water\n\nCoverage: Global\nSource: JRC Global Surface Water (https://www.nature.com/articles/nature20584.epdf?author_access_token=C5JSvooRop4jWxyp_qRPLNRgN0jAjWel9jnR3ZoTv0MqBuzCNsmw_DFxRd7sX93nfPzcbm_xTiPLlZMl7XrUhadm6EiT9cGdDNgn1s6EWrPWH3IeadLUjApplBoaS6xH)\nFormat and resolution: GeoTIFF, 30m spatial resolution, based on Landsat\nData coverage: From 80¬∞ north to 60¬∞ south. It is divided into tiles of 10 x 10 degrees. 1984-2015.\nData contents: Water occurrence, change, seasonality, recurrence, transitions and maximum extent\nLink: https://global-surface-water.appspot.com/download\n\nUSGS Global Surface Water\n\nAOI: Global\nSource: USGS Global Surface Water (https://science.sciencemag.org/content/342/6160/850.abstract)\nFormat and resolution: GeoTIFF, 30m spatial resolution, based on Landsat\nData coverage: From 80¬∞ north to 60¬∞ south. It is divided into tiles of 10 x 10 degrees. 2000 - 2012.\nLink: Bulk download - https://edcintl.cr.usgs.gov/downloads/sciweb1/shared/gtc/downloads/WaterMask2010_UMD.zip or as Individual tiles - https://edcintl.cr.usgs.gov/downloads/sciweb1/shared/gtc/downloads/WaterMask2010_UMD_individual/\n\n\nNatural Hazards\nFire Hotspot\n\nMODIS and VIIRS Hotspot\n\nCoverage: Global, 2000 - now\nSource: NASA FIRMS\nFormat: shp, csv, json\nLink: https://firms.modaps.eosdis.nasa.gov/download/\n\nATSR\nCoverage: Global, 1995 - 2012\nSource: ESA ATSR\nFormat: ASCII, http://due.esrin.esa.int/page_wfaformats.php\nLink: http://due.esrin.esa.int/page_wfa.php\nEarthquake\n\nHistorical earthquake events also available for download from USGS website: https://earthquake.usgs.gov/earthquakes/search/\nShakeMap is available from USGS website https://earthquake.usgs.gov/data/shakemap/ and user is required to choose the earthquake event from the list and if not available, usually can be found through ‚ÄúSearch ShakeMap archive‚Äù.\n\nLandslide\n\nCoverage: Global\nReference: https://maps.nccs.nasa.gov/arcgis/apps/webappviewer/index.html?id=824ea5864ec8423fb985b33ee6bc05b7\nDownload link: https://maps.nccs.nasa.gov/arcgis/apps/MapAndAppGallery/index.html?appid=574f26408683485799d02e857e5d9521\n\nLand Susceptibility\nCoverage: Global\nReference: https://earthobservatory.nasa.gov/images/89937/a-global-view-of-landslide-susceptibility and https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20170005840.pdf\nFormat: GeoTIFF, 1km spatial resolution\nDownload link: https://gpm.nasa.gov/sites/default/files/downloads/global-landslide-susceptibility-map-1-30-20.zip\n\n\n\nOpenStreetMap\n\nGlobal\n\nReference: OSM via GeoFabrik\nFormat and update: shapefile in zip-file, daily update\nFeature content:\n\nPoint Features: places; point of interest; places of worship; natural features; traffic related; transport infrastructure\nLine Features: road and paths; railways, subways, trams, lifts, and cable cars; waterways\nPolygon Features: building outlines; land use and land cover; bodies of water\n\nLink: Asia - http://download.geofabrik.de/asia.html, Indonesia - http://download.geofabrik.de/asia/indonesia-latest-free.shp.zip\n\nSurabaya\n\nSource: Perkumpulan OSM Indonesia (https://openstreetmap.id/data-surabaya/)\nFormat: shapefile, osm, geopackage\nFeature content: Batas administrasi, bangunan, jalan, infrastruktur\nLink:\n\nBatas administrasi (shp) https://drive.google.com/open?id=1ARvJ_aEA_rg_zh6x_zuI8Zx8wIb7rPwH\nBatas administrasi (osm) https://drive.google.com/open?id=1q0eQ9ctEv6KmLkw-8K4Id07W5bEOU4J4\nBatas administrasi (gpkg) https://drive.google.com/open?id=1bhU7U4jiynEjMyRW2qPvBeenaLrqiR5h\nBangunan (osm) https://drive.google.com/open?id=18ZgH78G4ridroPcfTOQsAF9OKqEoJmzG\nJalan (osm) https://drive.google.com/open?id=1yDljp7q87lLQ4GnR0HxZe3i72hICErCm\nInfrastruktur keseluruhan (shp) https://drive.google.com/open?id=1STnTZ2tljAIHzI_mbvqXu9boH5k0H6_K\nInfrastruktur terperinci:\nFasilitas ekonomi (gpkg) https://drive.google.com/open?id=1tM97pQW0mumWWWFdldOKX_TO1_OG5FgJ\nInfrastruktur komunikasi (gpkg) https://drive.google.com/open?id=1emY-3jNkskaNMp9rUIPRbhy6foRgedWO\nFasilitas pendidikan (gpkg) https://drive.google.com/open?id=1tghyX-Qgt2k8Fy3IVL-WbzR15YtZ3HA4\nLayanan kedaruratan (gpkg) https://drive.google.com/open?id=1eK4HMSC25km-j41GFkYOqBoBYFQ1EId-\nStasiun pengisian bahan bakar umum (gpkg) https://drive.google.com/open?id=1y24hCns8UlTY_YWH5hvMVDeCtcAKdVJr\nKantor pemerintah (gpkg) https://drive.google.com/open?id=1TLSdnbHJAMdPwNtupOP7QlkCjgpBjdWo\nFasilitas kesehatan (gpkg) https://drive.google.com/open?id=1HmcoR-HhD6ptLyTVTTQdNmup2-PrVU9L\nInfrastruktur kelistrikan (gpkg) https://drive.google.com/open?id=1fgw5phV_RFVUz1x24DhOSFN2_eYVeqeL\nTempat ibadah, fasilitas olahraga dan ruang publik (gpkg) https://drive.google.com/open?id=1KMHbjIG9Q_P0NtAC9IzTQWtxo2I65hIK\nSarana transportasi (gpkg) https://drive.google.com/open?id=1HzwN-DgYDQzgS20IA8UjPWluD1Wxy_Tw\nSarana perairan (gpkg) https://drive.google.com/open?id=1174Nb0j3xSLiKxONtTWik6qbuH5P8k8w\nJakarta\n\nSource: Perkumpulan OSM Indonesia (https://openstreetmap.id/data-dki-jakarta/)\nFormat: shapefile, osm, geopackage\nFeature content: Batas administrasi, bangunan, jalan, infrastruktur\nLink:\n\nBatas administrasi DKI (osm) https://drive.google.com/open?id=18HMJ62_bmLqDcn4hnJ1M8Ujt3f2xHZJB\nBatas administrasi kotamadya (gpkg) https://drive.google.com/open?id=1L64PZxK_8XTeu_HVorBYTZcbz35jgCLB\nBatas administrasi kecamatan (gpkg) https://drive.google.com/open?id=1iAxEwK7Z1c71nIhH6rb8OG2u8XTXf6El\nBatas administrasi kelurahan (gpkg) https://drive.google.com/open?id=18N6_1o5FR1wlBkubrzm-ZTnRMUleUqCe\nBatas administrasi rw (gpkg) https://drive.google.com/open?id=1iRM1AcnoRpY3ItqxWMwfs7twXOqWe1ia\nBatas administrasi rt (gpkg) https://drive.google.com/open?id=1ZKITXqDOa277Sd5KWDvZUyCljHOeB6Ub\nBangunan (shp) https://drive.google.com/open?id=1LvWXH9Vuv76R0PL9AwY-hVt-Ixg9dqGu\nJalan (shp) https://drive.google.com/open?id=1c00zYz0mtv14b8K87JUFvJVf-WxRANL4\nInfrastruktur keseluruhan (shp) https://drive.google.com/open?id=1MRA-NhAGohWVo0snlkymr99JYYUIenb-\nInfrastruktur terperinci:\nFasilitas ekonomi (shp) https://drive.google.com/open?id=1oSSblEWaUxXDgRARpIBa8hV5-3nOd5eI\nInfrastruktur komunikasi (shp) https://drive.google.com/open?id=110XxbarNPznlqtKKl_vzNTtHwAKwH6F-\nFasilitas pendidikan (shp) https://drive.google.com/open?id=1RQYdWS2zYQIdOy7BXGhUBLpq6cuGsF1Y\nLayanan kedaruratan (shp) https://drive.google.com/open?id=154vq9jpAH7P46abjzvnuHSKH7eHtDuRU\nStasiun pengisian bahan bakar umum (shp) https://drive.google.com/open?id=1DCRFhXCqIrtjvoR3XaEc_DnHwUKddk8r\nKantor pemerintah (shp) https://drive.google.com/open?id=1uc3UqVe4XXEepNZeTtc-kCCmQaVf5QXt\nFasilitas kesehatan (shp) https://drive.google.com/open?id=1azUAetAfVKHmkh8MdBnZv6owD-jzvBkd\nInfrastruktur kelistrikan (shp) https://drive.google.com/open?id=1E-n34SPkon9_tWa3H7DwEsrfRH4XMcMD\nTempat ibadah, fasilitas olahraga dan ruang publik (shp) https://drive.google.com/open?id=1gkA503jsLdtxLBfOfztGKUwbB9j92E8-\nSarana transportasi (shp) https://drive.google.com/open?id=1fSjRH71xbOWG8nT8--9raJyPTa4paEwr\nSarana perairan (shp) https://drive.google.com/open?id=1LIWCT63eRT_357u51sluRfjBQCo-EJLF\nSemarang\n\nSource: Perkumpulan OSM Indonesia (https://openstreetmap.id/data-semarang/)\nFormat: shapefile, osm, geojson, geopackage\nFeature content: Batas administrasi, bangunan, jalan, infrastruktur\nLink:\n\nBatas administrasi (shp) https://drive.google.com/open?id=1bGw7PgBwDf4C3sjX4elNWSbRBJMW9Jrp\nBatas administrasi (geojson) https://drive.google.com/open?id=1m2C1_dxiphOC9YDLQhsKKJ_FqVZqhiGh\nBatas administrasi kecamatan (geojson) https://drive.google.com/open?id=1ZGTjT-iqqUCdSKE-3b4LZ5-ezaLM7QFE\nBatas administrasi kelurahan (geojson) https://drive.google.com/open?id=1O2VMlY9LeNT49RsqFhsEL-E79nN_A2-w\nBatas administrasi rw (geojson) https://drive.google.com/open?id=1-sO0Em3hFSnrdkX_-BgpKXBZxP50ZLZX\nBangunan (shp) https://drive.google.com/open?id=1xGBYdW54vPIp-BE81UwVad2AVBC8owcV\nJalan (shp) https://drive.google.com/open?id=1By3_FwPaLOos0Xvo1SpBYQoAlUiX2gv5\nInfrastruktur keseluruhan (shp) https://drive.google.com/open?id=1Y0yF4RmKC-ZantMyPSKXimqJZQUfVJIs\nInfrastruktur terperinci:\nFasilitas ekonomi (gpkg) https://drive.google.com/open?id=1ItjIE7KA5efo8tVn1ZWhn_ibUu9XBm7z\nInfrastruktur komunikasi (gpkg) https://drive.google.com/open?id=16z5gPvkFjbdrMuWwt-oCZkcjMy5Z3OD7\nFasilitas pendidikan (gpkg) https://drive.google.com/open?id=1261mqijxkJZvdtlQIDpBoT-W24ySMa6X\nLayanan kedaruratan (gpkg) https://drive.google.com/open?id=1mvF5ifakBEsO744ckFdHuZmwRn-nAk27\nStasiun pengisian bahan bakar umum (gpkg) https://drive.google.com/open?id=1CRb59034Xtm_SD_AE2t2Rv6mTnP6X3W4\nKantor pemerintah (gpkg) https://drive.google.com/open?id=1Q-MjF3TBB_Dc4Av7t_wNoNwO60DVa5OC\nFasilitas kesehatan (gpkg) https://drive.google.com/open?id=1JP2YvK3KGFY-LpNzo_1sGnrEeZqqfdSj\nInfrastruktur kelistrikan (gpkg) https://drive.google.com/open?id=1I1TrQ4ACoryKs5yHFbLgQL1r8R4On6ou\nTempat ibadah, fasilitas olahraga dan ruang publik (gpkg) https://drive.google.com/open?id=1ct9NqJwLTl4vp-RqgzgP_l2QkKorvwkV\nSarana transportasi (gpkg) https://drive.google.com/open?id=1bKP9pEPr6ii0O3j1Lkx-7n_mGbZObeZu\nSarana perairan (gpkg) https://drive.google.com/open?id=18HAyKbBz8IQaDytbFiXw1ExfuAHQZy8W\n\n\n\nPhysical\nLandcover\nLand cover maps represent spatial information on different types (classes) of physical coverage of the Earth‚Äôs surface, e.g.¬†forests, grasslands, croplands, lakes, wetlands. Dynamic land cover maps include transitions of land cover classes over time and hence captures land cover changes. Land use maps contain spatial information on the arrangements, activities and inputs people undertake in a certain land cover type to produce, change or maintain it.\n\nCopernicus Global Landcover Services\n\nCoverage: Global\nBlog: https://blog.vito.be/remotesensing/lcviewer\nSource: https://land.copernicus.eu/global/products/lc\nFormat and resolution: GeoTIFF in zip-file, 100m spatial resolution\nDownload link: Tile for Indonesia\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E080N20_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E100N20_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E120N20_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E080N00_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E100N00_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E120N00_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\nhttps://s3-eu-west-1.amazonaws.com/vito-lcv/2015/ZIPfiles/E140N00_ProbaV_LC100_epoch2015_global_v2.0.1_products_EPSG-4326.zip\n\nESA CCI Annual Landcover\n\nCoverage: Global\nSource: ESA Climate Change Initiative https://www.esa-landcover-cci.org/?q=node/197\nFormat and resolution: NetCDF, 300m spatial resolution, annual 1992 - 2018\nDownload Link: https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=overview\n\nGlobCOVER\n\nCoverage: Global\nSource: ESA GlobCover (http://due.esrin.esa.int/page_globcover.php)\nFormat and resolution: GeoTIFF in zip-file, 300m spatial resolution\nLink: http://due.esrin.esa.int/files/Globcover2009_V2.3_Global_.zip\n\nBare Land\n\nCoverage: Global\nSource: GLAD UMD (https://glad.geog.umd.edu/dataset/global-2010-bare-ground-30-m)\nFormat and resolution: GeoTIFF, 30m spatial resolution, based on Landsat\nData coverage: From 80¬∞ north to 60¬∞ south. It is divided into tiles of 10 x 10 degrees.\nLink: https://glad.umd.edu/Potapov/Bare_2010/\n\nTree Cover and Loss\n\nCoverage: Global\nSource: http://earthenginepartners.appspot.com/science-2013-global-forest\nFormat and resolution: GeoTIFF, 30m spatial resolution, based on Landsat\nData coverage: From 80¬∞ north to 60¬∞ south. It is divided into tiles of 10 x 10 degrees.\nLink: http://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.7.html\n\nCropland based on MODIS\n\nCoverage: Global\nSource: GLAD UMD (https://glad.geog.umd.edu/dataset/gce/global-cropland-extent)\nFormat and resolution: GeoTIFF, 250m spatial resolution, based on MODIS\nData coverage: Each tile is 4800x4800 in MODIS Sinusoidal projection with a pixel size of 231.65635m. A map of MODIS tiles can be found here - https://glad.geog.umd.edu/croplands/sinusoidal.gif\nLink: Cropland Probability - https://glad.geog.umd.edu/dataset/gce/250mprob and Discrete Cropland/Non-Cropland https://glad.geog.umd.edu/dataset/gce/modis-global-crop-extent-discrete-croplandnot-cropland-data\n\nCropland based on Landsat\n\nCoverage: Global\nSource: USGS GFSAD (https://www.usgs.gov/centers/wgsc/science/global-food-security-support-analysis-data-30-m?qt-science_center_objects=0#qt-science_center_objects)\nFormat and resolution: GeoTIFF, 30m spatial resolution, based on Landsat\nLink: Southeast Asia - https://lpdaac.usgs.gov/products/gfsad30seacev001/\nTerrain\n\nDEMNAS\n\nCoverage: Indonesia\nSource: http://tides.big.go.id/DEMNAS/ - registration required before downloading the data.\nFormat and resolution: GeoTIFF, 8m spatial resolution\nLink:\nSumatera - http://tides.big.go.id/DEMNAS/Sumatera.php\nKalimantan - http://tides.big.go.id/DEMNAS/Kalimantan.php\nSulawesi - http://tides.big.go.id/DEMNAS/Sulawesi.php\nJawa dan Bali - http://tides.big.go.id/DEMNAS/Jawa.php\nNusa Tenggara - http://tides.big.go.id/DEMNAS/Nusa_tenggara.php\nMaluku - http://tides.big.go.id/DEMNAS/Maluku.php\nPapua - http://tides.big.go.id/DEMNAS/Papua.php\nBATNAS - http://tides.big.go.id/DEMNAS/Batnas.php\n\nSRTM\n\nCoverage: Global\nSource: NASA SRTM (https://earthdata.nasa.gov/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia)\nFormat and Resolution: GeoTIFF, 30m and 90m spatial resolution\nLink: 30m - http://dwtkns.com/srtm30m/ and 90m - http://dwtkns.com/srtm/\n\nASTER GDEM\n\nCoverage: Global\nSource: ASTER GDEM (https://asterweb.jpl.nasa.gov/gdem.asp)\nFormat and Resolution: GeoTIFF, 30m spatial resolution\nLink: https://search.earthdata.nasa.gov/search/granules/collection-details?p=C197265171-LPDAAC_ECS&q=ASTER%20GDEM&ok=ASTER%20GDEM\n\nNASADEM\nCoverage: Global\nReference: https://earthdata.nasa.gov/esds/competitive-programs/measures/nasadem\nForemat and resolution: HGT and NetCDF, 30m spatial resolution\nLink: https://lpdaac.usgs.gov/news/release-nasadem-data-products/\nTools for download the data: https://github.com/dshean/nasadem\nCopernicus Digital Elevation Model 30\n\nAOI: Global\nNews: https://bit.ly/3kEnXem\nSource: Copernicus\nFormat: GeoTIFF and DTED\nResolution: Europe 10m, Global 30 and 90m\nDownload link: https://spacedata.copernicus.eu/fr/dataset-details?articleId=394198\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200409-historical-flood-occurrence.html",
    "href": "blog/20200409-historical-flood-occurrence.html",
    "title": "Historical flood occurrence",
    "section": "",
    "text": "Inspired from Dr.¬†Ate Poortinga‚Äôs work in extracting water occurrence from satellite image, I tried to do similar analysis to get historical average of flood occurrence by month. The result from this analysis will be added as a probability information to enhance WFP‚Äôs VAMPIRE flood alert system (Extreme rainfall-triggering flood) that currently I am working on.\nThe steps below describe how to extract monthly water occurrence and seasonality water using Google Earth Engine code editor.\nStep 1. Define geographic domain\nStep 2. Import JRC Global Surface Water and extract permanent water from Seasonality layer\nStep 3. Define study period (example: extract January data from 1984 - 2018)\nStep 4. Detect all observation data with water\nStep 5. Calculate flood occurrence\nStep 6. Visualisation and add map to canvas\nStep 7. Export the result to Google Drive\nStep 8. Setting legend\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. Google Earth Engine user interface [/caption]\nFull script here\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20101215-using-google-to-disseminate-information-during-2010-tsunami-in-mentawai-islands.html",
    "href": "blog/20101215-using-google-to-disseminate-information-during-2010-tsunami-in-mentawai-islands.html",
    "title": "Using Google to disseminate information during 2010 Tsunami in Mentawai Islands",
    "section": "",
    "text": "Information Management in Emergency Situation\n¬†During post-disaster emergency response, the role of information management is highly important. Data and information exchange happens very fast, and everybody needs all kinds of information in regard with the disaster. Most often, up-to-date and reliable information are scarce at the beginning of emergency response. Good Information Management is very crucial to ensure timely dissemination of information. The information management steps that should be followed include collection, processing, analysis and dissemination of data and information.\n\n\nDissemination information using free tools from Internet\n\nWhy Choose Google?¬†Google is very popular, free and powerful\nWe must have a¬†Google Account¬†first before using the product from google, this account can be used to access all the Google products. The followings are products from Google that are used to support the Information Management Unit in terms of Response for Tsunami and Earthquake in Mentawai Islands on 25 October 2010:\n\nGmail\nGoogle Sites\nGoogle Code\nGoogle Docs: Documents, Spreadsheets, Presentations, Drawings and Forms\nGoogle Calendar\nGoogle Groups\nGoogle Translate\nGoogle Analytics\nGoogle Webmaster Tools\nGoogle Earth\nGoogle Maps\nGoogle Map Maker\nGoogle Apps\nYouTube\nPicasa Web Album\n\nGoogle Sites:¬†Mentawai Response website (http://www.mentawairesponse.org) developed using Google Sites as a primary tool for disseminating information. All Google products that we used integrated in one tool in Google Sites.\n\nÔªøGmail:¬†Gmail is a free, advertising-supported webmail, POP3, and IMAP[1] service provided by Google. Gmail is used for disseminating information by email and also tool for communicating with other humanitarian actors.\n\nGoogle Docs:¬†All data and information related to the disaster are stored in Google Docs (Material presentations, Minute of Meeting, Technical Reference, Tabular data: Humanitarian Impacts ‚Äì Distributed Relief Materials ‚Äì Population data ‚Äìetc). It allows users to create and edit documents online while collaborating in real-time with other users. Google Docs combines the features of Documents and Spreadsheets with a presentation program. Data storage of any files up to 1GB each in size.\n \nGoogle Code:¬†Google Code is Google‚Äôs site for developer tools, APIs and technical resources. Google Sites does support custom gadgets feature that can be added to websites hosted on Google Sites. Gadget is essentially a programming interface with custom code that can be embedded and used to provide content to iGoogle homepage and websites hosted on Google Sites. As a workaround, gadgets can be used to add, include or insert JavaScript (JS) code and other content that is otherwise prohibited.\nIn Mentawai Response website, we used some social networks and other plugins to promote our website in International and National Humanitarian forum i.e.¬†Facebook, Twitter, Whos Amung Us and AddThis Analytics. All these plugins contain Javascript in their components, so it does not work if included in the Google Sites, to overcome this, a simple XML script was created using Google Gadgets Editor which was accessible on Google Code.\n\nOn first login, an example code of ‚ÄúHello World‚Äù will be displayed, as show below:\n\n\n\n\n\n\n\nRemove the line of¬†Hello, world!¬†and replace it with the custom code.\nName the gadget by changing the¬†hello world example¬†in the line of¬†ModulePrefs.\n\n\nÔªøGoogle Calendar:¬†All Mentawai Response meeting events are stored online, meaning that the calendar can be viewed from any location that has Internet access. Multiple calendars can be added and shared, allowing various levels of permissions for the users.\n\nGoogle Groups:¬†We used this tool for supporting exchange of information amongst humanitarian actors through mailing list.\n\nGoogle Translate:¬†Whenever someone visits a Mentawai Response website in another language, they will be given the option to translate the content into the language of their choice. All they have to do is click on the translate link at the bottom right-hand side of the page. Now, the content on your site can be translated into 51 languages. Google Sites had integrated with the Google Translate Elements.\n\nGoogle Analytics:¬†It‚Äôs is a free service offered by Google that generates detailed statistics about the visitors to a website. Since Mentawai Response website launched, 200 people visited this website every day.\n\nGoogle Earth, Google Maps, Google Map Maker:¬†It is a combination of 3 tools to create an online map with input data from text or KML.\n\nGoogle Apps:¬†It is a service from Google providing independently customizable versions of several Google products under a custom domain name. It features several Web applications with similar functionality to traditional office suites, including: Gmail, Google Groups, Google Calendar, Talk, Docs and Sites.\n\nYouTube:¬†Free video sharing Web site which lets users upload, view, and share video clips.\n\nPicasa Web Album:¬†Picasa Web Albums (PWA) is a photo sharing web site from Google, often compared to Flickr and similar sites. It allows users with accounts at Google to store and share 1 GB of photos for free.\n\n\n\nMentawai Response: website content\n1.¬†¬†¬†¬†¬†Updates from Sikakap\nThe ‚Äúupdates from Sikakap‚Äù will provide concise information directly from Sikakap since 4 November ‚Äì 11 November 2010.\nhttp://www.mentawairesponse.org/updates-from-sikakap\n2.¬†¬†¬†¬†¬†Information Management\nInformation Management is a resource designed to assist Humanitarian Affairs professionals in managing information to support their work in ensuring better humanitarian planning and response. It consists of 3W, Contact Directory and Humanitarian Impacts.\nhttp://www.mentawairesponse.org/information-management\na.¬†¬†¬†¬†¬†3W\nThe Who does What Where database (3W) is providing more information on their activities respectively in terms of Mentawai Response.\nhttp://www.mentawairesponse.org/information-management/3w\nb.¬†¬†¬†¬†¬†Contact Directory (Agency contact list and sector support lead list in Padang and Mentawai)\n¬†¬†¬†¬†¬†¬†Contact lists provide the name, organization, location, e-mail and telephone number for humanitarian actors working in an operational environment in emergencies phase. Contacts are organized and provided to user and the lists are a simple yet useful coordination tool.\nhttp://www.mentawairesponse.org/information-management/contact-directory\nc.¬†¬†¬†¬†¬†Humanitarian Impacts\nHumanitarian impacts summarize casualties and damages caused by disaster in figures.\nhttp://www.mentawairesponse.org/information-management/humanitarian-impact\n3.¬†¬†¬†¬†¬†Map Centre\nThe Map Centre provides you with access to the complete set of Mentawai Islands maps produced and provided by OCHA and its partners. The page consists of Online Maps, Reference Maps and Situation Maps.\nhttp://www.mentawairesponse.org/map-centre\na.¬†¬†¬†¬†¬†Online Maps\nhttp://www.mentawairesponse.org/map-centre/online-maps\nb.¬†¬†¬†¬†¬†Reference Maps\nhttp://www.mentawairesponse.org/map-centre/reference-maps\nSource map: DLR, OCHA\nc.¬†¬†¬†¬†¬†Situation Maps\nhttp://www.mentawairesponse.org/map-centre/situation-maps\nSource map: DLR, OCHA, ECJRC, IFRC, PDC, US HIU and WFP\n4.¬†¬†¬†¬†¬†Sector Info\nIn general, the Sector Info page concludes information on sector support. To date, health and logistics were accommodated mainly.\nhttp://www.mentawairesponse.org/sectors-info\na.¬†¬†¬†¬†¬†Health\nb.¬†¬†¬†¬†¬†Logistics\n5.¬†¬†¬†¬†¬†Meetings\nThe Meeting page concludes information on minutes of meeting and its schedule conducted in Padang or Mentawai in various sectors (agriculture, coordination, logistics and emergency coordination) involved.\nhttp://www.mentawairesponse.org/meetings\na.¬†¬†¬†¬†¬†Meeting Minutes\nhttp://www.mentawairesponse.org/meetings/meeting-minutes\nMentawai: Agriculture, Coordination, Emergency Telecommunication\nPadang: Coordination, Logistics\nb.¬†¬†¬†¬†¬†Meeting Schedules\nhttp://www.mentawairesponse.org/meetings/meeting-schedule\n6.¬†¬†¬†¬†¬†Documents\nThe documents page concludes all pertinent documents/reports provided by partners along with related field map production, and regulation and law.\nhttp://www.mentawairesponse.org/documents\na.¬†¬†¬†¬†¬†Situation Report\nhttp://www.mentawairesponse.org/documents/situation-reports\nCaritas-Karina, CFK, CWS, HOPE, IBU, IFRC, Surfaid International, WHO, WVI\nb.¬†¬†¬†¬†¬†Field Map Production\nhttp://www.mentawairesponse.org/documents/field-map-production\nc.¬†¬†¬†¬†¬†Regulation and Law\nhttp://www.mentawairesponse.org/documents/regulation-and-law\n7.¬†¬†¬†¬†¬†Surveys\nDuring a humanitarian crisis a large number of assessments and surveys are carried out. The following displays report on assessment (form and report) had been conducted to date.\nhttp://www.mentawairesponse.org/surveys\na.¬†¬†¬†¬†¬†Report/Assessments\nhttp://www.mentawairesponse.org/surveys/report-assessments\nAssessment Form, Assessment Data, Assessment Report\nb.¬†¬†¬†¬†¬†Survey Tools\nhttp://www.mentawairesponse.org/surveys/survey-tools\n8.¬†¬†¬†¬†¬†Media Centre\nMedia Centre page concludes information in pictures, consists of Photo Gallery and Press Releases.\nhttp://www.mentawairesponse.org/media-centre\na.¬†¬†¬†¬†¬†Photo Gallery\nhttp://www.mentawairesponse.org/media-centre/photo-gallery\nhttp://picasaweb.google.com/mentawairesponse\nb.¬†¬†¬†¬†¬†Press Releases\nhttp://www.mentawairesponse.org/media-centre/press-releases\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240503-skip-pearson-fitting-on-climate-indices-python-package.html",
    "href": "blog/20240503-skip-pearson-fitting-on-climate-indices-python-package.html",
    "title": "Skip PEARSON fitting on climate-indices python package",
    "section": "",
    "text": "Source: https://gist.github.com/bennyistanto/e8710f89bfbebaf24498dd957a1fa961\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20190914-innovation-in-humanitarian-crisis-meeting.html",
    "href": "blog/20190914-innovation-in-humanitarian-crisis-meeting.html",
    "title": "Innovation in humanitarian crisis meeting",
    "section": "",
    "text": "Last week I went to Rome for a workshop and a task force meeting. In the 1st day, I attend workshop on Innovation in Humanitarian Crisis: an integrated approach to innovative tools for food security analysis and response in emergency settings. I deliver a presentation on Linking remote sensing technology, socio-economic vulnerability data and field-collected information to measure risk and impact using VAMPIRE+PRISM.\nIn day 2-4, I joined a task force meeting on Automated Livelihood Information Assistant (ALIA). The idea behind ALIA was generated from WFP programme officers who had several years of experience preparing for, responding to and recovering from climate-related disasters and understood the information gaps in place, and how enhanced predictive analytics could be harnessed to make WFP‚Äôs responses more effective.\nLast day was a meeting with VAM HQ on seasonal monitor and future product.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the workshop, spending time with friends sightseeing Rome, try the best Gelato and Pizza, dinner like Roman. üòÇ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also manage to visit Ruins of Pompeii in Napoli on Saturday. My flight back to Jakarta are scheduled to depart on Saturday midnight, so it was perfect time for short visit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200706-calculate-spi-using-imerg-data.html",
    "href": "blog/20200706-calculate-spi-using-imerg-data.html",
    "title": "Calculate SPI using IMERG data",
    "section": "",
    "text": "NOTES - 1 Mar 2022\nI have write a new and comprehensive guideline on SPI, you can access via this link https://bennyistanto.github.io/spi/\n\nI have completed step-by-step guideline written in Jupyter Notebook , here‚Äôs the link https://github.com/wfpidn/SPI/blob/master/SPI_based_on_IMERG.ipynb and last week was my first time using Jupyter Notebook. I found some errors in bullets and numbering when displaying the notebook in Github, but seems ok when open it using Jupyter directly. Sorry for that.\nBelow maps are example product for SPI 1 to 72-month, you can find more information about SPI and the application through below notebook.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20120611-first-day-at-wfp.html",
    "href": "blog/20120611-first-day-at-wfp.html",
    "title": "First day at WFP",
    "section": "",
    "text": "Today is my first day working at World Food Programme, Indonesia Country Office and based in Jakarta. I will work under Vulnerability Analysis and Mapping Unit as a GIS Programme Officer. I am happy moving back to Jakarta after spend almost 6 years working in the field.\nSince few months ago I am looking for a new job in Jakarta because my wife will back home soon from the Netherlands. I think it was a good reason to restart my life together.\nSince morning I am dealing with administrative issue and some mandatory training for new UN staff. I am really excited to start my work, as I will utilized my expertise on climate analytics, combined with GIS on food security issue.\nWish me luck!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031125-mekanika-gerak-dalam-dua-dimensi.html",
    "href": "blog/20031125-mekanika-gerak-dalam-dua-dimensi.html",
    "title": "Mekanika - gerak dalam dua dimensi",
    "section": "",
    "text": "Makalah berikut merupakan tugas dari mata kuliah Mekanika di semester 4, dibimbing oleh Hanedi Darmasetiawan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20101101-2010-mentawai-tsunami-and-earthquake.html",
    "href": "blog/20101101-2010-mentawai-tsunami-and-earthquake.html",
    "title": "2010 Mentawai Tsunami and Earthquake",
    "section": "",
    "text": "25 October 2010 - 21:42, that night I was in my room (Agus Salim stadium complex) working on map requested by Ignacio Leon - Head of OCHA Indonesia. He asked me to create some infographic about each natural disaster and it‚Äôs impact in Indonesia.\nWhen the shock occurs, I grab my go-bag and bicycle and spur fast towards Alai Market. But unfortunately traffic jam and extreme panic had occurred when I passed Padang Baru intersection.\nThen, the day after was a very busy day for me and team, coordinating data, analysis and information management to support the response. For first action, I create disaster event and general information map using the best available data that I can found from internet, to help government counterpart and humanitarian actors understand on the situation.\n\nDisaster event map\n \n\n\nGeneral administration map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230811-spi-based-drought-characteristics.html",
    "href": "blog/20230811-spi-based-drought-characteristics.html",
    "title": "SPI-based drought characteristics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blog/20060812-numerical-random-search.html",
    "href": "blog/20060812-numerical-random-search.html",
    "title": "Numerical random search",
    "section": "",
    "text": "In meteorological station, air humidity measurement often use hygrometer which can give direct value in RH unit (%), but many meteorological station did not has this feature, so they use temperature measurement to calculate air humidity data.\nAir humidity calculation using temperature data also give some problem about the data it self. Many meteorological stations have been recording temperature data in the minimum and maximum temperature daily only. So, if we want to know the variation of humidity every day, we might not get it, because we don‚Äôt have temperature data based on the time of day (for example per hour or per ten minutes), while this data is very much needed if we are going to learn about plant micro-climatology.\nTo calculate air humidity from temperature data could be use below equation:\n\\[E_s = 6.1078 \\exp\\left\\{\\frac{17.239T_{wb}}{T_{wb}+237.3}\\right\\}\\]\n\\[E_a = E_s - 0.661 (T_{db} - T_{wb})\\]\n\\[RH = \\frac{E_a}{E_s} \\times 100\\%\\]\nWhere:\n\\(E_s\\) = water vapor potential (mbar)\n\\(E_a\\) = water vapor actual (mbar)\n\\(T_{wb}\\) = Temperature of wet bulb thermometer (¬∞C)\n\\(T_{db}\\) = Temperature of dry bulb thermometer (¬∞C)\n\\(RH\\) = Relative humidity\nWe need information or data about air humidity fluctuation or variation in the one-day. How much the minimum, maximum and average the air humidity in relative humidity unit (%) if we only know the maximum and minimum temperature from dry and wet bulb thermometer in one-day?\nFormulation of the problem\nThis problem will be solve using Numeric Search ‚Äì Random Search.\nRelative humidity will calculate using above equation and the temperature data will provided by four data which recorded in one day:\n\nMaximum dry bulb temperature: X1_max\nMinimum dry bulb temperature: X1_min\nMaximum wet bulb temperature: X2_max\nMinimum wet bulb temperature: X2_min\n\nGenerate X1 and X2 random numbers R1 :\n\n\\(X_1 = X_{1,\\min} + R_i (X_{1,\\max} - X_{1,\\min})\\)\n\\(X_2 = X_{2,\\min} + R_i (X_{2,\\max} - X_{2,\\min})\\)\n\nCalculate the water vapor pressure by using above function:\n\n\\(E_s = 6.1078 \\exp\\left\\{\\frac{17.239 X_2}{X_2 +237.3}\\right\\}\\)\n\\(E_a = E_s - 0.661 (X_1 - X_2)\\)\n\\(RH = \\frac{E_a}{E_s} \\times 100\\%\\)\n\nRH calculation should be iterate base on minutes/day (1440 minutes)\nSave RH as temporary solution\nCompare and calculate :\nMaximum RH (* is maximum or temporary value)\n\nIF \\(RH &gt; RH_{\\max}^*\\) then \\(RH_{\\max}^* = RH\\); \\(X_1^* = X_1\\) and \\(X_2^* = X_2\\)\n\nMinimum RH ((* is minimum or temporary value)\nIF \\(RH &lt; RH_{\\min}^*\\) then \\(RH_{\\min}^* = RH\\); \\(X_1^* = X_1\\) and \\(X_2^* = X_2\\)\nAverage RH\n\n\\(RH_{avg} = \\frac{\\sum RH}{\\text{number of iteration}}\\)\n\nExpected result\nGet the maximum, minimum and average air relative humidity and how is the maximum and minimum temperature when the value has reached.\nFinding the optimal value\nFluctuation of air humidity that represent by relative humidity could be used in the microclimate condition on certain site. For example, if we have crop plantation we need information on potential water needed and photosynthesis rate of plant that could be estimated from transpiration rate, while transpiration process (dilatation) is determined by air humidity. So once we are able to identify maximum, minimum and average RH we can also identify stomata activity and potential photosynthesis daily.\nSolving the problem\nVisual Basic 6 used to solve above problem. For example, the data as input is :\n\nMaximum dry bulb temperature is 32¬∞C\nMinimum dry bulb temperature is 21¬∞C\nMaximum wet bulb temperature is 29¬∞C\nMinimum wet bulb tempereture is 17¬∞C\n\nNumber of iteration is 1440 (same as number of minutes/day)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210820-farewell-wfp.html",
    "href": "blog/20210820-farewell-wfp.html",
    "title": "Farewell WFP!",
    "section": "",
    "text": "As you may have heard, I‚Äôm moving on to the World Bank HQ after 9 wonderful years at the UN WFP in Indonesia. I started my career with WFP on 11 June 2012.\nToday is my last working days, I am visiting the office to clean up my things. Yes, this is my first time back to the office again since ‚Äúworking from home‚Äù mode started on March 11, 2020.\nLooking forward to doing more meaningful work in climate analytics and geospatial technology for greater impact.\n \n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230131-descriptive-statistics-analysis-using-climate-data.html",
    "href": "blog/20230131-descriptive-statistics-analysis-using-climate-data.html",
    "title": "Descriptive statistics analysis using climate data",
    "section": "",
    "text": "Climate is a complex system that is affected by a variety of factors, including atmospheric composition, solar radiation, and ocean currents. Understanding the Earth‚Äôs climate is essential for predicting future changes and developing strategies to mitigate the impacts of climate change. One important tool for understanding the Earth‚Äôs climate is statistical descriptive analysis, which is a set of techniques used to summarize, describe and make sense of large sets of data. These techniques help to identify patterns, trends and relationships in data, which can be used to understand how the climate is changing over time.\n\nIntroduction\nClimate data is collected from a variety of sources, including weather stations, satellites, and climate models. Each of these sources has its own strengths and limitations, and scientists use a combination of data from multiple sources to get a comprehensive understanding of the Earth‚Äôs climate system. Weather stations, for example, provide detailed information about temperature, precipitation, and other weather conditions at specific locations. Satellites, on the other hand, provide global coverage of the Earth‚Äôs surface and can measure a wide range of climate variables, including temperature, precipitation, and atmospheric composition. Climate models are computer simulations that are used to understand how the Earth‚Äôs climate system works.\nScientists must use a variety of statistical techniques to make sense of the data. One important technique is data cleaning, which is the process of removing errors and inconsistencies from the data. After the data is cleaned, statistical descriptive analysis techniques can be used to summarize and describe the data. Examples of these techniques include mean, median, and standard deviation, which help to identify patterns and trends in climate data over time. Another technique used in statistical descriptive analysis is correlation and regression analysis, which is used to understand the relationship between two or more variables. Data visualization is another important technique, which is the process of creating visual representations of data to make it more understandable and accessible.\nIn this blog post, we will explore some common descriptive statistics methods and how we can utilize a new technology/tools to do the analysis in more detail and give examples of how the output could be used to understand the Earth‚Äôs climate system. We will also discuss the limitations and potential sources of error of these techniques and the importance of continued research and monitoring of the Earth‚Äôs climate to improve our understanding of the complex interactions between the atmosphere, oceans, and land surface.\nDescriptive statistics can be broken down into the following categories:\n\nFrequency distribution is a method of counting how many times a specific aspect appears in a data set. This information is recorded in a table format and is useful for analyzing both qualitative and quantitative data.\nCentral Tendency, includes three calculations: Mean, Median, Mode. These results represent the central value of the data set, providing a summary of the total number of occurrences.\nVariability, it describes how far apart the data points are from each other. It shows the range of dispersion and the degree of variance in the sample, from the highest to the lowest value.\n\nBelow are measures of central tendency and variability, respectively, used in descriptive statistics to summarize and describe a set of numerical data.\n\nmin (minimum value) is the smallest value in a dataset. It provides an idea of the lower limit of the data. It can be represented as the equation: \\[\\min = \\min(x)\\] where \\(x\\) is the set of data.\nmean (average) is the sum of all values in the dataset divided by the number of data points. It provides an estimate of the central tendency of the data, but can be skewed by outliers. It can be represented as the equation: \\[\\bar{x} = \\frac{\\sum x}{n}\\] where \\(x\\) is the set of data and \\(n\\) is the number of data points in the set.\nmedian is the value in the middle of a dataset when it is sorted in ascending or descending order. It provides a measure of the central tendency of the data, and is not affected by outliers. It can be represented as the equation: \\[\\text{median} = x_{[n/2]}\\] where \\(x\\) is the sorted set of data and \\(n\\) is the number of data points in the set. If \\(n\\) is even, then the median is the average of the two middle values.\nmode is the value that occurs most frequently in a dataset. It is useful in finding the most common value in a dataset, but it doesn‚Äôt provide much insight into the spread or distribution of the data. It can be represented as the equation: \\[\\text{mode} = \\max(f_i)\\] where \\(f_i\\) is the frequency of each value in the set of data \\(x\\).\nmax (maximum value) is the largest value in a dataset. It provides an idea of the upper limit of the data. It can be represented as the equation: \\[\\max = \\max(x)\\] where \\(x\\) is the set of data.\nrange is the difference between the maximum and minimum value in a dataset. It gives an idea of the spread of the data. It can be represented as the equation: \\[\\text{range} = \\max(x) - \\min(x)\\] where \\(x\\) is the set of data.\nstdev (standard deviation) is a measure of the spread of a dataset. It quantifies how much each value deviates from the mean. A low standard deviation means the values are clustered closely around the mean, while a high standard deviation indicates that the values are more spread out. It can be represented as the equation: \\[s = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n}}\\] where \\(x\\) is the set of data, \\(\\bar{x}\\) is the mean of the data, and \\(n\\) is the number of data points in the set.\n\n\n\nData and Tools\nThe analysis utilizes data from NASA GLDAS-2.1 which is accessible from Google Earth Engine (GEE) Data Catalog. The model was initiated on January 1, 2000 and was based on the conditions from the previous GLDAS-2.0 model. It utilized atmospheric analysis fields (Derber et al., 1991) from the National Oceanic and Atmospheric Administration‚Äôs Global Data Assimilation System (NOAA/GDAS), precipitation data from the Global Precipitation Climatology Project (GPCP), and radiation fields from the Air Force Weather Agency‚Äôs AGRicultural METeorological modeling system (AGRMET) starting from March 1, 2001.\nThe analysis utilizes a free geospatial cloud computing platform GEE to do the computation and visualize the result. GEE has several advantages such as: fast access, interactive algorithm development with instant access to petabytes of data, and reducing computational challenge (cost on hardware, software license and internet bandwidth) - if the analysis required a lot of earth observation data with large coverage areas to be downloaded and processed.\n\n\nHow-to?\nHere is a step-by-step guide for performing a statistical descriptive analysis on GLDAS 2.1 3-hourly data data for the last 10-years, from 2013-2022 using GEE:\n\nCreate a configuration setting on:\n\nPoint of Interest, to extract time series information by a coordinate. In this analysis, we would like to extract daily air temperature at below coordinate, which is the location of the Department of Geophysics and Meteorology - IPB.\n\n//---  To extract timeseries information by location\nvar geometry = ee.Geometry.Point([106.73079706340155, -6.557929971767509]);\n\nData availability checking\n\n//---  Check data availability\nvar firstImage = ee.Date(ee.List(GLDAS.get('date_range')).get(0));\nvar latestImage = ee.Date(GLDAS.limit(1, 'system:time_start',  false).first().get('system:time_start'));\nAccess the GLDAS 2.1 and other boundary data and load the data into a GEE ImageCollection: You can access it using the Earth Engine Data Catalog and then load it into a GEE ImageCollection, which is a collection of images. This will allow you to process and analyze the data in GEE.\n//---  Global Land Data Assimilation System Dataset Availability:\nvar GLDAS = ee.ImageCollection('NASA/GLDAS/V021/NOAH/G025/T3H'); //GLDAS21\n// These are USDOS LSIB boundaries simplified for boundary visualization.\nvar bnd = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\");\nFilter the data based on the required time range: we can define the start and end dat for the analysis..\n//---  Start and End date\nvar startDate = ee.Date('2013-01-01'); \nvar endDate = ee.Date('2023-01-01');\nWrite a function to convert Kelvin to Celsius\n//---  Function to Celsius\nfunction toCelcius(image){\n var Temp = image.select('Tair_f_inst').subtract(273.15);\n var overwrite = true;\n var result = image.addBands(Temp, ['Tair_f_inst'], overwrite);\n return result; \n}\n//---  GLDAS air temperature in degC\nvar GLDAS_Tair = GLDAS.select(['Tair_f_inst']).map(toCelcius);\nReduce the data to a single image and perform the descriptive analysis: Use the reduce() function to reduce the ImageCollection to a single image, use the filterDate() function to extract only the data that you need for the analysis, then calculate descriptive statistics, such as mean, median, standard deviation, minimum and maximum values, and others, using the reducers in GEE.\n//---  MEAN\nvar Tair_mean = ee.ImageCollection(\n ee.List.sequence(0, numberOfDays.subtract(1))\n   .map(function (dayOffset) {\n     var start = startDate.advance(dayOffset, 'days');\n     var end = start.advance(1, 'days');\n     return GLDAS_Tair\n       .filterDate(start, end)\n       .mean()\n       .rename('Tmean')\n       .set('system:index', start.format('YYYY-MM-dd'))\n       .set('date', start.format('YYYY-MM-dd'))\n       .set('system:time_start', start.millis())\n       .set('system:time_end', start.millis());\n   })\n);\n//---  Sorted time\nvar Tair_mean_sorted = Tair_mean.sort(\"system:time_start\");\nVisualize the results: Finally, you can visualize the results of the descriptive analysis using the visualization tools in GEE, such as a map or a chart such as: line plot, box plots, scatter plots, and others.\n\nMap\n\n//---  Map visualisation\nMap.addLayer(Tair_mean_sorted.mean(), Tair_vis, '10-years daily Tmean');\n\nChart\n\n//---  Combine min, mean, median, mode and max into single image collection\nvar Tair_temp = Tair_min.combine(Tair_mean)\n                       .combine(Tair_median)\n                       .combine(Tair_max);  \nvar Tair = Tair_temp.sort(\"system:time_start\");\n//---  Visualise all variable in single chart\nvar chartTair = \n   ui.Chart.image\n     .series({\n       imageCollection: Tair, \n       region: geometry, \n       reducer: ee.Reducer.mean(), \n       scale: 27830\n     })\n     .setSeriesNames(['Tmax', 'Tmean', 'Tmedian', 'Tmin'])\n     .setOptions({\n         title: 'Daily Air Temperature',\n         hAxis: {title: 'Date', titleTextStyle: {italic: false, bold: true}},\n         vAxis: {\n           title: 'degC',\n           titleTextStyle: {italic: false, bold: true}\n         },\n         lineWidth: 1,\n         colors: ['d7191c', 'fdae61', 'abd9e9', '2c7bb6'],\n         curveType: 'function'\n       });\nprint(chartTair);\nSave the data: Once we have completed the analysis, save the data to a format that can be easily shared with others, this could be a CSV file.\n//---  Generate all temperature at single location\nvar timeSeriesTair = Tair.map(function (image) {\n var mean = image.reduceRegion({\n   reducer: ee.Reducer.mean(),\n   geometry: geometry,\n   scale: image.projection().nominalScale()\n });\n return ee.Feature(null, mean)\n   .set({\n     'date': image.date().format('yyyy-MM-dd'),\n     'system:time_start': image.date().millis()\n   });\n});\n//---  Export all variable to csv\nExport.table.toDrive({\n collection: timeSeriesTair, \n description: 'gldas21_ipb_geomet_tair_2013_2022',\n folder: 'GLDAS_csv',\n selectors: ['date', 'Tmin', 'Tmean', 'Tmedian', 'Tmax']\n});\n\n\n\nResults\nIn statistical descriptive analysis, we often use measures such as mean, median, and standard deviation to summarize a dataset. For example, the mean temperature can give us an idea of the average temperature in a region, while the standard deviation provides information on how much the temperature values are spread out.\nIf the temperature data in a tropical region is normally distributed, then we would expect to see a bell-shaped histogram, with most of the temperature values clustered around the mean and a decreasing number of values as we move away from the mean in either direction. In this case, the mean, median, and mode of the temperature data would be approximately equal.\nHowever, if the temperature data in a tropical region is not normally distributed, we might see a histogram that is skewed or has multiple modes. For example, if there are a few extreme temperatures that are much higher or lower than the majority of the temperature values, then the mean temperature would be influenced by these outliers, making it higher or lower than the median temperature. In such cases, the mean may not be a representative summary of the central tendency of the temperature data.\nThus, the choice of summary statistics to describe air temperature data in the tropical region (or any other region) will depend on the distribution of the data and the goals of the analysis.\nThe four maps below consist of the 10-years daily mean of air temperature in Indonesia from 2013-2022, presented as Minimum, Median, Mean and Maximum Temperature. Visualizing using rainbow color, ranging from 0 - 40 degC, from dark blue to dark red.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GEE, we are also able to extract based on point (coordinate) location. Below chart is visualizing GLDAS air temperature data extracted from GEE where the point is located at IPB Dramaga Campus in Bogor Indonesia. Red color is daily maximum, orange is daily mean, light blue is daily median and blue is daily min temperature.\n\nFrom this data, we can see that the average daily temperature at IPB Dramaga Campus in Bogor is 26.5¬∞C. The median temperature over this time period is 25.0¬∞C. The range of temperatures over this 10 year period is 2¬∞C, and the standard deviation is 1.2¬∞C.\nIn the 10 year period between 2013-2022, the highest temperature recorded at IPB Dramaga Campus was 34.1¬∞C and the lowest was 19.4¬∞C. The maximum temperature was observed in the months of September and October, while the minimum temperature was observed in the months of July and August. Despite the occasional spikes and dips in the air temperature, generally speaking, the air temperature at IPB Dramaga Campus stays within a relatively narrow range, ranging from 23.4¬∞C to 29.7¬∞C.\n\n\nLimitation of the analysis\nThe GLDAS air temperature data is a valuable source for providing insights into the global climate, but it is important to understand the limitations and potential sources of error when performing statistical descriptive analysis. One major limitation of GLDAS data is that it is a data-driven model and its accuracy is affected by the amount of data available. Thus, the GLDAS data may be inaccurate in areas where there is limited data or where the terrain is complex. Additionally, since GLDAS data is based on a simulation, it may contain errors due to the quality of the model and its input data.\nThe accuracy of GLDAS air temperature data can also be affected by varying temporal and spatial resolutions. GLDAS data is usually provided with a temporal resolution of 3 hours, while weather station data is usually provided with a temporal resolution of 1 hour. This difference in temporal resolution can lead to errors when comparing or combining the two data sets. Similarly, GLDAS data is usually provided.\nSimilarly, GLDAS data is usually provided in aggregated form over larger spatial scales which can make it difficult to discern local-scale changes in conditions. To address this issue, advanced techniques such as remote sensing and machine learning can be used to interpret the data and develop more localized interpretations. This is particularly useful in areas where detailed records are lacking, or in cases where the data collected is incomplete. Additionally, recent advancements in AI technology have allowed for the development of sophisticated models that can effectively interpret large and complex datasets, providing a more comprehensive understanding of the data.\nAll content within this report is merely based upon the GLDAS data, without any adjustment or bias correction with station data. As the climate phenomena is a dynamic situation, the current realities may differ from what is depicted in this report. Ground check is necessary to ensure if satellite and field situation data are corresponding.\n\n\nCode\nThe GEE code for generate statistical value, maps and the chart are available here: https://code.earthengine.google.com/076b1e65b3d16925bdb088bbb796ede4\nNotes: this analysis required a lot of computation resources, all the results especially maps are not instantly available. Please be patient.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20190126-extreme-winter-in-mongolia.html",
    "href": "blog/20190126-extreme-winter-in-mongolia.html",
    "title": "Extreme winter in Mongolia",
    "section": "",
    "text": "Last week, I had the opportunity to visit Mongolia. This country sounds familiar to me, especially during my childhood studied history at elementary school level about the battle of Mongol forces against the Majapahit Kingdom in East Java.\nCurrently, Mongolia is in (extreme) winter season. Winter lasts from early November until April, with the coldest period being between mid-December and the end of February or mid-March when the temperatures drops to -20¬∞C or -30¬∞C, occasionally even lower. This time, trip to Mongolia is very hard for me. I have felt winter in several countries, but never felt this cold, -33¬∞C.\n\nIn Ulaanbaatar (UB) I have several meeting with various government official and international organization: NEMA, NAMEM, NSO, ALAGAC, WHO and World Vision.\nHere‚Äôs some pictures in UB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230415-fuzzy-inference-system-fis-for-flood-risk-assessment.html",
    "href": "blog/20230415-fuzzy-inference-system-fis-for-flood-risk-assessment.html",
    "title": "Fuzzy Inference System (FIS) for Flood Risk Assessment",
    "section": "",
    "text": "1 Implementation\nIn the implementation phase of this analysis, we utilized Python and the Scikit-Fuzzy library to develop a fuzzy logic-based flood risk assessment model. This model took into account four essential factors affecting flood risks: precipitation intensity, soil moisture, land cover, and slope. By defining the fuzzy sets and rules for these variables, the model was able to estimate the flood risk for various combinations of input values. The ultimate goal of this implementation was to identify the conditions under which low flood risks could be achieved, even in situations where precipitation intensity was at its maximum.\n1.1 How-to?\nIn the first stage of the analysis, we defined the variables that influence flood risk. These variables include precipitation intensity, soil moisture, land cover, and slope. Each of these variables was represented as a fuzzy variable using the Scikit-Fuzzy library‚Äôs Antecedent class. Additionally, we defined the output variable flood_risk using the Consequent class. This stage set the foundation for the fuzzy logic-based flood risk assessment model by establishing the key variables that the model would use to estimate flood risk.\nIn the second and third stages, we focused on defining the fuzzy sets and their respective membership functions for each of the variables defined in the first stage. We used the automf() function to automatically generate triangular membership functions for precipitation intensity, soil moisture, and flood risk, each with three levels: low, medium, and high. For the land cover and slope variables, we manually defined triangular membership functions, specifying the appropriate ranges for each fuzzy set (urban, vegetation, and bare_soil for land cover, and flat, moderate, and steep for slope). These stages were critical for establishing the relationships between the input variables and the output flood risk, which would later be used to evaluate different combinations of input values in the fuzzy inference process.\nIn the fourth stage, we defined the rules that describe the relationships between the input variables (precipitation intensity, soil moisture, land cover, and slope) and the output variable (flood risk). We first created a list of classifications for each input variable and the output variable. Using the multiplication principle, we calculated the total number of possible combinations of these classifications, resulting in 81 unique rules.\nFor each combination of input classifications, we determined the appropriate flood risk level based on a set of predefined conditions. These conditions were based on expert knowledge and domain understanding, considering factors such as high precipitation and soil moisture, bare soil land cover, and steep slopes. After determining the flood risk level for each combination, we created a fuzzy rule using the Scikit-Fuzzy library‚Äôs Rule class, linking the input conditions with the corresponding flood risk level. These rules formed the basis of the fuzzy inference system that was used to evaluate different scenarios and estimate the corresponding flood risks.\nIn the fifth stage, we created the control system and simulation by combining the defined rules from the previous stage. The Scikit-Fuzzy library‚Äôs ControlSystem and ControlSystemSimulation classes were used for this purpose. The ControlSystem class takes the set of rules as input and initializes the fuzzy inference system, while the ControlSystemSimulation class initializes a simulation environment that can be used to compute the output based on the input values.\nIn the sixth stage, we provided example input values for each input variable (precipitation, soil moisture, land cover, and slope) to test the fuzzy inference system. The input values were assigned to their corresponding input variables in the simulation, and the compute method of the ControlSystemSimulation object was called to perform the fuzzy inference process and obtain the output flood risk level.\nIn the final stage, we output the computed flood risk level and visualize the result using the Scikit-Fuzzy library‚Äôs built-in plotting capabilities. The flood risk level was displayed as a numerical value, while the visualization provided a graphical representation of the membership functions and the defuzzified output. This allowed us to assess the performance of the fuzzy inference system and analyze the relationships between the input variables and the flood risk.\nThis will returned:\nFlood Risk Value: 83.33333333333336\nFlood Risk Category: high\nAnd a plot below\n\nThe initial implementation of the fuzzy inference system for flood risk assessment has been completed successfully. By providing example input values for precipitation (100), soil moisture (50), land cover (25), and slope (30) in Stage 6, we have demonstrated the functionality of the fuzzy system. The system processes these inputs through the defined membership functions, rules, and defuzzification methods to produce an output flood risk value and the corresponding flood risk category.\nUpon evaluating the system with the given input values, a flood risk value is generated, and the flood_risk.view(sim=flood_risk_sim) function provides a visual representation of the output. The plot displays the aggregated output membership functions and indicates the defuzzified crisp value. In this case, the plot reflects the flood risk level based on the provided inputs, and the computed flood risk category helps to understand the risk associated with the given conditions. With this initial implementation, we have set the foundation for further analyses and can adapt or extend the fuzzy system as needed to address specific flood risk assessment scenarios.\n1.2 Plot the membership function of the input variables\nThe provided code visualizes the membership functions for each of the input variables (Precipitation Intensity, Soil Moisture, Land Cover, and Slope) and the output variable (Flood Risk Level) in the fuzzy inference system. Here‚Äôs a summary of what each part of the code does:\n\nprecipitation.view(sim=flood_risk_sim): Plots the membership functions for the Precipitation Intensity variable, displaying how the input values are categorized into low, medium, and high precipitation levels.\nsoil_moisture.view(sim=flood_risk_sim): Plots the membership functions for the Soil Moisture variable, showing how the input values are categorized into low, medium, and high soil moisture levels.\nland_cover.view(sim=flood_risk_sim): Plots the membership functions for the Land Cover variable, illustrating how the input values are categorized into low, medium, and high land cover levels.\nslope.view(sim=flood_risk_sim): Plots the membership functions for the Slope variable, demonstrating how the input values are categorized into low, medium, and high slope levels.\nflood_risk.view(): Plots the membership functions for the output variable, Flood Risk Level, indicating how the output values are categorized into low, medium, and high flood risk levels.\nflood_risk.view(sim=flood_risk_sim): Plots the final Flood Risk Level for given input values, illustrating how the fuzzy inference system computes the flood risk based on the input variable values and the defined fuzzy rules.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo interpret the plots, observe how each input variable is divided into categories (low, medium, high) based on the membership functions. These categories represent the degree to which an input value belongs to a particular category.\nThe output variable plot shows how the flood risk levels are determined based on the input variables‚Äô membership values and the fuzzy rules defined in the system. The final plot, Flood Risk Level for Given Input Values, displays the aggregated output membership functions and the computed flood risk level as a single value.\n1.3 2D Plot\nThe provided code generates a 2D contour plot of flood risk as a function of Precipitation Intensity and Land Cover, while fixing the values of Soil Moisture and Slope. Here‚Äôs a summary of what each part of the code does:\n\nCreate grid points for input variables: Define a range of values for each input variable (Precipitation Intensity, Soil Moisture, Land Cover, and Slope) using np.linspace().\nDefine compute_flood_risk() function: This function takes Precipitation Intensity (P), Soil Moisture (M), Land Cover (L), and Slope (S) as inputs and computes the flood risk using the fuzzy inference system (flood_risk_sim).\nFix Soil Moisture and Slope values: Assign fixed values to Soil Moisture (M_fixed) and Slope (S_fixed).\nCreate flood risk matrix: Initialize a matrix with the size of the combination of Precipitation Intensity (P_values) and Land Cover (L_values). Iterate through each combination of these values and compute the flood risk using the compute_flood_risk() function with the fixed values of Soil Moisture and Slope.\nPlot the 2D contour plot: Using plt.contourf(), create a contour plot that visualizes the flood risk as a function of Precipitation Intensity and Land Cover. The color map ‚Äòviridis‚Äô is used to represent the flood risk levels, with 20 contour levels.\nAdd colorbar, labels, and title: Add a colorbar to represent the flood risk values, label the axes, and add a title that includes the fixed values of Soil Moisture and Slope.\n\n\nTo interpret the plot, observe how the flood risk values change as the Precipitation Intensity and Land Cover values vary. The plot shows how the flood risk is influenced by these two input variables while keeping the other two (Soil Moisture and Slope) fixed at specific values.\nThe contour lines in the plot represent different levels of flood risk, with the color intensity indicating the flood risk level. Darker colors represent lower flood risk, and lighter colors represent higher flood risk.\n1.4 3D Plot\nThe provided code generates a 3D surface plot of flood risk as a function of Precipitation Intensity and Land Cover, while fixing the values of Soil Moisture and Slope. Here‚Äôs a summary of what each part of the code does:\n\nCreate a 3D plot figure: Initialize a new figure using plt.figure() and add a 3D subplot with the projection='3d' argument.\nCreate the 3D surface plot: Use the ax.plot_surface() function to create a 3D surface plot for Precipitation Intensity (Y-axis) vs Land Cover (X-axis), with the flood risk as the Z-axis. The color map ‚Äòviridis‚Äô is used to represent the flood risk levels.\nAdd colorbar, labels, and title: Add a colorbar to represent the flood risk values, label the axes, and add a title that includes the fixed values of Soil Moisture and Slope.\n\n\nTo interpret the plot, observe how the flood risk values (Z-axis) change as the Precipitation Intensity and Land Cover values (X and Y axes) vary. The plot shows how the flood risk is influenced by these two input variables while keeping the other two (Soil Moisture and Slope) fixed at specific values. The color intensity on the surface indicates the flood risk level, with darker colors representing lower flood risk and lighter colors representing higher flood risk.\nThe 3D surface plot provides a more detailed visualization of the relationship between flood risk, Precipitation Intensity, and Land Cover compared to the 2D contour plot. You can observe the shape of the surface to identify areas with high or low flood risk and better understand the interaction between the input variables.\n2 Minimizing Flood Risks under Maximum Precipitation\nThis chapter emphasizes the focus on reducing flood risks under the most challenging conditions (maximum precipitation) while highlighting the three main variables (soil moisture, land cover, and slope) being examined in the analysis.\nFlood risk management is a critical aspect of urban planning and environmental protection. Understanding the factors that contribute to flood risks and identifying strategies to minimize these risks is essential for creating resilient communities. In this analysis, we explore the relationships between four key variables - precipitation intensity, soil moisture, land cover, and slope - to determine their influence on flood risk. Our goal is to identify the combinations of these variables that result in low flood risks, even under conditions of maximum precipitation.\nUsing a fuzzy logic-based simulation model, we examine the interactions between these variables and their impact on flood risk. The model incorporates expert knowledge and rule-based systems to predict flood risk levels based on various input scenarios. By analyzing the simulation results, we aim to provide insights into the conditions that can effectively mitigate flood risks, helping policymakers and urban planners make informed decisions for better flood management strategies.\nThe analysis includes a scatterplot matrix visualization that highlights the relationships between soil moisture, land cover, and slope under maximum precipitation conditions. By interpreting this matrix, we can identify patterns and correlations between these variables that contribute to lower flood risks. These insights will help guide future efforts in designing urban areas and implementing flood management measures that are both effective and sustainable.\nThe provided code performs a sensitivity analysis to minimize flood risks under maximum precipitation conditions. It evaluates flood risk categories based on all input variables, generates a dataset of data points with different combinations of soil moisture, land cover, and slope values, and finally creates a scatterplot matrix. Here‚Äôs a summary of what each part of the code does:\n\nDefine the maximum precipitation intensity: Set the value of max_precipitation to 100, which is considered high precipitation intensity.\nDefine the flood risk category function: Create a function get_flood_risk_category() that takes precipitation, soil moisture, land cover, and slope as input variables, and returns the flood risk category using the previously defined categorize_flood_risk() function.\nGenerate data points for soil moisture, land cover, and slope: Create arrays of evenly spaced values for each of these input variables.\nIterate through all combinations of soil moisture, land cover, and slope: For each combination, use the maximum precipitation value and the get_flood_risk_category() function to obtain the flood risk category. If the category is not None, append the combination to the data_points list.\nCreate a DataFrame containing the data points: Convert the list of data points into a pandas DataFrame, which makes it easier to analyze and visualize the data.\nCreate a scatterplot matrix: Use the seaborn library‚Äôs pairplot() function to create a scatterplot matrix of the data points, with flood risk categories represented by different colors.\n\n\nHere‚Äôs how to read the scatterplot matrix:\n\nThe diagonal plots (from the top-left to the bottom-right) are bar plots showing the distribution of each variable. These plots give an idea of the frequency of different values for each variable when the flood risk is low under maximum precipitation conditions.\nThe off-diagonal plots are scatter plots showing the relationships between pairs of variables. These plots help identify any patterns or correlations between the variables. The color of the dots indicates the flood risk category associated with each data point.\nIn the off-diagonal plots, if you see that dots of a specific color (in this case, low flood risk) are clustered in a particular region, it indicates that certain combinations of variables are more likely to result in low flood risk conditions.\n\nTo interpret the plots, consider the following:\n\nIn the scatterplot between soil moisture and land cover, if there is a pattern or a specific region where low flood risk dots are clustered, it would suggest that there‚Äôs a relationship between these two variables that contributes to lower flood risks under maximum precipitation conditions.\nSimilarly, in the scatterplot between soil moisture and slope, look for clusters or patterns of low flood risk dots to identify any relationships between these variables that contribute to lower flood risks.\nFinally, in the scatterplot between land cover and slope, examine the distribution of low flood risk dots to determine if there‚Äôs a connection between these variables that leads to lower flood risks.\n\nBy analyzing these plots, we can gain insights into the relationships between soil moisture, land cover, and slope that contribute to low flood risks even under maximum precipitation conditions.\n3 Sensitivity Analysis\nAfter obtaining the flood risk assessment results from the Fuzzy Inference System (FIS), we can assess the quality of the model by comparing its predictions to observed data. To do this, we‚Äôll need a dataset containing historical flood events along with the corresponding values of the input variables (Precipitation Intensity, Soil Moisture, Land Cover, and Slope).\nWhat if observation data on flood events never exist?\nIf we don‚Äôt have any observation data to compare the FIS model results, evaluating the model‚Äôs performance becomes more challenging. However, we can still follow some steps to ensure that your FIS model is reasonable and plausible:\n\nExpert knowledge: Consult with experts in the field of flood risk assessment to ensure that your fuzzy sets, membership functions, and fuzzy rules are realistic and based on sound principles. This can help us refine our FIS model even without actual observation data.\nSensitivity analysis: Perform a sensitivity analysis to understand how the output flood risk varies with changes in input variables. By altering the input variables within their expected range and studying the corresponding changes in flood risk, we can gain insight into the behavior of the model and identify any unrealistic responses.\nComparison with other models: If there are other flood risk assessment models available (either deterministic or statistical), compare your FIS model‚Äôs predictions with those from the other models. Although this is not a direct comparison with observed data, it can provide some indication of how your model‚Äôs performance compares to alternative approaches.\nSimulation data: If we have access to hydrological or hydraulic models that can simulate flood events, we can use the simulated data as a proxy for observed data. Although this approach has its limitations, as the simulated data may not perfectly represent real-world conditions, it can still provide valuable information for evaluating your FIS model.\nTemporal validation: If we have historical data for some of the input variables but not for the flood risk, we can still evaluate your FIS model by analyzing its performance over time. For instance, we can assess whether the model‚Äôs predictions of high flood risk align with periods of heavy rainfall, high soil moisture, or other conditions known to increase flood risk.\n\nRemember that without observed data, it is more challenging to assess the performance of your FIS model accurately. However, following the steps outlined above can help us gain some confidence in your model and identify areas for potential improvement.\nLet‚Äôs try Sensitivity Analysis\nWe‚Äôll use the One-at-a-time (OAT) sensitivity analysis method to understand the effect of varying each input variable while keeping the others fixed. Assume that we have the FIS model already built and implemented in Python using the variables and fuzzy rules defined earlier.\nThis code performs a sensitivity analysis to study the relationship between input variables (Precipitation, Soil Moisture, Land Cover, and Slope) and the output variable (Flood Risk) in a FIS. It evaluates the FIS model for different values of the input variables, keeping the other input variables at their median values.\nHere is a summary of the main steps in the code:\n\nDefine the range and step size for each input variable.\nCalculate the flood risk for each input variable using the FIS model. The sensitivity_analysis function iterates over different values of each input variable while keeping the other input variables fixed at their median values.\nCategorize the flood risk levels (low, medium, high) based on the computed flood risk values.\nPlot the sensitivity analysis results, showing how flood risk varies with changes in the input variables.\n\nThe plot consists of four subplots, one for each input variable, with flood risk on the y-axis and the input variable on the x-axis. The background of each plot is filled with colors corresponding to the flood risk categories (low, medium, and high). The data points are plotted with different markers (‚Äòo‚Äô, ‚Äòs‚Äô, ‚Äòx‚Äô) based on the input variable‚Äôs categories (low, medium, and high).\n\nTo interpret the plot, observe how the flood risk changes as the input variable value increases or decreases. A steep slope in the plot indicates that the flood risk is highly sensitive to changes in the input variable. If the flood risk remains relatively constant despite changes in the input variable, it suggests that the flood risk is less sensitive to that input variable.\nTo understand the meaning of the plot, consider that it represents how much the flood risk is affected by each input variable, given that other input variables are kept constant. By analyzing the plot, you can identify which input variables have a more significant impact on flood risk and prioritize interventions or mitigation strategies accordingly.\n4 Summary\nFlood risk assessment is a critical component of disaster management and urban planning. Accurate and reliable flood risk estimation helps authorities make informed decisions, prioritize resources, and implement effective mitigation strategies. With the increasing impacts of climate change and urbanization, there is a growing need for advanced techniques that can provide better insights into flood risk under varying conditions.\nFuzzy Inference Systems (FIS) offer a robust and flexible approach to model complex relationships between multiple input variables and an output variable, such as flood risk. By incorporating expert knowledge and handling uncertainties, FIS models can capture the intricacies of real-world systems, providing more accurate and reliable estimates of flood risk compared to traditional methods.\nFIS models have gained popularity in the field of hydrological modeling and flood risk assessment due to their ability to handle imprecise and incomplete data, as well as their capability to incorporate human reasoning and intuition in the form of linguistic rules. This ability to integrate expert knowledge with quantitative data provides a valuable advantage, especially in situations where data availability is limited or uncertain.\nThe utilization of FIS in flood risk assessment typically involves defining input variables that influence flood risk, such as precipitation intensity, soil moisture, land cover, and slope. These variables are then used to estimate the flood risk level, which can be categorized into different levels, such as low, medium, or high.\nTo build an FIS model for flood risk assessment, the first step is to identify relevant input variables and their value domains. Next, fuzzy sets and membership functions are defined for each variable, followed by the formulation of fuzzy rules that describe the relationship between input variables and flood risk. These rules are derived from expert knowledge or empirical data and are used to determine the output flood risk level.\nOne of the critical aspects of FIS models is their ability to handle uncertainties and vagueness in the input data. This is particularly important in the context of flood risk assessment, where data can be scarce or subject to significant measurement errors. By using fuzzy sets and membership functions, FIS models can accommodate these uncertainties, providing more reliable and robust estimates of flood risk.\nSensitivity analysis is a valuable tool for evaluating the performance of FIS models in flood risk assessment. By varying input variables within their expected range and studying the corresponding changes in flood risk, modelers can gain insight into the behavior of the model and identify any unrealistic responses or potential areas for improvement.\nFIS models can be further enhanced by incorporating optimization techniques to identify the most critical factors contributing to flood risk. This can help decision-makers focus on specific areas or interventions that have the most significant impact on reducing flood risk and improving overall resilience.\nOne of the challenges in applying FIS models for flood risk assessment is the lack of observed data for model validation. In such cases, the performance of the model can be evaluated using expert knowledge, sensitivity analysis, comparison with other models, or the use of simulated data from hydrological or hydraulic models.\nIn conclusion, FIS provides a promising approach for flood risk assessment, offering a flexible and robust framework for modeling complex relationships and handling uncertainties. By incorporating expert knowledge and quantitative data, FIS models have the potential to significantly improve our understanding of flood risk and support more effective decision-making in disaster management and urban planning.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20050108-satelit-geostasioner.html",
    "href": "blog/20050108-satelit-geostasioner.html",
    "title": "Satelit Geostasioner",
    "section": "",
    "text": "Mengamati bumi melalui satelit Geostasioner (GOES EAST, METEOSAT, IODC, GMS dan GOES WEST) tanggal 25 Oktober 2004, Jam 18.00.\nSatelit-satelit yang mengorbit bumi sebagian besar berada pada orbit geostasioner, dengan ketinggian sekitar 35.800 km dari permukaan bumi. Satelit semacam ini umumnya digunakan untuk keperluan pengamatan cuaca (meteorologi) dan hubungan komunikasi jarak jauh (telekomunikasi). Pemanfaatan satelit semacam ini membutuhkan kedudukan satelit yang harus tepat longitude akusisi dari daerah atau stasiun bumi yang diliput. Hal ini dapat dipenuhi oleh orbit geostasioner yang berada pada bidang khatulistiwa, berbentuk lingkaran, dan mempunyai periode yang sinkron denga periode riel rotasi bumi.\nCitra yang dihasilkan oleh kelima satelit pada tanggal 25 Oktober 2004, jam 18.00 GMT adalah sebagai berikut:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDan cakupan wilayah masing-masing satelit seperti gambar berikut:\n\nMosaic yang dihasilkan oleh kelima satelit pada tanggal 25 Oktober 2004, jam 18.00 GMT adalah sebagai berikut:\n\nAnimasi citra satelit GMS tanggal 14-31 Oktober 2004\n\nPada citra satelit GMS tanggal 14-31 Oktober 2004, dapat dilihat pola penyebaran awan di Indonesia sebagian besar tersebar di wilayah Indonesia bagian barat. Hal ini disebabkan oleh angin muson barat yang berasal dari Asia yang membawa udara lembab. Angin muson yaitu angin yang terjadi akibat adanya perbedaan tekanan antara daerah di belahan bumi utara dan daerah di belahan bumi selatan.\nPerbedaan tekanan ini disebabkan oleh gerak semu matahari. Pada bulan Oktober, matahari bergerak ke selatan sehingga daerah di belahan bumi utara memasuki musim dingin sedangkan daerah di belahan bumi selatan akan memasuki musim panas. Tekanan udara pada daerah di belahan bumi selatan relatif rendah sedangkan tekanan di belahan bumi utara relatif tinggi akibatnya angin betiup dari Asia (BBU) menuju Australia (BBS) atau disebut juga dengan Angin Muson Barat atau Angin Monsun Barat atau Angin Musim Penghujan. Angin Muson Barat menyebabkan musin penghujan karena angin ini melewati lautan relatif lebih jauh dibanding dengan Angin Muson Timur yang berasal dari Australia. Namun demikian, tidak semua wilayah di Indonesia tertutup awan dan berpeluang terjadi hujan terutama di wilayah Indonesia bagian timur.\nHal ini dikarenakan bulan Oktober merupakan awal musim penghujan. Selain dipengaruhi oleh angin muson dan sirkulasi atmosfer meridional (sirkulasi Hadley), wilayah Indonesia juga dipengaruhi oleh sirkulasi zonal (sirkulasi Walker) dan sirkulasi lokal (konvektif). Pengaruh sirkulasi zonal dapat dilihat pada perawanan di Irian.\nDari hasil pantauan pada kelima satelit cuaca di atas, didapati sebaran awan didekat ekuator pada wilayah benua Afrika, Amerika, Australia. Untuk wilayah Eropa dan Asia perawanan yang relatif kecil (tidak merata), dengan perawanan yang seperti ini dapat dikatakan bahwa penyebaran awan tidak terdistribusi dengan merata.\nDiketahui bahwa pergerakan matahari pada bulan September, menuju ke selatan dan menurut teori pergerakan ITCZ (Inter Tropical Convergence Zone), ITZC bergerak mengikuti pergerakan semu matahari (ke selatan). Garis ITZC menunjukkan wilayah berpotensi curah hujan yang tinggi. Akan tetapi jika melihat animasi kelima satelit tersebut pada tanggal 14-31 Oktober 2004, garis ITZC tidak terlihat jelas. Hal ini terjadi karena adanya tanda-tanda awal pembentukan siklon di wilayah samudera Atlantik sehingga penyebaran awan memusat pada wilayah itu.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200721-number-of-dry-and-wet-spell.html",
    "href": "blog/20200721-number-of-dry-and-wet-spell.html",
    "title": "Number of dry and wet-spell",
    "section": "",
    "text": "Now, it‚Äôs easy to get statistics information on AVERAGE, MINIMUM and MAXIMUM number of consecutive dry and wet days for a certain period using Google Earth Engine platform. Copy below code and paste into GEE.\nYou can modify:\n\nBounding box or area of interest\nStart and End of YEAR, MONTH and DATE\nPrecipitation threshold which can be considered as rain day.\nSymbology as SLD style\nPalette\n\nExample of the analysis\n \nLink for the full code\nNotes: The code was compiled from various source (GEE help, GEE Groups, StackExchange)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031210-anak-umptn-memang-beda.html",
    "href": "blog/20031210-anak-umptn-memang-beda.html",
    "title": "Anak UMPTN memang beda!",
    "section": "",
    "text": "Di kampus lagi sering ledek-ledakan antara mahasiswa yang masuk IPB lewat jalur USMI dan UMPTN. Gak penting juga sebenernya hahaha‚Ä¶\nTapi sensasi menunggu hasil UMPTN memang berbeda, apalagi yang sampe UMPTN nya 2x, pasti terasa spesial. Setelah kecewa¬† di tahun 2000, dini hari menunggu loper Kedaulatan Rakyat di dekat bunderan UGM dan nama yang dicari ternyata tidak ada. Akhirnya namaku muncul di tahun 2001, ga perlu jauh-jauh ke bunderan UGM, cukup nunggu loper Jawa Pos di dekat rumah.\nDiterima di Jurusan Geofisika dan Meteorologi, FMIPA, IPB - pilihan pertama dan ga salah pilih seperti mahasiswa lainnya. Keren ga? üòÄ\n\n\n\nJawa Pos, 6 Agustus 2001\n\n\nJawa Pos, 6 Agustus 2001\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20171124-openstreetmap-geoweek-2017.html",
    "href": "blog/20171124-openstreetmap-geoweek-2017.html",
    "title": "OpenStreetMap GeoWeek 2017",
    "section": "",
    "text": "November 12‚Äì18 is OSM Geography Awareness Week. 2017 OSMGeoWeek calls on community groups, teachers, students, governments, private sector, map lovers, and motivated individuals around the world to come together to celebrate geography, spatial data and its usage especially in humanitarian response, innovation and economic development.\nLast week I participated this event hosted by Humanitarian OpenStreetMap Organization (HOT). In Indonesia, the event run in parallel with WFP mapathon in Jakarta and Bali Mapathon with BNPB and Udaya University in Bali.\nThe event featured HOT‚Äôs partners, from public, private, donor, IO, NGO, university, each explaining how they use OpenStreetMap or innovative using spatial technologies to tackle social, economic, and ecological challenges as well as to further the attainment of sustainable development goals and moderated by Nadia Atmaji from Metro TV.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy OSM presentation on travel time and accessibility model started at 55‚Äô\nOSM Geography Awareness Week 2017: Innovative Technology and Partnership for Resilient Communities. @america, Jakarta\nSource: https://www.facebook.com/atamerica/videos/2170917652933630/\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230614-sentinel-1-modified-radar-vegetation-index.html",
    "href": "blog/20230614-sentinel-1-modified-radar-vegetation-index.html",
    "title": "Sentinel-1 modified Radar Vegetation Index",
    "section": "",
    "text": "The Sentinel-1 modified Radar Vegetation Index (RVI) based on Google Earth Engine (GEE) script below originally developed by my friend Jose Manuel Delgado Blasco (Scholar, Linkedin) as part of our team (GOST) activities to support during Ukraine response last year, published as GOST Public Good‚Äôs Github repo https://github.com/worldbank/GOST_SAR/tree/master/Radar_Vegetation_Index\nThe original GEE script was meant to be used only for individual updates, as time progresses and the need for vegetation monitoring continually increases, I believe it‚Äôs necessary to obtain this RVI time-series data, which can be matched with monthly rainfall time-series data for monitoring food crop phenology.\nFor this reason, I‚Äôve added a function to mosaic every ten days and batch downloading if the list of data is quite extensive.\nAll credit goes to the awesome work of Jose Manuel! Hats off to him!\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1277‚Äù] RVI in Crimean Peninsula [/caption]\nAbove picture is Vegetation Indices based on Sentinel-1 (generated using the GEE script), below picture is Vegetation Indices for the same period based on Sentinel-2 (generated using Climate Engine https://climengine.page.link/sZnR)\n\nNDVI in Crimean Peninsula\nFull GEE code is here: https://code.earthengine.google.com/62f799954525c997629cefdd435c500e\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180402-climate-expertise-in-the-humanitarian-field.html",
    "href": "blog/20180402-climate-expertise-in-the-humanitarian-field.html",
    "title": "Climate expertise in the humanitarian field",
    "section": "",
    "text": "Last month I was invited by my alma mater to an academic workshop which aims to (i) evaluate the curriculum for the period (2013-2017), (ii) develop a curriculum that is relevant to technology and science state of the art, and (iii) support the achievement of the competencies expected and in line with the direction of IPB towards a techno-socio-enterpreneurial university.\nI was asked to give an example of the application of climate in the humanitarian field, and what skills (social and technical) are needed to compete in the international development works.\nI deliver a short presentation and provide some example from my current office - WFP on ‚ÄúClimate expertise in the field of humanitarian: needs and challenges‚Äù\nOutline\n\nWFP and Climate\nUN core values and competencies\nRelevance of the curriculum to market needs\n\nI explain on how WFP applies climate analytics to support their goal to end hunger globally.\nWFP‚Äôs Climate and Disaster Risk Reduction programme\n\nDisaster risk reduction\nPreventing and reducing the impact of disasters\nResilience building\nBuilding the capacity of communities potentially exposed to hazards to resist, adapt and recover\nFood assistance for assets\nAddressing immediate food needs while promoting the building or boosting of assets to improve long-term food security and resilience\nClimate and food security analyses\nUnderstanding the links and potential impacts of climate change on food insecurity\nClimate change adaptation\nHelping governments and communities build resilience against and adapt to growing climate risks\nRisk management, insurance and finance\nLinking safety nets with innovative tools to address climate risks\n\nOur competencies also important, so it can be perfectly match with what international organization need. I only mention some of the core value and competencies needed by the UN.\nUnited Nations (UN) competencies for the future\nCore values\n\nIntegrity\n\nDoes not abuse power or authority\nTakes prompt action in cases of unprofessional or unethical behaviour\n\nProfessionalism\n\nShows pride in work and in achievements\nIs conscientious and efficient in meeting commitments, observing deadlines and achieving results\n\nRespect for Diversity\n\nWorks effectively with people from all backgrounds\nDoes not discriminate against any individual or group\n\n\nCore competencies\n\nCreativity\n\nActively seeks to improve programmes or services\nOffers new and different options to solve problems or meet client needs\nTakes an interest in new ideas and new ways of doing things\nIs not bound by current thinking or traditional approaches\n\nTechnological Awareness\n\nKeeps abreast of available technology\nUnderstands applicability and limitations of technology to the work of the office\nActively seeks to apply technology to appropriate tasks\nShows willingness to learn new technology\n\nJudgement/Decision-making\n\nChecks assumptions against facts\n\n\nLast, I talked about relevance of the curriculum to market needs. And how we can improve it.\nHighlight for curriculum programme design\n\nNature and quality of instruction\n\nCourse instruction in the program should be student-centric, employ active learning, and draw upon effective practices revealed by discipline-based research in higher education.\nInvolving undergraduates in research early in their program is highly encouraged.\n\nAdvisory\n\nThe diversity of career paths and opportunities within atmospheric and related sciences elevates the importance of academic advising and mentoring.\nIdeally, advisors should be experienced faculty in the program. Student should meet with their advisor at least once during each academic term.\nAdvisory meetings should include conversations about the student‚Äôs career goals and interests as they evolve over the student‚Äôs academic career.\n\n\nMore skill needed\nIn addition to knowledge of specific topics in atmospheric science, competency in the following areas is essential. Opportunities for enhancement of these skills within discipline-specific course work is strongly recommended.\nScientific data computing and data analytics\n\nComputing skills in data analysis, modeling, and visualization of atmospheric/climate problems\nExperience in developing (scientific) softwares\nAbility to apply numerical and statistical methods to atmospheric science problems\n\nDiverse career options\n\nPrivate sectors\n\nIn recent years private sector has started to provide weather impacts analysis and decision-support services, which require sector-specific knowledge and technical skills, such as analytics and machine learning\n\nHuman dimension of weather and climate\n\nBoundary spanner - e.g beyond the technical expertise, we need the capacity to be able to communicate the output of analysis better\nTrend in most sciences, interdisciplinary studies and opportunities that link human dimensions with physical science are rapidly increasing. Some topical areas that lie at the intersection of human dimensions and atmospheric sciences include risk communication, disaster sociology, hazards geography, behavioral economics, and environmental law and policy.\nLiaison - working relationship is important in any context, a collaborator who brings together and translate the gap between physical and social sciences\nStudents with interests in these human dimensions will benefit from an array of introductory courses and activities in these fields. Students benefit especially from minors or second majors in fields of particular interest. A strong understanding of the common theories and methodological approaches of these fields will enable a student to become a boundary spanner, an important kind of collaborator who brings together and translates between physical and social science colleagues.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200505-30-minutes-rainfall-and-landslide-research.html",
    "href": "blog/20200505-30-minutes-rainfall-and-landslide-research.html",
    "title": "30-minutes rainfall and landslide research",
    "section": "",
    "text": "Jemblung Landslide - Banjarnegara 2014\nLandslide occured on 12 Dec 2014 in Jemblung of Sampang village, Karangkobar sub-district, Banjarnegara district, Central Java (109.719792, -7.279643) at about 5:30 pm. The event affected settlement area with more than 300 population (53 households). Victims recorded including 15 injured, 108 death (95 bodies found and 13 lost).\nTwo days prior to the landslide (10‚Äì11 Dec), rainfall intensity in the area had been very high, but as the area isnt located near ground weather station, records aren‚Äôt available.\nRecent development in opendata allows more access to high resolution climate/weather data and to computing platform that allows users to run geospatial analysis in the cloud. This leads to further exploration like never before.\nOpen access to 30 mins temporal rainfall data at Google Earth Engine platform provided detail information on rainfall intensity during 10‚Äì11 Dec 2014 prior to the landslide in Banjarnegara. Such information help advancing research on rainfall model or identification of threshold for extreme rainfall that could trigger a landslide.\nThe steps below describe how to extract 30-minute rainfall from NASA GPM-IMERG, based on point location and convert it into CSV file using Google Earth Engine code editor.\nStep 1: Define a point of interest as a landslide location.\nStep 2: Choose date when landslide was occurred\nStep 3: Import NASA GPM IMERG 30 minute data and calculate accumulation for 10days.\nStep 4: Extract the rainfall value as time-series data\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. Chart of time series data generated using above script. [/caption]\nStep 5: Extract the rainfall value as accumulation data\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 2. Chart of accumulation data generated using above script.¬†2 days before the event, the area receiving rainfall around 150mm. [/caption]\nStep 6: Visualise time-series data as a chart\nStep 7: Visualise accumulation data as a chart\nStep 8: Create visualisation paramaters for rainfall and add it into canvas\nStep 9: Export the data to Google Drive as a CSV file.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 3. Google Earth Engine user interface [/caption]\nFull script here\nTo do:\n\nGet NASA Global Landslide Catalog from here\nCreate Landslide list (OBJECTID, Lon, Lat, Date YYYY-MM-DD)\nEnhance the script to be able read Location (Lon and Lat) and Date information from CSV GEE assets, and loop Export.table.toDrive with description based on OBJECTID\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210824-qgis-auto-mapping.html",
    "href": "blog/20210824-qgis-auto-mapping.html",
    "title": "QGIS auto mapping",
    "section": "",
    "text": "This week I learned a new thing, create a layout and automatically export to PNG files from various datasets using QGIS. It was a good timing, I need to produce a monthly bulletin on climate and vegetation update that required few maps to produce regularly for specific theme.\nThis is modified QGIS Automap from my office at the headquarter and adjusted for this bulletin. As the covered countries for the bulletin are spanning from South Asia to the Pacific, we divided the countries into 3 region/window analysis.\nBelow is the list\n\nRegion 1: TJK, KGZ, AFG and PAK\nRegion 2: IND, BTN, NPL, BGD, LKA, MMR, THA, KHM, LAO, VNM, PHL and DPRK\nRegion 3: IDN, TLS, PNG and SLB\n\nTo do a custom automatic mapping based on your case, you must prepare QGIS layout file, before continue the guideline below. This case, I have prepared *.qgz file before hand, list of products in csv format, symbology files and other supporting files.\n\nSetting up environment\nThe code for the automap is written in Python 3. It is recommended to use either the Miniconda3 (minimal Anaconda) or Anaconda3 distribution. The below instructions will be Anaconda specific (although relevant to any Python virtual environment), and assume the use of a bash shell.\n\n\nRequirements\n\nDownload and install Anaconda Python on your machine for macOS, Linux, or Windows.\n\nFollow Installing Anaconda on macOS, Linux, Windows.\nOr you can use Miniconda for macOS, Linux, or Windows. And follow the installation guideline for macOS, Linux, or Windows.\n\nDownload and install QGIS Long Term Release 3.16.8 on your machine for macOS, Linux, or Windows.\nDownload ‚Äúautomap_RBB‚Äù folder and it‚Äôs content via this link: https://github.com/wfpidn/automap_RBB/archive/refs/heads/main.zip.\nExtract it into your working directory, you will have ‚Äúautomap_RBB‚Äù folder.\nDownload geospatial data from this link: https://www.dropbox.com/s/ptfdp1p9ltrvwwa/_layers.zip?dl=0\nExtract above ‚Äúzip‚Äù file and move the contents to ‚Äúautomap_RBB/templates/_layers‚Äù\n\n\n\n\nConfigure the python environment\nA new Anaconda environment can be created using the conda environment management system that comes packaged with Anaconda. This step must only be done the first time. Once the environment has been created there is no need to do it again.\n\nNavigate to ‚Äúautomap_RBB‚Äù folder in your Terminal or Command-Prompt\nCreate the environment from the ‚ÄúautomapQGIS.yml‚Äù file.\nType conda env create -f automapQGIS.yml then execute.\nActivate the environment\nType conda activate automapQGIS then execute.\n\nThe environment now has been activated. The python scripts must be run inside this environment in order to work.\n\n\nProducing maps\nYou need to get the geospatial data (in this case I got climate and vegetation data from the headquarter). Usually the data in IDRISI format (*.rst) are uploaded in this folder, link: https://drive.google.com/drive/u/0/folders/1kmpsja6_5BYhj7AtxUPou3e6BIOVZTJk.\nThen you can easily convert it to GeoTIFF using GDAL. If you does not have GDAL in your machine, please install it via ‚Äúconda‚Äù.\nGet the data\n\nInstall GDAL, type in your Terminal or Command-Prompt: conda install -c conda-forge gdal\nDownload latest *.rst files from above link, let assume the result are inside Downloads folder (It can be in ‚Äú~/Downloads‚Äù or ‚ÄúC:/Downloads‚Äù) and your file is ‚Äúdrive-download-20210811T141722Z-001.zip‚Äù.\nUnzip file ‚Äúdrive-download-20210811T141722Z-001.zip‚Äù\nNavigate to folder ‚Äúdrive-download-20210811T141722Z-001‚Äù, and create a new folder called ‚ÄúTIF‚Äù\nConvert *.rst to *.tif by typing below script in your Terminal or Command-Prompt\nfor i in find \\*.rst; do gdal_translate -of GTiff -co COMPRESS=LZW -co PREDICTOR=1 \\(i ./TIF/\\)i.tif; done\nMove all the result (GeoTIFF files) inside TIF folder to ‚Äúautomap_RBB/products‚Äù. The files must respect the naming convention.\n\nIf your data already in GeoTIFF, you are good to go, no need to doing conversion as explain in above steps.\n\n\nRunning the script\nIn your Terminal or Command-Prompt,\n\nNavigate to the ‚Äúautomap_RBB‚Äù folder. Run the automap.py script by typing python automap.py\nBy default all *.tif files in the products folder will be mapped as *.png maps in the maps folder.\nThe maps will be stored in 3 different subfolders (Region1, Region2 and Region3) for each sub-region.\n\n\n\nOptions\nWhen running the script, 4 additional flags can be added:\n‚Äìselect\n\n\nA window with the available product types will appear so that the user can choose which products to map.\n\n\nBy default all of the products are mapped\n‚Äìoverwrite\n\n\nAll the products will be mapped\n\n\nBy default, only the products which are not present in the output folder will be mapped\n‚Äìinput\n\n\nSelect input folder with the .tiff files\n\n\nDefault is automap_RBB/products\n‚Äìoutput\n\n\nSelect output folder for the PNG maps\n\n\nDefault is automap_RBB/maps\nExample: python automap.py ‚Äì-select ‚Äìinputorpython automap.py ‚Äìoverwrite\nThe user can add a new product type.\n\nUpdate the ‚Äúproduct_types.csv‚Äù file in ‚Äúautomap_RBB/csv‚Äù\nYou should add the name of the product type (in capital letters), the title and the name of its color-ramp.\nCreate the associated color ramp (*.qml file) and add it in ‚Äúautomap_RBB/templates/_layer_styles‚Äù\n\n\nExample Maps\nRegion 1\n\nRegion 2\n \nRegion 3\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230228-unit-hydrographs.html",
    "href": "blog/20230228-unit-hydrographs.html",
    "title": "Unit Hydrographs",
    "section": "",
    "text": "Unit hydrographs are a fundamental tool in the analysis of floods and their impacts on watersheds. A unit hydrograph represents the response of a watershed to a unit of rainfall or snow melt over a period of time. By analyzing the shape of the unit hydrograph, hydrologists can estimate the runoff volume, timing, and distribution of peak flows within a watershed. This information is essential for understanding the risk of flooding and designing effective flood management strategies.\nAt its core, a unit hydrograph is a simple concept. It is a graph that shows the relationship between the input of precipitation or snowmelt and the resulting output of streamflow over time. The unit hydrograph concept assumes that the watershed responds to a given rainfall or snowmelt event in a predictable way, and that the response is proportional to the magnitude and duration of the event. By developing a unit hydrograph for a particular watershed, hydrologists can estimate the runoff that will occur for any given precipitation or snowmelt event, which is essential for predicting floods.\nUnit hydrographs are useful in a range of applications, from designing flood control structures to evaluating the impacts of land use changes on hydrologic processes. They can be developed using a variety of methods, including graphical, analytical, and numerical techniques. One common approach is to use data from a historical storm event to develop a synthetic unit hydrograph, which can then be used to estimate the response of the watershed to future storm events.\nTo develop a unit hydrograph, hydrologists first divide the watershed into subareas or subcatchments, each with its own unique hydrologic characteristics. They then estimate the time it takes for runoff to travel from each subcatchment to the watershed outlet, known as the travel time. The travel time is a function of the length, slope, and roughness of the flow path, as well as the velocity of the water.\nOnce the travel times for each sub catchment have been estimated, hydrologists can develop the unit hydrograph by adding together the contributions from each subcatchment. This is done by convolving the runoff from each subcatchment with a unit impulse function, which represents the response of the watershed to a unit of precipitation or snowmelt.\nThe resulting unit hydrograph shows the response of the watershed to a unit of rainfall or snowmelt over time, typically in the form of a graph. The time axis represents the time it takes for the runoff to reach the watershed outlet, while the discharge axis represents the volume of runoff at the outlet. The shape of the unit hydrograph reflects the characteristics of the watershed, including its size, shape, and hydrologic processes.\nBy analyzing the shape of the unit hydrograph, hydrologists can estimate the volume, timing, and distribution of peak flows within the watershed for any given storm event. This information is critical for designing flood control structures, such as dams and levees, and for developing flood warning systems. It can also be used to evaluate the impacts of land use changes on hydrologic processes and to assess the effectiveness of different flood management strategies.\nOverall, unit hydrographs are an essential tool for understanding floods at the watershed level. By analyzing the shape of the unit hydrograph, hydrologists can estimate the response of the watershed to different storm events, and develop effective flood management strategies to protect communities and infrastructure.\n\nExercise 1:\nObtain a Unit Hydrograph for a basin of 282.6 km2 of area using the rainfall and streamflow data tabulated below.\n\n\n\n\n\n\n\n\n\nTime (h)\nObserved Hydrograph (m3/s)\nTime (h)\nGross Precipitation (GRH)(cm/h)\n\n\n\n\n0\n160\n0 - 1\n0.25\n\n\n1\n150\n1 - 2\n2.75\n\n\n2\n350\n2 - 3\n2.75\n\n\n3\n800\n3 - 4\n0.25\n\n\n4\n1200\n\n\n\n\n5\n900\n\n\n\n\n6\n750\n\n\n\n\n7\n550\n\n\n\n\n8\n350\n\n\n\n\n9\n225\n\n\n\n\n10\n150\n\n\n\n\n11\n140\n\n\n\n\n\nAnswer\nWe will utilize Python to construct the process and calculate the Unit Hydrographs. First we need to define the variables.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the time interval (in hours)\ndelta_t = 1\n\n# Define the duration of the storm (in hours)\nt_storm = 8\n\n# Define the observed hydrograph data\nobs_hydro = [160, 150, 350, 800, 1200, 900, 750, 550, 350, 225, 150, 140]\n\n# Define the gross precipitation data\ngross_precip = [0.25, 2.75, 2.75, 0.25]\n\n# Define the basin area in square meters\nA = 282.6 * 1000000\n\n# Define the baseflow in m^3/s\nbaseflow = 150\nThen follow with calculation process\n# Calculate the number of time intervals\nn = len(obs_hydro)\n\n# Calculate the direct runoff hydrograph by subtracting the baseflow from the observed hydrograph\nrunoff = np.array(obs_hydro) - baseflow\n\n# Calculate the volume of direct runoff in m^3 by summing the direct runoff over the duration of the storm\nVDRH = np.sum(runoff) * delta_t * 3600\n\n# Calculate the effective precipitation by subtracting the initial abstraction (Ia) from the gross precipitation\nIa = 0.2 * np.sum(gross_precip) * t_storm\nPe = np.array(gross_precip) - Ia / delta_t\n\n# Calculate the volume of effective rainfall in m^3 by summing the effective precipitation over the duration of the storm\nVERH = np.sum(Pe) * delta_t * A / 100\n\n# Calculate the depth of direct runoff in meters by dividing the volume of direct runoff by the basin area\ndepth_DRH = VDRH / A\n\n# Create an empty array to store the unit hydrograph\nuh = np.zeros(n)\n\n# Calculate the unit hydrograph by normalizing the direct runoff hydrograph\nuh = runoff / depth_DRH / 100\n\n# Define the time vector\ntime = np.arange(n) * delta_t\n\n# Calculate the rainfall intensity\nrainfall_intensity = np.zeros(n)\nfor i in range(4):\n    rainfall_intensity[i] = gross_precip[i] * 100 / delta_t  # Convert cm/h to mm/h to mm/interval\nfor i in range(4, n):\n    rainfall_intensity[i] = gross_precip[3] * 100 / delta_t  # Convert cm/h to mm/h to mm/interval\nNext plot the result as a line chart using below code.\n# Plot the observed hydrograph, direct runoff hydrograph, unit hydrograph, and rainfall intensity\nplt.plot(time, obs_hydro, label='Observed Hydrograph')\nplt.plot(time, baseflow * np.ones(n), label='Baseflow')\nplt.plot(time, runoff, label='Direct Runoff Hydrograph')\nplt.plot(time, uh, label='Unit Hydrograph')\nplt.plot(time, rainfall_intensity, label='Rainfall Intensity')\nplt.xlabel('Time (hours)')\nplt.ylabel('Discharge (m^3/s) or Rainfall Intensity (mm/interval)')\nplt.legend()\nplt.show()\nIt will produce a chart below.\n\nWe can generate the table that constructs the above chart too.\n# Create the table\ntable_data = np.vstack((time, obs_hydro, baseflow * np.ones(n), runoff, uh, rainfall_intensity)).T\nheaders = ['Time (h)', 'OH (m^3/s)', 'Baseflow (m^3/s)', 'DRH (m^3/s)', 'UH (m^3/s/cm)', 'RI (cm/h)']\nprint('\\t'.join(headers))\nfor row in table_data:\n    print('\\t'.join(str(cell) for cell in row))\nThe script will generate table below\n\n\n\n\n\n\n\n\n\n\n\nTime (h)\nOH (m^3/s)\nBaseflow (m^3/s)\nDRH (m^3/s)\nUH (m^3/s/cm)\nRI (cm/h)\n\n\n\n\n0\n160\n150\n10\n2\n25\n\n\n1\n150\n150\n0\n0\n275\n\n\n2\n350\n150\n200\n40\n275\n\n\n3\n800\n150\n650\n130\n25\n\n\n4\n1200\n150\n1050\n210\n25\n\n\n5\n900\n150\n750\n150\n25\n\n\n6\n750\n150\n600\n120\n25\n\n\n7\n550\n150\n400\n80\n25\n\n\n8\n350\n150\n200\n40\n25\n\n\n9\n225\n150\n75\n15\n25\n\n\n10\n150\n150\n0\n0\n25\n\n\n11\n140\n150\n-10\n-2\n25\n\n\n\nNext is determining the duration D of the ERH associated with the UH obtained in step above\n# Calculate the volume of losses\nVGRH = np.sum(gross_precip) * t_storm * 3600 * A / 100\nVDRH2 = np.sum(runoff) * delta_t * 3600 * A / 100\nVLosses = (VGRH - VDRH2 - Ia * A / 100)\n\n# Calculate the f-index\ntr = t_storm * delta_t\nf_index = VLosses / tr\n\n# Calculate the ERH by subtracting the f-index from the gross precipitation\n# ERH = np.array(gross_precip) - f_index\nERH = np.zeros(n)\nERH[:4] = [0.0, 2.5, 2.5, 0.0]\n\n# Determine the duration of the effective rainfall hyetograph\nERH_duration = len(ERH[ERH &gt; 0]) * delta_t\nLast step is calculating the Predicted Hydrograph and visualizing the result as a line chart.\n# Convolve the ERH and UH to obtain the predicted hydrograph\nn_P = len(ERH)\ntime_P = np.arange(n_P) * delta_t\n\n# Pad the ERH array with zeros if its length is less than the UH length\nif n_P &lt; n:\n    ERH_padded = np.pad(ERH, (n - n_P, 0), mode='constant')\nelse:\n    ERH_padded = ERH\n\n# the conversion factor from m^3/s/m to m^3/s/mm by multiply uh with 0.0005\nQ = np.convolve(ERH_padded, 0.0005 * uh) * delta_t * 3600\n\n# Plot the predicted hydrograph\nplt.plot(time_P, Q[:n_P], label='Predicted Hydrograph')\nplt.xlabel('Time (hours)')\nplt.ylabel('Discharge (m^3/s)')\nplt.legend()\nplt.show()\nIt will produce a line chart below.\n\n\n\nExercise 2:\nUtilize the results of the Problem-1 to finish the Problem-2. Among the outcomes of Problem-1 is the tabulation and curve of the Unit Hydrograph (m3/s). Assuming that in the same watershed there are four effective rains (P1, P2, P3, and P4) with each effective rain having a duration of 2 hours, the short tabulation is as follows:\n\n\n\nTime (h)\nPm (cm)\n\n\n\n\n0 - 2\n2\n\n\n2 - 4\n3\n\n\n4 - 6\n1.5\n\n\n6 - 8\n0.5\n\n\n\nAnswer\nThe given code is an implementation of a hydrological model that predicts the runoff flow for a catchment area. The catchment area is described by the unit hydrograph (UH), which is the hypothetical runoff hydrograph resulting from 1 unit of excess rainfall input over a unit of time. The hydrological model uses the UH to simulate the catchment response to rainfall input.\nThe model requires the specification of four effective rainfall hyetographs (ERHs) that represent the four rainfall events with different intensities and durations. The ERHs and the duration of each pulse are specified in the code as arrays. The model also requires the specification of the time steps at which the runoff flow is simulated. The time steps are also specified as an array in the code.\nThe hydrological model is implemented using a table array that contains the values of the UH, the ERHs, and the calculated runoff flow values. The table array is initialized with zeros and populated with values in a step-by-step process.\n\nFirstly, the time column is populated with the time steps.\nSecondly, the Pm value is calculated for each ERH pulse by multiplying the ERH value by the pulse duration.\nThen, the UH column is populated with the given UH values.\nThe P1UH column is calculated by multiplying the Pm value of the first ERH pulse with the corresponding UH value.\nThe P2UH column is calculated by applying the convolution operation between the UH and the Pm value of the second ERH pulse. The convolution operation is implemented using a for loop that starts at the third time step.\nThe P3UH column is calculated in a similar way by applying the convolution operation between the UH and the Pm value of the third ERH pulse. The P3UH calculation starts at the fifth time step.\nThe P4UH column is calculated by multiplying the UH value by the Pm value of the fourth ERH pulse. The P4UH calculation starts at the seventh time step.\nThe DRH (direct runoff hydrograph) column is calculated by summing up the values in the P1UH, P2UH, P3UH, and P4UH columns for each time step.\nThe Baseflow column is populated with the constant value of 150.\nThe Total column is calculated by summing up the values in the DRH and Baseflow columns for each time step.\n\nSee below implementation using python.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the given UH\nuh = np.array([0, 40, 130, 210, 150, 120, 80, 40, 15, 0])\n\n# Define the given ERH\nerh = np.array([2.0, 3.0, 1.5, 0.5])\n\n# Define the duration of each ERH pulse\nduration = 1  # hours\n\n# Define the time steps\nt = np.arange(1, 17)\n\n# Initialize table array\ntable = np.zeros((len(t), 9))\n\n# Populate time column\ntable[:, 0] = t\n\n# Calculate Pm for each ERH pulse\npm = np.array(erh) * duration\n\n# Populate UH column\ntable[:, 1] = np.pad(uh, (0, len(t) - len(uh)), 'constant', constant_values=(0,))\n\n# Populate P1*UH column\nfor i in range(len(t)):\n    if i &lt; len(uh):\n        table[i, 2] = pm[0] * uh[i]\n\n# Populate remaining P2*UH values\nfor i in range(3, len(t)):\n    if table[i, 3] == 0:\n        table[i, 3] = pm[1] * table[i-2, 1]\n\n# Populate P3*UH column\nfor i in range(5, len(t)):\n    if table[i, 4] == 0:\n        table[i, 4] = pm[2] * table[i-4, 1]\n\n# Populate P4*UH column\nfor i in range(7, len(t)):\n    if table[i, 5] == 0:\n        table[i, 5] = pm[3] * table[i-6, 1]\n\n# Populate DRH column\nfor i in range(len(t)):\n    table[i, 6] = np.sum(table[i, 2:6])\n\n# Populate Baseflow column\ntable[:, 7] = 150\n\n# Populate Total column\ntable[:, 8] = table[:, 6] + table[:, 7]\n\n# Print the table\nprint(\"{:&lt;10} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12}\".format(\n    \"Time(h)\", \"UH(m3/s/cm)\", \"P1*UH(m3/s)\", \"P2*UH(m3/s)\", \"P3*UH(m3/s)\", \n    \"P4*UH(m3/s)\", \"DRH(m3/s)\", \"Baseflow(m3/s)\", \"Total(m3/s)\"\n))\nfor i in range(len(t)):\n    print(\"{:&lt;10} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12} {:&lt;12}\".format(\n        table[i, 0], table[i, 1], table[i, 2], table[i, 3], table[i, 4], \n        table[i, 5], table[i, 6], table[i, 7], table[i, 8]\n    ))\nprint('_______________________________________________')\nFinally, the table array is printed with the column headings and the calculated values for each time step. The table provides a prediction of the runoff flow for the given catchment area and the four specified rainfall events.\nHere is the table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (h)\nUH (m3/s/cm)\nP1*UH (m3/s)\nP2*UH (m3/s)\nP3*UH (m3/s)\nP4*UH (m3/s)\nDRH (m3/s)\nBaseflow (m3/s)\nTotal (m3/s)\n\n\n\n\n1\n0\n0\n\n\n\n0\n150\n150\n\n\n2\n40\n80\n\n\n\n80\n150\n230\n\n\n3\n130\n260\n0\n\n\n260\n150\n410\n\n\n4\n210\n420\n120\n\n\n540\n150\n690\n\n\n5\n150\n300\n390\n0\n\n690\n150\n840\n\n\n6\n120\n240\n630\n60\n\n930\n150\n1080\n\n\n7\n80\n160\n450\n195\n0\n805\n150\n955\n\n\n8\n40\n80\n360\n315\n20\n775\n150\n925\n\n\n9\n15\n30\n240\n225\n65\n560\n150\n710\n\n\n10\n0\n0\n120\n180\n105\n405\n150\n555\n\n\n11\n\n\n45\n120\n75\n240\n150\n390\n\n\n12\n\n\n0\n60\n60\n120\n150\n270\n\n\n13\n\n\n\n22.5\n40\n62.5\n150\n212.5\n\n\n14\n\n\n\n0\n20\n20\n150\n170\n\n\n15\n\n\n\n\n7.5\n7.5\n150\n157.5\n\n\n16\n\n\n\n\n0\n0\n150\n150\n\n\n\nNext plot the result as a line chart using below code.\n# Plot the chart\nplt.plot(t, table[:, 1], label='UH')\nplt.plot(t, table[:, 2], label='P1*UH')\nplt.plot(t, table[:, 3], label='P2*UH')\nplt.plot(t, table[:, 4], label='P3*UH')\nplt.plot(t, table[:, 5], label='P4*UH')\nplt.plot(t, table[:, 6], label='DRH')\nplt.plot(t, table[:, 7], label='Baseflow')\nplt.plot(t, table[:, 8], label='Total')\n\n# Set chart title and labels\nplt.title('Hydrograph Components')\nplt.xlabel('Time (hours)')\nplt.ylabel('Discharge (m3/s)')\n\n# Show the legend\nplt.legend()\n\n# Show the chart\nplt.show()\nIt will produce a chart below.\n\n\n\nExercise 3:\nCharacteristics of two catchments M and N measured from a map are given below:\n\n\n\nItem\nCatchment M\nCatchment N\n\n\n\n\nLca\n76 km\n52 km\n\n\nL\n148 km\n106 km\n\n\nA\n2718 km2\n1400 km2\n\n\n\nFor the 6-h unit hydrograph in catchment M, the peak discharge is at 200 m3/s and occurs at 37 h from the start of the rainfall excess. Assuming the catchments M and N are meteorologically similar; determine the elements of the 6-h synthetic unit hydrograph for catchment N by using Snyder‚Äôs method.\nHere‚Äôs a Python implementation to solve the problem:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Given data\nLca_M = 76  # km\nL_M = 148  # km\nA_M = 2718  # km^2\nLca_N = 52  # km\nL_N = 106  # km\nA_N = 1400  # km^2\nT_M = 6  # hours\nQ_M = 200  # m^3/s\nt_peak_M = 37  # hours\n\n# Snyder's unit hydrograph parameters\nK_M = 0.0136 * A_M**0.77 * L_M**0.385 / T_M\nK_N = K_M * (A_N / A_M)**0.385 * (Lca_N / Lca_M)**0.54\nTw_M = 0.78 * L_M**0.5\nTw_N = Tw_M * (L_N / L_M)**0.8\n\n# Compute the ordinates of the unit hydrograph for catchment M\ntime_M = np.arange(0, t_peak_M + Tw_M + T_M, T_M)\nUH_M = (1 / (K_M * Tw_M)) * (time_M / Tw_M)**0.5 * np.exp(-time_M / Tw_M)\n\n# Scale the unit hydrograph to peak discharge Q_M\nUH_M *= Q_M / UH_M.max()\n\n# Compute the ordinates of the unit hydrograph for catchment N\ntime_N = np.arange(0, t_peak_M + Tw_N + T_M, T_M)\nUH_N = (1 / (K_N * Tw_N)) * (time_N / Tw_N)**0.5 * np.exp(-time_N / Tw_N)\n\n# Scale the unit hydrograph to peak discharge Q_N\nQ_N = Q_M * (A_N / A_M)\nUH_N *= Q_N / UH_N.max()\n\n# Print out the results\nprint(f'K_M = {K_M:.2f} m^(1/3)/s, Tw_M = {Tw_M:.2f} hours')\nprint(f'K_N = {K_N:.2f} m^(1/3)/s, Tw_N = {Tw_N:.2f} hours')\nprint('6-hour synthetic unit hydrograph for catchment M:')\nprint(UH_M)\nprint(f'Peak discharge: {Q_M:.2f} m^3/s')\nprint('6-hour synthetic unit hydrograph for catchment N:')\nprint(UH_N)\nprint(f'Peak discharge: {Q_N:.2f} m^3/s')\n\n# Create a table of the synthetic unit hydrograph\ntable_data = np.column_stack((time_M, UH_M, UH_N))\nheader = '{:&lt;10} {:&lt;20} {:&lt;20}'.format('Time (h)', 'UH for M (m3/s)', 'UH for N (m3/s)')\nprint(header)\nprint('-' * len(header))\nfor i in range(len(time_M)):\n    print('{:&lt;10.1f} {:&lt;20.2f} {:&lt;20.2f}'.format(table_data[i, 0], table_data[i, 1], table_data[i, 2]))\nIt will print the result below:\nK_M = 6.84 m^(1/3)/s, Tw_M = 9.49 hours\nK_N = 4.32 m^(1/3)/s, Tw_N = 7.27 hours\n6-hour synthetic unit hydrograph for catchment M:\n[ 0. 200. 150.29306573 97.80893338 60.01251688 35.65256641 20.75273644 11.9108489 6.76601005]\nPeak discharge: 200.00 m^3/s\n6-hour synthetic unit hydrograph for catchment N:\n[ 0. 103.01692421 63.79260217 34.21075734 17.29731912 8.46799408 4.06179415 1.92104724 0.89925065]\nPeak discharge: 103.02 m^3/s\nTime (h) UH for M (m3/s) UH for N (m3/s)\n\n0.0 0.00 0.00\n6.0 200.00 103.02\n12.0 150.29 63.79\n18.0 97.81 34.21\n24.0 60.01 17.30\n30.0 35.65 8.47\n36.0 20.75 4.06\n42.0 11.91 1.92\n48.0 6.77 0.90\nLet‚Äôs visualise it as a chart.\n# Plot the synthetic unit hydrographs\nplt.plot(time_M, UH_M, label='Catchment M')\nplt.plot(time_N, UH_N, label='Catchment N')\nplt.xlabel('Time (h)')\nplt.ylabel('Ordinates of unit hydrograph (m$^3$/s)')\nplt.title('6-hour Unit Hydrograph for Catchments M and N')\nplt.legend()\nplt.show()\nIt will produce a chart below:\n\n\n\nStrength and Weakness of Unit Hydrographs:\nThe Unit Hydrographs is a commonly used technique for flood analysis on a watershed. Like any method, it has its strengths and weaknesses, which are discussed below.\nStrengths:\n\nUnit hydrographs are a simple yet powerful tool for analyzing the response of a watershed to precipitation or snowmelt events. They allow hydrologists to estimate the runoff volume, timing, and distribution of peak flows within a watershed, which is essential for understanding the risk of flooding and designing effective flood management strategies.\nUnit hydrographs can be developed using a variety of methods, including graphical, analytical, and numerical techniques. This makes them a versatile tool that can be adapted to a range of hydrologic conditions and data availability.\nBy analyzing the shape of the unit hydrograph, hydrologists can estimate the travel times and storage capacities of different subcatchments within the watershed. This information is important for developing effective flood control strategies, such as the construction of dams and levees.\nUnit hydrographs can be used to assess the impacts of land use changes on hydrologic processes, such as urbanization or deforestation. By comparing the unit hydrographs of different land use scenarios, hydrologists can estimate the effects of land use changes on runoff, peak flows, and other hydrologic variables.\nUnit hydrographs are a widely accepted and established tool in the field of hydrology. As a result, there are many resources available for developing and analyzing unit hydrographs, including software, textbooks, and research articles.\n\nWeaknesses:\n\nDeveloping an accurate unit hydrograph requires a significant amount of data, including precipitation records, streamflow measurements, and topographic data. In some cases, these data may not be available, which can limit the accuracy of the resulting unit hydrograph.\nThe assumptions underlying the unit hydrograph concept may not always hold true, particularly in complex or heterogeneous watersheds. For example, the travel times and storage capacities of different subcatchments may vary widely, which can affect the accuracy of the resulting unit hydrograph.\nThe accuracy of the unit hydrograph is dependent on the accuracy of the input data, including precipitation and streamflow measurements. Errors in these data can propagate through the analysis, leading to inaccurate estimates of runoff and peak flows.\nUnit hydrographs do not capture the complex interactions between different hydrologic processes within the watershed. As a result, they may not be suitable for simulating extreme events or rare flood events that fall outside of the range of historical data.\nDeveloping a unit hydrograph requires a significant amount of expertise and experience in the field of hydrology. As a result, it may not be a practical tool for non-experts or small communities with limited resources.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200630-monthly-weather-data-from-era5-and-fldas.html",
    "href": "blog/20200630-monthly-weather-data-from-era5-and-fldas.html",
    "title": "Monthly weather data from ERA5 and FLDAS",
    "section": "",
    "text": "Since 2015, considerable amount of new data (high resolution satellite-based product combined with social media data) developed using new technology through artificial intelligence (machine learning, computer vision, etc.) available for public, but further utilisation remains lack.\nNow in cloud computing era, we can easily get instant access to massive weather and climate data and do processing from the browser via Google Earth Engine platform.\nLet‚Äôs try to access monthly weather data from NASA and ECMWF:\n\nhttps://developers.google.com/earth-engine/datasets/catalog/NASA_FLDAS_NOAH01_C_GL_M_V001\nhttps://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_MONTHLY\n\nImport above data from EE Data Catalog\nAccess the precipitation data, visualise it and add to the map layer\n\nPrecipitation unit in FLDAS data: ‚Äúkg m-2 s-1‚Äù, is the International System of Units (SI) used for precipitation. While 1kg = 1Liter = 10e6 mm3 and 1m2 = 10e6 mm2. Therefore 1kg of rain over 1 m2 is equivalent to 1mm.\nMultiply the rate (given in kg m-2 s-1) by 1 to get the rainfall in terms of mm/second. Or multiply the value by 24*60*60 to get the total rainfall in mm per day. 1 kg/m2/s-1 = 86400 mm/day\nFor the monthly value, daily value can be multiplied by the number of days e.g.¬†86400 x kg/m2/s x number of days in a month (28/29/30/31).\nPrecipitation in ERA5 data is in m, so we need to multiply it by 1000 to get mm.\n\n\nTemperature data\n\nKelvin to Celsius temperature conversion can be done by subtracting 273.15 from the Kelvin temperature in order to obtain the required celsius temperature. The Kelvin scale and the celsius scale are two different temperature scales which are related by the following formula: K = C + 273.15 (or) C = K - 273.15.\n\n\nRelative Humidity\n\nFLDAS only provide Specific Humidity data, so we need to convert it into Relative Humidity. From equation in this page: https://earthscience.stackexchange.com/a/2361 we can easily calculate RH as follow:\nRH = 100 * (w/ws) ‚âà 0.263 * p * q * [exp‚Å°(17.67(T‚àíT0)T‚àí29.65)]^-1\n\n\nWind Speed and Direction\nFrom Google Earth Engine Developer Groups, Gennadii Donchyts share his code on how to visualise wind vector. Lets adapt the code to use with ERA5 data.\n\nFinally, if you want to download the data, you can add below code:\nLink for the full code\nNotes: The code was compiled from various source (GEE help, GEE Groups, StackExchange)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20081225-potential-soil-loss.html",
    "href": "blog/20081225-potential-soil-loss.html",
    "title": "Potential soil loss",
    "section": "",
    "text": "During my assignment with ADB project called Earthquake and Tsunami Emergency Support Project (ETESP) - Package 38, I am working on development of potential soil loss map.\nPotential soil loss based on RUSLE model, only takes into account erosional factors and does not consider the effect of sediment deposition. Annual erosion is calculated by combining spatially explicit raster maps according to this formula:\n\\[E = R \\cdot K \\cdot LS \\cdot C \\cdot P\\]\nWhere: \\(E\\) is Annual Erosion, \\(R\\) is Rainfall Erosivity Factor, \\(K\\) is Soil Erodibility Factor, \\(LS\\) is Slope Factor, \\(C\\) is Vegetative Cover and \\(P\\) is Management Factor.\nThe SRTM30 Dataset was used to calculate the slope factor (LS). The Landsat land cover dataset, classified according to the IGBP (International Geosphere-Biosphere Programme) scheme (based on¬†http://duckwater.bu.edu/lc/mod12q1.html), was used to calculate the vegetative management factor (CP). K was calculated based on the USDA global soil sub order dataset, available through the NRCS (http://soils.usda.gov/technical/classification/taxonomy/). The erodibility of the soil is determined by the following soil properties: organic matter content, percentage of silt and fine sand, percentage of sand, soil texture and permeability. Using interpolated daily global rainfall data from CMORPH ‚Äì NOAA (ftp.cpc.ncep.noaa.gov), the average rainfall erosivity index (R) was calculated. The equation to be used to calculate R is based on ILWIS application guide chapter 23 (ftp.itc.nl/pub/ilwis/pdf/appch23.pdf).\n \nThe calculated potential soil loss is expressed at scale 1:250.000. The model resuires a higher resolution input for more accurate calculation of potential soil loss.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20190708-pixel-wise-regression-between-rainfall-and-sea-surface-temperature.html",
    "href": "blog/20190708-pixel-wise-regression-between-rainfall-and-sea-surface-temperature.html",
    "title": "Pixel-wise regression between rainfall and sea surface temperature",
    "section": "",
    "text": "General sensitivity of rainfall to sea surface temperature (sst) changes in NINO region\nClimate variability directly influences many aspects of food and nutrition security, particularly food availability and access. Variation in rainfall is a common element of many natural disasters ‚Äì droughts, floods, typhoons and tsunamis ‚Äì and is influenced by global, regional and/or local factors.\nGlobal climate factors including El Ni√±o, La Ni√±a and the dipole mode; regional factors include monsoon circulation, the Madden-Julian oscillation and fluctuations in the surface temperature of the Indonesian Sea; and local factors can include elevation, island position, the circulation of land and sea breezes, and land cover.\nThe level of climate risk is measured based on the strength of ENSO signal on rainfall variability using correlation analysis. This approach is applied because production loss of food crops in Indonesia is closely associated with the ENSO phenomena. El Nino years is normally associated with drought years, while La-Nina is often related to wet years which can cause flood hazards. The correlation analysis is applied to monthly rainfall anomaly and sea surface temperature anomaly in NINO3.4.\nThe NINO-3.4 region is optimal for monitoring El Ni√±o-Southern Oscillation (ENSO) and its impacts in Indonesia and possibly Southeast Asia. Then I would like to see the correlation between rainfall and SST by looking the change in rainfall with 1¬∞ increase in sea surface temperature (SST) of NINO-3.4 region, as a signal for moderate El Ni√±o.\nMethod\nFor this analysis, I used monthly rainfall from CHIRPS - CHC UCSB: https://www.chc.ucsb.edu/data/chirps and SST anomaly from ERSST v4 - NOAA: https://www.ncdc.noaa.gov/teleconnections/enso/indicators/sst/ for year 1981 - 2017. Rainfall data available in GeoTIFF, but SST is in text format. In order to do pixel-wise regression between two rasters, both data must available in GeoTIFF raster format and in the same dimension (width and height).\nAs the SST data is only from single location in NINO3.4 then I need to do some data tweak (I used ArcGIS Model Builder to create SST anomaly raster):\n\nPrepare list of SST anomaly data from above link and save as Excel (*.xls) file, and add ID as first column.\nAdd Iterate Row Selection to Model Builder, choose sst_anomaly.xls as Input Table and ID as field.\nAdd Get Field Value from Model Only Tools within the Model Builder, add SSTANOM asInput Table and SSTANOM column as Field.\nAdd 1 of rainfall raster as example and will use this data as a template for SST to generate new raster with same dimension as rainfall.\nBy using Raster Calculator toolbox, and add\n(precip_anom_0.tif / precip_anom_0.tif) * Value2\nand give name sst_%Value%.tif as output raster.\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. Model Builder for generating SST raster [/caption]\nTime series regression will apply to indicate the correlation between rainfall anomaly in each area to anomaly of SST in the Pacific Ocean which represent ENSO signals. And doing a pixel-wise regression between two raster time series can be useful for several reasons, for example:\n\nFind the relation between vegetation and rainfall for each pixel, e.g.¬†a low correlation could be a sign of degradation.\nDerive regression coefficients to model depending variable using the independent variable (e.g.¬†SST with rainfall data)\n\nUsing R Statistics, it is easy and fast to do pixel-wise time series regression between rainfall and sea surface temperature. Then put everything in one single stack and derive rasters showing the slope, intercept and R¬≤ of the two time series as pixel values:\nThen calculate the slope using below function\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú866‚Äù] Figure 2. Slope map [/caption]\nSlope map above shows the changes in monthly rainfall likely to result from a 1¬∞C change in sea surface temperature (SST). Areas in white exhibit very high negative change in rainfall: those in light to dark green exhibit very low negative changes to normal. Each square in the grid represents a 5.6 x 5.6 km square.\nFor correlation and p-value, we can used Gridded Correlation of Time Series Raster Data that available in Hakim Abdi‚Äôs Github Gist: https://gist.github.com/hakimabdi/7308bbd6d9d94cf0a1b8\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 3. Correlation and p-value map [/caption]\nTimor-Leste data was used to test the script, and its worked. So we can apply it using data from other areas or with bigger dimension.\nReference:\n\nhttps://matinbrandt.wordpress.com/2014/05/26/pixel-wise-regression-between-two-raster-time-series/\nhttps://matinbrandt.wordpress.com/2013/11/15/pixel-wise-time-series-trend-anaylsis-with-ndvi-gimms-and-r/\nhttps://www.hakimabdi.com/blog/test-pixelwise-correlation-between-two-time-series-of-gridded-satellite-data-in-r\n\nNotes:\n\nI completed data manipulation process for SST using ArcGIS Model Builder in 4 Dec 2018, after got feedback and insight from discussion in GIS ID Telegram group.\nThis works done by Anggita Annisa, a final year student from Department of Statistics - IPB who is doing an internship under my supervision in VAM unit of WFP Indonesia from June - August 2019.\n\nUpdate - 17 May 2020\nData and script: https://github.com/wfpidn/Pixel-wise-Regression\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200829-kriging-and-idw-interpolation-in-gee.html",
    "href": "blog/20200829-kriging-and-idw-interpolation-in-gee.html",
    "title": "Kriging and IDW interpolation in GEE",
    "section": "",
    "text": "I have automatic weather station coordinates from BMKG, along with example data on precipitation accumulation for 1 - 10 Jan 2017 in csv format with column structure: Lon, Lat, Rainfall.\nI uploaded this data into GEE assets, available as Feature Collection and accessible from this address: ‚Äúusers/bennyistanto/datasets/shp/rainfall_201701d1‚Äù\n\nIs it possible to do Kriging and Inverse Distance Weighting (IDW) interpolation using Google Earth Engine?\nYes, it is. Follow below script.\nFull link\nSee below comparison from ArcGIS and GEE for both method. The pattern is slightly different due to GEE‚Äôs layer visualised using stretched colour while ArcGIS‚Äôs layer visualised using classified colour.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20211211-mean-annual-temperature-and-number-of-hot-days-in-a-year.html",
    "href": "blog/20211211-mean-annual-temperature-and-number-of-hot-days-in-a-year.html",
    "title": "Mean annual temperature and number of hot days in a year",
    "section": "",
    "text": "This week I got request from my colleagues to calculate mean annual temperature and number of hot days in a year using MODIS Daily Land Surface Temperature (LST) data from 2000 until 2020 for Indonesia and Latin America Countries. The first thing that comes to mind is to use Google Earth Engine (GEE).\nFor this task, I will use MOD11A1.006 Terra Land Surface Temperature and Emissivity Daily Global 1km - https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD11A1. From the description: The MOD11A1 V6 product provides daily land surface temperature (LST) and emissivity values in a 1200 x 1200 kilometer grid. The temperature value is derived from the MOD11_L2 swath product. Above 30 degrees latitude, some pixels may have multiple observations where the criteria for clear-sky are met. When this occurs, the pixel value is the average of all qualifying observations. Provided along with both the day-time and night-time surface temperature bands and their quality indicator layers are MODIS bands 31 and 32 and six observation layers.\n\nFrom above picture, we can see MOD11A1 has many bands. For this exercise, I only use 4 bands: LST_Day_1km, QC_Day, LST_Night_1km and QC_Night.\nIf you would like to test, you can access link from GEE: https://code.earthengine.google.com/?scriptPath=Examples:Datasets/MODIS_006_MOD11A1\n\nLand Surface Temperature\nOk, let start! I need to define the geographic domain and the time range.\nAdd boundary layer as a guidance when overlaying LST data, and I can use US Department of State LSIB 2017 data in Earth Engine data catalog. https://developers.google.com/earth-engine/datasets/catalog/USDOS_LSIB_SIMPLE_2017?hl=en\nI need a function to translate MODIS LST data to Celsius, with equation: Digital Number (DN) to Celsius = DN* 0.02 -273.15. I found an example of function written by Gennadii Donchyts at GEE Developers Group https://groups.google.com/g/google-earth-engine-developers/c/ebrSLPCjZ8c/m/TiL2gtSnDgAJ\nSlightly different with code from above link, I will create two function, for LST_Day_1km and LST_Night_1km variable.\nNext, I will add MOD11A1, select the LST and pre-process Quality Control Band\nThe tricky part will be considering only pixel with good quality from QC bitmask. From the catalog, we can see:\nQC Bitmask\n---\nBits 0-1: Mandatory QA flags\n0: LST produced, good quality, not necessary to examine more detailed QA\n1: LST produced, other quality, recommend examination of more detailed QA\n2: LST not produced due to cloud effects\n3: LST not produced primarily due to reasons other than cloud\nThen I can use below script to select only the best quality images for each data. Below is the function for each Day and Night data.\nAs alternative way to get good quality image, I can use bitwiseExtract function from Daniel Wiell\nNext, let‚Äôs apply above function to get all the image collection cleaned, only considering pixel with good quality. After that we can do mosaic Day and Night for further analysis.\nNow I can calculate the annual Mean, Max and Min using below script.\n\n\nHot Days\nTo get number of hot days in a year, I will use 35 degree Celsius as a threshold, so every LST exceeding the threshold, will added to new collection and later will summed to get total number in a year.\nThis case I try to get number of hot days using LST Day (average day time temperature) and LST Mean (average daily temperature - 24h average)\n\n\nNumber of Consecutive Hot Days\nAlthough this is not part of my objective, I found a script to calculate number of consecutive hot days from Jamon Van Den Hoek at GEE Developers Group and I think this is still relevant with what I have done. So I decided to put this script too, maybe someday I need it.\n\n\nSymbology, Downloads and Legend\nIt‚Äôs time to visualize all the result and displaying into map with legend, and download it via Google Drive.\nFull script available here: https://code.earthengine.google.com/f2109fc305e987b97503146d4c209871 (Update 22 Dec 2021)\nAnd below is the example results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20040708-model-biomassa-dan-neraca-air.html",
    "href": "blog/20040708-model-biomassa-dan-neraca-air.html",
    "title": "Model biomassa dan neraca air",
    "section": "",
    "text": "Model ini merupakan tugas akhir mata kuliah Model Simulasi Pertanian di Semester 6. Melanjutkan model sebelumnya yang telah diajarkan pada praktikum, kali ini saya menambahkan beberapa variable sehingga pengguna dapat:\n\nMemilih jenis tanaman, yang sudah dilengkapi dengan Suhu Dasar dan Heat Unit.\nMemilih tanggal tanam\nMemilih jenis tanah, yang berkaitan dengan Titik Layu Permanen dan Kapasitas Lapang\nMenentukan jenis irigasi\nMenentukan nilai Indeks Luas Daun, Kedalaman Tanah, Kadar Air Tanah di awal tanam, Kemiringan Lahan\n\nSelain itu pengguna juga dapat mencoba data iklim dari beberapa stasiun cuaca yang disediakan, seperti gambar di bawah\n\nGambar di bawah ini merupakan tampilan dari model biomassa dan neraca air.\n\nScript di bawah ini digunakan untuk menghitung neraca air pada model.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20050327-termodinamika-dalam-pertanian-rumahkaca.html",
    "href": "blog/20050327-termodinamika-dalam-pertanian-rumahkaca.html",
    "title": "Termodinamika dalam pertanian rumahkaca",
    "section": "",
    "text": "Makalah berikut merupakan tugas dari mata kuliah Termodinamika di semester 4, dibimbing oleh Hanedi Darmasetiawan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210830-upgrade-mbp-mc724-early-2011.html",
    "href": "blog/20210830-upgrade-mbp-mc724-early-2011.html",
    "title": "Upgrade the MBP MC724 Early 2011",
    "section": "",
    "text": "This year is my 10th year using MacBook Pro 13‚Äù Early 2011. I bought my Mac on 5 Jun 2011 at Ibox Menteng Central with original specification 13.3‚Äù 2.7GHz i7/4GB RAM/500GB HDD with OS X Snow Leopard 10.6.6, it costed around USD1,600* or equal to IDR14,149,000 according to the receipt. [*based on the exchange rates on June 2011]\n2-weeks after using it, I got new Mac replacement because the letter P on the keyboard was mallfunction. I should wait the new unit for a week, because it‚Äôs came from the Singapore. So far the process was smooth and no hasle.\nSince then, I use my Mac to support my activities on earth observation, geospatial and climate analytics, both using OS X and Windows 7 running in Parallels Desktop 6. It was very good experience and I could achieve many milestone on my careers.\nDuring 2011 - 2018, I am gradually upgrading my Mac and buy some tools, it costs a lot of money, but I also get a decent compensation for the work I do. Below is the complete list:\n\nMacbook Pro, MC724LL. IDR14,149,000\nApple Care, MD015FE. IDR1,999,000\nMini DisplayPort to VGA adapter. MB572Z. IDR300,000\nApple Magic Mouse, MB829ZM. IDR699,000\nCORSAIR Mac Memory 2 x 8GB DDR3 1333MHz PC-12800. IDR1,520,200\nBaseqi Ninja Stealth Drive for MacBooks 103A. IDR352,602\nCORSAIR Force Series LE 960GB SATA3 6Gb/s SSD. IDR4,725,000\nSSD/HDD Caddy 9.5mm for DVD slot. IDR30,000\nPatriot P200 2TB 2.5‚Äù SATA3 6Gb/s SSD. IDR2,775,000\nBattery for A1278, 3 times, each costed IDR750,000. Total IDR2,250,000\n\nIn addition to the hardware, I also bought a lot of original software that supports my work. Some of them are (I recommend this as the essential paid application for your Mac):\n\n1Password\nAlfred\nBartender\nBetterZip\nCleanMyMac\nDiskSensei\nFolx Pro\niMazing\nOmniGraffle\nParagon NTFS\nParallels Desktop\nTotal Finder\nTransmit\n\nIn early 2019, my Mac experiencing a serious problem, unable to charge. After discussing with friend and finding similar problem at several Mac forums, I decided to change the integrated circuit (IC) charging controller. Good friend in OSX telegram group recommend me to go to Bengkel Mac in North Jakarta.\nIt was a good decision, as they offer the lowest price of all the places I surveyed, and they promised to finish it within 3 working days. It costed IDR1,300,000\nIn early 2021, my Mac starting lost the Bluetooth and experiencing intermittent on the WiFi or have failed completely. I started looking for Bluetooth card replacement, but most of the review is makes me unhappy, because it doesn‚Äôt completely solve the problem.\nOther problems, over the past few months my Mac has been running consistently hotter and hotter. I have try most of the recommended action in the forums, from blow the dust, reset SMC, NVRAM and did some application checking via Activity Monitor. And the CPU temperature still high, 100¬∞C.\nI decided to clean off the old thermal paste, apply a new layer using Arctic Silver 5 following awesome guideline from IFIXIT, and follow on how to remove the logic board beforehand. And now the CPU temperature is 56¬∞C during idle, and around 80¬∞C when I use it to process simple analysis.\nWeeks ago I found 802.11ac + Bluetooth 4.2 for Unibody MacBook Pro sell by Subtle Design, very expensive USD198, but a lot better than other similar product in local market place, as they support newer wireless and bluetooth standards than the originals. From their reviews seems promising.\nIt takes 7-days for DHL express to send the package from Austin, TX, US to Bogor, Indonesia. I got the package today and start to install the component, it‚Äôs very easy and they also provide the guideline just in case you are not sure how to do it.\nIt‚Äôs really nice to see your old MacBook Pro now have no problems on Bluetooth and WiFi, having some feature which previously only available to MacBook Pro release after 2012. Some of these features are:\n\nNew Bluetooth 4.2, chipset 20703A1\n802.11ac faster wireless\nHandoff enabled\nÔ£ø Watch auto unlock\nThe old Mac now can recognised new device (latest iPhone or Mac) on AirDrop\n\nI know my old Mac may be slow compared to the new one but it might extend the life for a while yet. The new ones are just too expensive.\nI hope it can last a few more years.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20081109-model-simulasi-penyakit-busuk-daun-tanaman-kentang.html",
    "href": "blog/20081109-model-simulasi-penyakit-busuk-daun-tanaman-kentang.html",
    "title": "Model simulasi penyakit busuk daun tanaman kentang",
    "section": "",
    "text": "User interface model simulasi penyakit busuk daun (late blight) pada tanaman kentang (Solanum tuberosum L.) dibangun menggunakan Visual Basic 6.\nModel ini membutuhkan input data cuaca (hujan, suhu minimum dan maksimum, kelembaban relatif dan lama penyinaran) harian selama setahun.\nTerdapat dua opsi simulasi:\n\nPerlakuan standar dengan penyemprotan fungisida setiap minggu\nBukan perlakuan standar\n\nTanpa penyemprotan\nDengan penyemprotan pada hari setelah tanam (HST): xx\n\n\nModel simulasi ini mempunyai 3 sub model:\n\nNeraca air, yang akan menghasilkan informasi tentang fluktuasi kadar air tanah tersedia (mm/hari), evapotranspirasi aktual (mm/hari) dan intersepsi daun (mm/hari)\nPerkembangan, yang akan menghasilkan informasi tentang jumlah hari selama fase perkembangan (tanam-muncul tunas, vegetatif, pembentukan umbi, pengisian umbi dan pematangan umbi), thermal heat unit, dan proporsi alokasi biomassa (%).\nPertumbuhan, yang akan menghasilkan informasi indeks luas daun (ILD/LAI), jumlah biomassa untuk daun, batang, akar, umbi dan total keseluruhan dalam ton/ha.\n\nHasil luaran akhir dari model simulasi ini adalah informasi peluang tanaman terinfeksi (%) dan intensitas serangan (%).\nPenyusunan model ini dipimpin oleh:\nDr.¬†Ir. Handoko | Laboratorium Meteorologi Pertanian IPB / Seameo BIOTROP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20191102-sf-innovation-bootcamp-2019.html",
    "href": "blog/20191102-sf-innovation-bootcamp-2019.html",
    "title": "SF Innovation Bootcamp 2019",
    "section": "",
    "text": "Last month I had the opportunity to attend 2019 WFP Innovation Accelerator Bootcamp in San Francisco, CA, USA.\nFor one week, 55 innovators from around the world joined in San Francisco to work alongside leading tech companies and experts. They were divided into six Early Stage innovation teams looking to build and test their ideas, and five advanced stage Scale-Up projects who had already proven concepts and were looking to scale their impact and reach.\nEleven outstanding ideas were presented. Among the Early Stage Start-Ups, projects included HungerMapLive, an AI-assisted crisis map to provide humanitarian responders with data to predict hunger crises before they happen; the GrainATM that dispenses grains to create access to food in rural India; Fenik‚Äôs off-grid refrigeration unit that requires no energy; Roambee‚Äôs Internet of Things tracking system for delivery to the last mile and every point in between; and the Tamwini digital ID app that allows people in Iraq to access services without having to rely on ineffective paper systems.\nThe Scale-Up teams represented innovations already being tested in WFP‚Äôs field operations, with the objective of scaling up their impact or reach. ShareTheMeal is an app which mobilizes microdonations to support WFP‚Äôs global operations; H2Grow is a hydroponic system that allows people to grow food with no soil and 90% less water; PLUS School Menus is software that calculates the most cost-effective menus that meet the nutritional requirements of children of different age groups; EMPACT is a digital skills training program that prepares refugee and war-affected youth for the future of work; and VAMPIRE/PRISM combines remote sensing data and population vulnerability information to produce risk and impact analytics.\nThe Early Stage innovators were hosted at the inspiring Google Developers Launchpad space in San Francisco. The bootcamp for advanced Scale-Up innovators was hosted by Orange Silicon Valley. Each team met with specialized tech sector professionals, mentors, and other innovators, analyzed their ideas, injected lean-startup business processes, and considered human-centered design.\nWith team, went through the pitch night at Googleplex to attract investors and show what we have made to #disrupthunger using VAMPIRE/PRISM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBesides following the bootcamp, I have great discussion in Malala Yousafzai meeting room at Planet office and check out the well-known Dove+RapideEye satellite.\nI also enjoy views of the city from Bernal Height Park, taste the delicious oysters at Ferry Building and the best taco+burrito in Mission Street, and visit the famous Golden Gate bridge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20130415-cloudless-satellite-image.html",
    "href": "blog/20130415-cloudless-satellite-image.html",
    "title": "Cloudless satellite image",
    "section": "",
    "text": "I found excellent Image-compositing scripts for filtering weather out of satellite images https://github.com/celoyd/wheather and decided to give a try.\nThis analysis based on MODIS subsets image from NASA - https://lance-modis.eosdis.nasa.gov. See below:\nLocated directly on the Equator and covered with mountains and rainforest, Indonesia is one of the cloudiest places on Earth. Getting cloud free image from MODIS is difficult\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1024‚Äù] Sample quick look, 23 July 2012 [/caption]\nFollowing the script, I downloaded the MODIS quick look. And you can see every day sometimes cloud-free and mostly not.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1024‚Äù] 1 - 31 July 2012, 1km pix res [/caption]\nIf we could create an average image from all the collection in the previous picture, it will look as below picture.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1024‚Äù] Simple average [/caption]\nThen we can sort per pixel of all the image collection, and leave the least cloudy-looking layers and average the rest. Details are below:\n\nSplit them into strips\nSort the strips, pixelwise, to remove cloud cover\nAverage the cloudless strips\nRejoin the strips into a cloudless image.\n\n\nFinally choose the top of quality and we got below picture.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20070210-automatic-coordinate-register-system.html",
    "href": "blog/20070210-automatic-coordinate-register-system.html",
    "title": "Automatic Coordinate Register System",
    "section": "",
    "text": "Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, from different times, or from different viewpoints [1]. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements. [1] Wikipedia, http://en.wikipedia.org/wiki/Image_registration\nThis program is designed to Small Format Aerial Photo Application. This process is to manipulate the ER Mapper Files (*.ers) using LISTRIG file (*.txt) in batch. After this process, you will have ER Mapper files which have an additional GCP inside generated from LISTRIG file, then you can rectify those files in batch using the automatic rectification wizard from ER Mapper.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240525-xkcd-style-for-country-map.html",
    "href": "blog/20240525-xkcd-style-for-country-map.html",
    "title": "xkcd style for Country map",
    "section": "",
    "text": "Source: https://gist.github.com/bennyistanto/7b391b11e861334bc020dd03c06815f2\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180206-vampire-won-2017-innovation-challenge.html",
    "href": "blog/20180206-vampire-won-2017-innovation-challenge.html",
    "title": "VAMPIRE won 2017 Innovation Challenge",
    "section": "",
    "text": "Today we celebrate the VAMPIRE project, won the 2017 WFPs Innovation Challenge. See the news in WFP Innovation blog here: https://medium.com/@WFPInnovation/wfp-staff-show-entrepreneurial-side-in-annual-competition-be03924215\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230315-pycpt-for-subseasonal-forecasts-in-indonesia.html",
    "href": "blog/20230315-pycpt-for-subseasonal-forecasts-in-indonesia.html",
    "title": "PyCPT for Subseasonal Forecasts in Indonesia",
    "section": "",
    "text": "1 Introduction\nPyCPTis a Python library for statistical analysis and forecasting of climate data. It provides a range of tools for time series analysis, including trend analysis, seasonal decomposition, and correlation analysis. These tools can be used to identify patterns and relationships in climate data, which can then be used to develop forecasts for subseasonal climate conditions.\nSubseasonal forecasting is an important area of research in climate science, particularly in regions such as Indonesia where the climate is strongly influenced by the Madden-Julian Oscillation (MJO) and other large-scale climate drivers that operate on a timescale of several weeks to a few months. By developing subseasonal forecasts, it is possible to anticipate changes in climate conditions and take measures to prepare for and adapt to these changes.\n\nFigure 1. PyCPT S2S. Source: https://bitbucket.org/py-iri/iri-pycpt/src/master/\nTo use PyCPT for subseasonal forecasting in Indonesia, the first step is to acquire and preprocess the relevant climate data. This may include data on temperature, precipitation, and other climate variables, as well as data on MJO and other large-scale climate drivers. The data should be cleaned, validated, and formatted into a suitable time series format for analysis.\nOnce the data has been prepared, it can be analyzed using PyCPT‚Äôs time series analysis tools. This may involve identifying trends, subseasonal patterns, and correlations between different climate variables. These insights can then be used to develop statistical models for forecasting subseasonal climate conditions.\nPyCPT includes a range of statistical models for subseasonal forecasting, including autoregressive integrated moving average (ARIMA) models, vector autoregression (VAR) models, and dynamic linear models (DLMs). These models can be used to make point forecasts of subseasonal climate conditions, as well as to estimate the uncertainty associated with these forecasts.\nIn addition to point forecasts, PyCPT can be used to generate probabilistic forecasts of subseasonal climate conditions. This can be particularly useful in regions such as Indonesia, where the climate is highly variable and uncertain. Probabilistic forecasts can provide information on the likelihood of different climate scenarios, which can help decision-makers to plan for and adapt to future climate conditions.\nPyCPT also includes tools for evaluating the accuracy and performance of different forecasting models. These tools can be used to compare different models and identify the most effective approach for forecasting subseasonal climate conditions in Indonesia.\nIn addition to its forecasting capabilities, PyCPT can also be used for climate impact assessments and scenario planning at the subseasonal timescale. By simulating different subseasonal climate scenarios, it is possible to assess the potential impacts of future climate conditions on agriculture, water resources, and other sectors. This information can then be used to develop adaptation strategies and policies.\nPyCPT is a powerful tool for subseasonal forecasting in Indonesia and other regions with complex and variable climates. Its range of statistical tools and models, combined with its ease of use and integration with Python, make it a valuable resource for climate scientists, researchers, and decision-makers.\nBy using PyCPT to develop accurate and reliable subseasonal forecasts, it is possible to better anticipate and adapt to the impacts of climate change in Indonesia. This can help to reduce the risks and costs associated with extreme weather events, protect vulnerable populations, and promote sustainable development.\n2 PyCPT set-up\nI have written notes on how to set up the PyCPT from preparing the installation (Anaconda and CPT), supporting files, and modified notebook for running a sub seasonal and seasonal forecast in Indonesia. It‚Äôs accessible via this link.\n3 Configuring PyCPT\nTo develop a subseasonal Multi-Model Ensemble (MME), we will utilize the CFSv2, GEFSv12, and FIM SubX models. Similar to the seasonal forecasting case, in subseasonal forecasting, the application of Model Output Statistics (MOS) using PyCPT necessitates a series of decisions. These include selecting the appropriate SubX model and observed dataset, choosing the MOS methodology, selecting predictor and predictand variables, specifying the spatial domains of interest, choosing the forecast date and training season, and selecting the forecast lead intervals. It is recommended that you first designate a case name for your experiment, establish a working directory, and indicate the root directory for the CPT tool.\nRegarding the SubX model and observed dataset, precipitation data from several sources, including ECMWF, NCEP-CFSv2, EMC-GEFSv12, ECCC-GEPS6, and ESRL-FIM, can be utilized as predictors. The spatial resolution of the SubX data is 1 degree, while S2S data is 1.5 degrees, but note that the native model resolutions may be more precise.\n########Model (choose between ECMWF, CFSv2, GEFS, CFSv2_SubX, GEPS6)\nmodel= [‚ÄòCFSv2_SubX‚Äô, ‚ÄòESRL‚Äô]\nThe predictand variable of choice is precipitation from CHIRPS, although other options, such as TRMM, CPC, IMD1deg, and IMDp25deg, can be selected. It is worth noting that temperature will soon be added as a predictand, but rainfall frequency is not yet available in the subseasonal version.\n########Obs (choose between CHIRPS, TRMM, CPC, IMD1deg, IMDp25deg)\nobs=‚ÄòCHIRPS‚Äô\nChoice of MOS method is another critical consideration. CCA, PCR, and no-MOS can all be employed with the subseasonal models, and ELR, which is applied gridpoint-by-gridpoint, is under development. CCA is presently recommended as the default option.\n########MOS method (choose between None, PCR, CCA and ELR)\nMOS=‚ÄòCCA‚Äô\nFor the forecast date and training season length, subseasonal forecasts are initialized every week, and the training sample of hindcasts can be increased by selecting multiple start times each year, as long as these fall within a climatologically homogeneous season, such as a monsoon season.\n########Forecast date¬†\n##‚Äì If ECMWF, it needs to be a Monday or a Thursday! CFSv2: any day; GEFS: Wednesdays.\nmon=‚ÄòMar‚Äô # Forecast month\nfyr=2023 # Forecast year\nfday=1 # Forecast day¬† (Monday and Thursday in ECMWF model; yesterday in CFSv2: real time)\n#training_season=mon\ntraining_season=‚ÄòFeb-Apr‚Äô¬† # with *mon* in the middle, e.g., ‚ÄòFeb-Apr‚Äô if mon=‚ÄòMar‚Äô\nPredictor and predictand spatial domains should be chosen carefully.\n\nSpatial domain for predictor\nnla1=16 # Northernmost latitude\nsla1=-18 # Southernmost latitude\nwlo1=85 # Westernmost longitude\nelo1=150 # Easternmost longitude\n# Spatial domain for predictand\nnla2=11 # Northernmost latitude\nsla2=-13 # Southernmost latitude\nwlo2=90 # Westernmost longitude\nelo2=145 # Easternmost longitude\n\nFigure 2. Predictor and Predictand domain.\nWhile forecast lead time intervals, which determine how far into the future is predicted and over what time frame the forecast is averaged, should be set according to the desired outcome. The exact values of day1 and day2 depend on the selected model and variable, and careful attention should be paid to the instructions in the Jupyter notebook.\n########Forecast lead interval¬† ‚Äì This counts using L, the lead time; e.g., ECMWF L=0..46 (NOT calendar days)\nnwk=4 # Number of weeks to process (leads)\n# Lists for looping over lead times ‚Äì ALL these arrays should have the same dimension (see nwk above), as they correspond\nwk = ¬† [1 ¬† ¬† ¬† ,2 ¬† ¬† ¬† ,3 ¬† ¬† ¬† ,34 ¬† ¬† ¬† ,4 ¬† ¬† ¬† ]¬† # week-lead number label (e.g., corresponding to week1, week2, week3, week4, week1-4/month 1)\nwknam= [‚ÄòWeek 1‚Äô,‚ÄòWeek 2‚Äô,‚ÄòWeek 3‚Äô,‚ÄòWeek 3-4‚Äô,‚ÄòMonth 1‚Äô]¬† #naming the weeks (for plots) ‚Äìcan use a different language here\nday1 = [1 ¬† ¬† ¬† ,7 ¬† ¬† ¬† ,14¬† ¬† ¬† ,14¬† ¬† ¬† ,21¬† ¬† ¬† ¬† ]¬† # first lead day of target weeks\nday2 = [7 ¬† ¬† ¬† ,14¬† ¬† ¬† ,21¬† ¬† ¬† ,28¬† ¬† ¬† ,28 ¬† ¬† ¬† ]¬† # last lead day of target weeks\n# ECMWF - first day is day 0, 0000Z accumulated rainfall; specify day1=1 for week 1\n# GEFS - first day is day 0.5 (daily average rainfall rate); specify day1=0 for week 1\n# CFSv2 - first day is day 1, 0000Z accumulated rainfall over the first day; specify day1=1 for week 1\nTraining and verification periods should be determined based on the limited hindcast period, which must be used to both train and assess skill, each of which requires large sets of independent non-overlapping data. PyCPT-Subseasonal uses Retroactive Forecasting to train and verify the MOS model, which requires choosing the length of initial training period and update interval.\nlit=28 # Initial training period for retroactive forecasts (in timesteps)\nliti=10 # Update interval for retroactive forecasts (in timesteps)\nA simple rule of thumb is to set the length of the initial training period equal to 50% of the hindcast dataset and the update interval to once a year. The values of the initial training period and update interval should be adjusted if necessary, but note that overfitting the MOS model may occur if skill results are excessively ‚Äútuned‚Äù to maximize them without some independent data available to re-test the model.\n########Model-dependent parameters\nif model==‚ÄòCFSv2‚Äô:\n¬† ¬† hstep = 7 # use all starts in the trainng period with this daily step between them\n¬† ¬† nlag¬† = 3¬† # length of the lagged ensemble in days\n¬† ¬† ntrain= 55 # Length of training period (5 weeks x 11 hindcast years) (363 before)\n¬† ¬† lit =¬† 28\n¬† ¬† liti = 10\nelif model==‚ÄòECMWF‚Äô or model==‚ÄòECMWFrt‚Äô:\n¬† ¬† hstep = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† nlag¬† = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† ntrain= 160¬† # Length of training period\n¬† ¬† lit =¬† 110\n¬† ¬† liti = 20\nelif model==‚ÄòGEFS‚Äô:\n¬† ¬† hstep = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† nlag¬† = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† ntrain= 219¬† # Length of training period (1999-2016)\n¬† ¬† lit =¬† 100\n¬† ¬† liti = 20\nelif model==‚ÄòGEPS6‚Äô:\n¬† ¬† hstep = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† nlag¬† = 0 # bogus but needed by functions (eliminate in the future)\n¬† ¬† ntrain= 260¬† # Length of training period (1998-2017)\n¬† ¬† GEPShdate1 = ‚Äò0000 4 Jun‚Äô # first hindcast date each year in training season\n¬† ¬† # Must be a THURSDAY in the forecast year since GEPS is an on-the-fly model\n¬† ¬† GEPShdate1 = parse.quote(GEPShdate1)\n¬† ¬† lit =¬† 120\n¬† ¬† liti = 20\nelif model==‚ÄòCFSv2_SubX‚Äô:\n¬† ¬† hstep = 7 # use all starts in the trainng period with this daily step between them\n¬† ¬† nlag¬† = 3¬† # length of the lagged ensemble in days\n¬† ¬† ntrain= 234 # Length of training period for 3 month season (1638/7=234)\n¬† ¬† lit =¬† 100\n¬† ¬† liti = 20\nFinally, it is common to divide data into three parts: the training, validation, and test sets, to check the model‚Äôs performance on data not seen during training.\n4 PyCPT outputs\nThe CPT output provides several diagnostic plots and statistical metrics that can be used to assess the quality of the subseasonal forecast. These diagnostics include:\n4.1 EOF analysis\nEOF analysis (Empirical Orthogonal Function analysis) is a method used to identify dominant patterns of variability in a dataset. PyCPT‚Äôs EOF analysis map produces maps of X EOF Spatial Loadings, Y EOF Spatial Loadings, and charts of X and Y Principal Components.\n\nFigure 3. EOF analysis result for week 1.\nThe X EOF Spatial Loadings map shows the spatial pattern of variability for the Xth EOF mode. This map is useful for understanding the dominant spatial patterns of the variability in the data. The Y EOF Spatial Loadings map shows the spatial pattern of variability for the Yth EOF mode, and is useful for identifying the dominant spatial patterns of the second variable in the dataset.\nThe charts of X and Y Principal Components show the time evolution of the principal components for the Xth and Yth EOF modes, respectively. The X Principal Component chart shows how the Xth mode of variability changes over time, while the Y Principal Component chart shows how the Yth mode of variability changes over time. These charts can be used to identify the times when the dominant patterns of variability are strongest, and how they change over time.\nOverall, the EOF analysis map produced by PyCPT is a powerful tool for identifying dominant patterns of variability in subseasonal forecast datasets. By visualizing the spatial patterns and time evolution of these patterns, forecasters can gain a better understanding of the underlying dynamics of the forecast data, which can help inform better forecasting decisions.\n4.2 CCA analysis\nThe Canonical Correlation Analysis (CCA) is a statistical technique used in PyCPT to explore relationships between two multivariate datasets X and Y. The CCA analysis in PyCPT is applied to subseasonal forecast data to identify patterns of covariability between climate predictors and predictands.\n\nFigure 4. EOF analysis result for week 1.\nThe output of the CCA analysis map in PyCPT includes three types of plots:\nMaps of X CCA Spatial Loadings: The CCA spatial loadings are maps that show the correlation between the predictor variables and the predictand variable at each location. The values in the map represent the strength and direction of the correlation.\nMaps of Y CCA Spatial Loadings: The Y CCA spatial loadings are maps that show the correlation between the predictand variable and the predictor variables at each location. The values in the map represent the strength and direction of the correlation.\nCharts on X and Y Temporal Scores: The temporal scores are time series that represent the variability of the predictor variables and the predictand variable over time. The charts show how the predictor variables and the predictand variable vary together over time.\nThe CCA analysis map is useful for identifying regions where the predictor variables and the predictand variable are highly correlated, which can help in developing subseasonal forecast models. By analyzing the spatial loadings and temporal scores, users can identify the patterns of variability in the predictor variables and the predictand variable that are driving the correlations.\n4.3 Skill maps\nThe Skills map is a set of skill scores that measures the performance of the forecast model against the observed data. The following skill scores are included in the Skills map:\n4.3.1 Spearman\nSpearman‚Äôs rank correlation coefficient is a measure of the strength and direction of the relationship between two variables. It is particularly useful for non-linear relationships. Spearman‚Äôs rank correlation coefficient ranges from -1 to 1, where a coefficient of 1 indicates a perfect positive correlation, 0 indicates no correlation, and -1 indicates a perfect negative correlation.\n\nFigure 5. Spearman\nStrengths:\n\nSpearman‚Äôs rank correlation coefficient is useful for evaluating the relationship between two variables when the relationship is non-linear.\nIt is a robust measure that is not sensitive to outliers.\nIt can be used to evaluate the skill of both deterministic and probabilistic forecasts.\n\nWeaknesses:\n\nSpearman‚Äôs rank correlation coefficient is less sensitive to small differences than other correlation measures.\nIt is not suitable for evaluating the skill of forecasts with categorical or ordinal data.\nSpearman‚Äôs rank correlation coefficient does not provide information about the magnitude of the correlation.\n\nPotential applications:\n\nSpearman‚Äôs rank correlation coefficient can be used to evaluate the skill of forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nIt can be used to evaluate the relationship between different variables in a system, which can be useful for identifying potential drivers of a particular phenomenon.\nIt can be used to identify situations in which a forecast adds value over a reference forecast, which can be useful for decision-making and risk management.\n\n4.3.2 2AFC (Two-Alternative Forced Choice)\n2AFC is a binary classification skill score that compares the forecast probabilities to a threshold. It is defined as the percentage of times that the forecast probability is higher (or lower) than the threshold when the observed event occurs (or does not occur). A perfect forecast has a 2AFC of 100%, while a forecast with no skill has a 2AFC of 50%.\n\nFigure 6. 2AFC\nStrengths:\n\n2AFC is a simple and intuitive skill score that can be used to evaluate binary forecasts.\n2AFC is a useful skill score for evaluating forecasts in situations where only two possible outcomes are of interest (such as a yes/no decision).\n2AFC is robust to changes in the threshold, so it can be used to evaluate forecasts with different probability thresholds.\n\nWeaknesses:\n\n2AFC does not take into account the magnitude of the forecast probabilities, only their direction relative to the threshold.\n2AFC assumes that the cost of false positives and false negatives is equal, which may not always be the case in practice.\n2AFC may not be as informative as some other skill scores for evaluating probabilistic forecasts.\n\nPotential applications:\n\n2AFC can be used to evaluate the skill of binary forecasts in a variety of contexts, such as weather forecasting, sports betting, and medical diagnosis.\n2AFC can be used to compare the skill of different binary forecasts or to evaluate the value of adding new information to an existing binary forecast.\n2AFC can be used to identify situations in which a binary forecast adds value over a simple decision rule (such as always betting on the favorite in a sports game).\n\n4.3.3 RocAbove and RocBelow (Receiver Operating Characteristic)\nRocAbove and RocBelow are binary classification skill scores that compare the forecast probabilities to a threshold. They are defined as the area under the ROC curve above (RocAbove) or below (RocBelow) the threshold. A perfect forecast has a RocAbove (or RocBelow) of 1, while a forecast with no skill has a RocAbove (or RocBelow) of 0.5.\n \nFigure 7. RocAbove and RocBelow.\nStrengths:\n\nRocAbove and RocBelow are skill scores that can be used to evaluate binary forecasts in a variety of contexts.\nRocAbove and RocBelow are more informative than some other binary classification skill scores (such as 2AFC) because they take into account both the magnitude and direction of the forecast probabilities.\nRocAbove and RocBelow can be used to evaluate the skill of binary forecasts with different probability thresholds.\n\nWeaknesses:\n\nRocAbove and RocBelow assume that the cost of false positives and false negatives is equal, which may not always be the case in practice.\nRocAbove and RocBelow may not be as intuitive as some other skill scores for some users.\nRocAbove and RocBelow may not be as informative for evaluating probabilistic forecasts as some other skill scores, such as RPSS or Ignorance.\n\nPotential applications:\n\nRocAbove and RocBelow can be used to evaluate the skill of binary forecasts in a variety of contexts, such as weather forecasting, medical diagnosis, and credit scoring.\nRocAbove and RocBelow can be used to compare the skill of different binary forecasts or to evaluate the value of adding new information to an existing binary forecast.\nRocAbove and RocBelow can be used to identify situations in which a binary forecast adds value over a simple decision rule.\n\n4.3.4 RPSS (Ranked Probability Skill Score):\nRPSS is a skill score that compares the forecast probabilities with the observed frequencies. It is defined as the difference between the skill score of the forecast and that of a reference forecast, divided by the maximum possible improvement in the skill score over the reference forecast. A perfect forecast has an RPSS of 1, while a forecast with no skill has an RPSS of 0.\n\nFigure 8. RPSS.\nStrengths:\n\nRPSS is a skill score that can be used to evaluate probabilistic forecasts.\nIt is a more informative measure of skill than some other skill scores (such as the Brier Score) because it takes into account the skill of the reference forecast.\nRPSS can be useful for identifying situations in which a probabilistic forecast adds value over a reference forecast (such as climatology or persistence).\n\nWeaknesses:\n\nRPSS requires the use of a reference forecast, which may not always be available or appropriate.\nRPSS can be sensitive to the choice of reference forecast, so it is important to choose a reference forecast that is appropriate for the specific application.\nRPSS may not be as intuitive as some other skill scores, such as the Brier Score or the ROC curve, for some users.\n\nPotential applications:\n\nRPSS can be used to evaluate the skill of probabilistic forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nRPSS can be used to compare the skill of different probabilistic forecasts or to evaluate the value of adding new information to an existing probabilistic forecast.\nRPSS can be used to identify situations in which a probabilistic forecast adds value over a reference forecast, which can be useful for decision-making and risk management.\n\n4.3.5 Ignorance\nIgnorance is a skill score that measures the degree to which the forecast adds information beyond the climatological probability. It is defined as the difference between the information gained from the forecast and the information gained from the climatology, divided by the maximum possible information gain. A perfect forecast has an Ignorance score of 1, while a forecast with no skill has an Ignorance score of 0.\n\nFigure 9. Ignorance.\nStrengths:\n\nIgnorance provides a measure of the degree to which the forecast adds value beyond the climatological probability, which can be useful for decision-making and risk management.\nIgnorance can be used to evaluate the skill of probabilistic forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nIgnorance can be useful for identifying situations in which a probabilistic forecast adds value over the climatological probability.\n\nWeaknesses:\n\nIgnorance requires the use of a climatological probability, which may not always be appropriate or accurate.\nIgnorance may not be as intuitive as some other skill scores, such as the Brier Score or the ROC curve, for some users.\n\nPotential applications:\n\nIgnorance can be used to evaluate the skill of probabilistic forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nIgnorance can be used to compare the skill of different probabilistic forecasts or to evaluate the value of adding new information to an existing probabilistic forecast.\nIgnorance can be used to identify situations in which a probabilistic forecast adds value over the climatological probability, which can be useful for decision-making and risk management.\n\n4.3.6 GROC (Gross Relative Operating Characteristic)\nGROC is a skill score that measures the degree to which a forecast is better than random. It is defined as the area under the ROC curve, divided by the maximum possible area under the ROC curve. A perfect forecast has a GROC of 1, while a forecast with no skill has a GROC of 0.5.\n\nFigure 10. GROC.\nStrengths:\n\nGROC is a skill score that can be used to evaluate the skill of probabilistic forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nGROC can provide a measure of the degree to which a forecast is better than random, which can be useful for decision-making and risk management.\nGROC is a more informative measure of skill than some other skill scores (such as the Brier Score) because it takes into account the entire range of possible forecast probabilities.\n\nWeaknesses:\n\nGROC may not be as intuitive as some other skill scores, such as the Brier Score or the ROC curve, for some users.\n\nPotential applications:\n\nGROC can be used to evaluate the skill of probabilistic forecasts in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nGROC can be used to compare the skill of different probabilistic forecasts or to evaluate the value of adding new information to an existing probabilistic forecast.\nGROC can be used to provide a measure of the degree to which a forecast is better than random, which can be useful for decision-making and risk management.\n\n4.4 Forecast maps\nThe Forecasts Map is an output of PyCPT that provides maps of probabilistic and deterministic forecasts for a given forecast variable. The probabilistic forecast shows the probability of the forecast variable exceeding a certain threshold, while the deterministic forecast shows the expected value of the forecast variable.\n \nFigure 11. Deterministic and Probabilistic forecast map.\nStrengths:\n\nThe probabilistic forecast provides a range of possible outcomes with associated probabilities, which can be useful for decision-making and risk management.\nThe deterministic forecast provides a single value that can be used as a point estimate for the forecast variable, which may be useful in some applications.\nThe forecasts map can provide a visual representation of the forecasted values, making it easier to identify areas of potential impact or concern.\n\nWeaknesses:\n\nThe accuracy of the forecasts may be limited by the quality and availability of the input data.\nThe probabilistic forecast may be difficult for some users to interpret, particularly if they are not familiar with probability concepts.\nThe deterministic forecast may not accurately capture the full range of possible outcomes.\n\nPotential applications:\n\nThe forecasts map can be used in a variety of contexts, such as weather forecasting, hydrological forecasting, and financial forecasting.\nThe probabilistic forecast can be particularly useful in situations where there is a high degree of uncertainty, such as in long-range weather forecasting or in predicting the likelihood of extreme events.\nThe deterministic forecast may be useful in some applications where a single point estimate is sufficient, such as in short-term weather forecasting or in predicting the expected value of a financial variable.\n\n4.5 Flexible Forecast maps\nThe Flexible Forecasts map produced by PyCPT shows the probability (%) of exceeding the 50th percentile for a given forecast period. This map is particularly useful for decision-making as it provides information on the likelihood of exceeding the median value for a given variable.\n\nFigure 12. Flexible forecast map.\nStrengths:\n\nThe Flexible Forecasts map provides a quick and easy way to assess the probability of exceeding the median value for a given variable.\nThis map can be particularly useful for decision-making in a variety of applications, including agriculture, water management, and energy planning.\nThe 50th percentile is a commonly used reference point in decision-making, making this map easy to interpret.\n\nWeaknesses:\n\nThe Flexible Forecasts map provides a limited view of the forecast distribution, as it only shows the probability of exceeding the median value.\nThis map does not provide information on the tails of the distribution, which may be important for certain applications.\nThe accuracy of the forecast probabilities depends on the skill of the underlying model, which can vary depending on the variable being forecast and the forecast lead time.\n\nPotential applications:\n\nThe Flexible Forecasts map can be used in a variety of decision-making contexts, including agricultural planning (e.g., planting decisions), water resource management (e.g., reservoir operations), and energy planning (e.g., electricity generation).\nThis map can also be useful in risk management contexts, as it provides information on the likelihood of extreme events (i.e., events that exceed the median value).\nThe Flexible Forecasts map can be used in combination with other forecast products (e.g., deterministic or probabilistic forecasts) to provide a more complete picture of the forecast distribution.\n\n4.6 Flexible Forecast at a specific location\nFlexible Forecasts for a particular location is a tool that provides probabilistic forecast information for a specific location. It produces a chart of probability of exceedance and probability density function, showing the likelihood of different outcomes for a particular forecast variable (such as temperature or precipitation) at a specific location.\n\nFigure 12. Flexible forecast at Kebun raya Bogor, with coordinates: 106.79739, -6.59800\nStrengths:\n\nProvides probabilistic forecast information for a specific location, which can be useful for decision-making and risk management at the local level.\nShows both the probability of exceedance and the probability density function, providing a comprehensive view of the forecast distribution.\nCan be used to compare different forecast models or scenarios for a specific location.\n\nWeaknesses:\n\nMay not be as useful for large-scale decision-making or risk management, as it only provides information for a single location.\nMay require expertise in interpreting probability distributions and making decisions based on uncertain information.\n\nPotential applications:\n\nCan be useful for local decision-making and risk management, such as in agriculture or energy management.\nCan be used to compare different forecast models or scenarios for a specific location, helping to identify the most likely outcomes and potential risks.\nCan be used in conjunction with other forecast tools and information to inform decision-making and risk management at the local level.\n\nOverall, the CPT output provides a comprehensive assessment of the quality of the subseasonal forecast, allowing for evaluation of the model skill and identification of areas for improvement.\n5 Strength and Weakness of PyCPT for Subseasonal Forecasts\nCPT has been developed to forecast weather conditions over a period of weeks to months. These tools rely on various data sources, including historical observations, computer models, and physical relationships between the atmosphere and oceans. While CPTs have shown great promise for Subseasonal Forecasting, their accuracy remains limited, and they have both strengths and weaknesses that must be considered.\nOne of the strengths of CPTs for Subseasonal Forecasting in Indonesia is their ability to capture large-scale climate patterns that can influence weather conditions over weeks to months. For example, the El Ni√±o-Southern Oscillation (ENSO) phenomenon, which causes warming or cooling of the ocean in the equatorial Pacific, can have significant impacts on rainfall patterns in Indonesia. CPTs can accurately predict the strength and duration of an ENSO event, providing valuable information to decision-makers in industries such as agriculture, water management, and disaster preparedness.\nHowever, CPTs also have weaknesses that can limit their accuracy in Subseasonal Forecasting for Indonesia. One of the main limitations is the quality and availability of data. For example, in Indonesia, there are gaps in historical weather data, which can make it difficult to calibrate CPTs accurately. Additionally, the quality of data from remote sensing technologies such as satellite observations can be influenced by cloud cover, atmospheric interference, and other factors, leading to errors in the model output.\nAnother limitation of CPTs for Subseasonal Forecasting in Indonesia is the complex terrain and heterogeneous climate patterns found in the country. Indonesia‚Äôs islands and mountain ranges create a unique climate system that can be difficult to capture accurately in CPTs. The monsoon climate in Indonesia is also highly variable, with different regions experiencing different levels of rainfall and temperature fluctuations. This variability makes it challenging to develop accurate models that can capture the subseasonal weather patterns across the country.\nIn conclusion, while CPTs hold great promise for Subseasonal Forecasting in Indonesia, they also have limitations that must be considered. To improve their accuracy, researchers must continue to invest in high-quality data sources and develop models that can capture the complex climate patterns of Indonesia. Improved accuracy in Subseasonal Forecasting can help decision-makers prepare for extreme weather events, manage natural resources more effectively, and support sustainable development across the country.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20170802-blending-satellite-precipitation-and-gauge-observations.html",
    "href": "blog/20170802-blending-satellite-precipitation-and-gauge-observations.html",
    "title": "Blending Satellite Precipitation and Gauge Observations",
    "section": "",
    "text": "Blending of point and grid data\nI have some station data! Can I blend it with the remote sensing data? How?\n\nThe answer is Yes, and you must have: enough data, reasonable relationship between ground data and remote sensing parameter\n\nOn different situations:\n\nLong term point and long-term gridded data: station rainfall data and satellite precipitation estimates (SPE)\nLong term point and average gridded data: station temperature data and WorldClim average temperature grid\nAverage point data and average gridded data\n\nPossible pairs:\n\nRainfall ‚Äì SPE\nTemperature ‚Äì Land Surface Temperature\n\n\n\nMethod\nRegression ‚Äì Kriging\n\nDefine regression between point data and pixel values of a number of related/useful GRIDDED variables, e.g.\nTair = f(altitude, LST, NDVI, coastdistance, other)\nRain = f(SPE, other)\n\nProduce modelled parameter and residuals, e.g.\n\n\\(\\text{Rain}_{\\text{Estimates}} = 1.43 \\times \\text{SPE}\\)\n\\(\\text{residual} = \\text{Rain}_{\\text{Estimates}} - \\text{Rain}\\)\n\nInterpolate residuals then Add back to the initial estimate\n\n\nExample\nExtract SPE values at 152 station location: X, Y, Obs, SPE\n\nRegression model Rainfall - SPE\n\nModelled rainfall grid and residuals\n\nAdd back to the initial estimate\n\n\n\nExercise\nI am sure that you are familiar with and frequently use CHIRPS data in your climate-related analyses. But have you ever used CHIRP data?\nCHIRP and CHIRPS are gridded rainfall time series product from Climate Hazard Group of University California Santa Barbara, and the difference between the two data is CHIRPS incorporates station data. Let see comparison on below maps.\n\nPrepare observation data\nLet‚Äôs try the exercise on blending data using CHIRP and station rainfall from BMKG.\nIn this exercise, BMKG provide the daily rain gauge data in spreadsheet format and have two worksheet: Station and Data Original.\nFolder 01_Download/BMKG\nOpen file BMKG_152station_dailyobs_2017_Dekad.xlsx\n\nWe will calculate dekad value from daily data. This can be done in many ways, we can use PIVOT or just using other simple formula.\n\nAssign Dekad value on every Date\nMerge year, month and day column into Date: YYYYMMDD\nMerge year, month and dekad date into Date: YYYYMMDekad\nCalculate the accumulation rainfall on each dekad. Try SUM IF with aray formula.\n{SUM(IF(WORKSHEET!DATARANGE= DEKADDATE, WORKSHEET!DATARANGE))}\nIf using an array formula, make sure to confirmed with ctrl+shift+enter (not just enter) so that curly brackets appear around it {}. Those brackets can not be entered manually.\nTRANSPOSE your data, and the format will be Station ID, Dekad Date, ..\nYou can add coordinate column using VLOOKUP, right after the Station ID. And your data are ready to add into GIS software.\nBefore adding to GIS software, copy final workbook into new one using PASTE VALUE. This is to make sure the final workbook doesn‚Äôt contain any formula. For the rainfall value, leave it only 2017 Dekad 1 data.\n\nLoad data as XY table\nThis step will use ArcGIS Desktop, but it is also possible to use other GIS software i.e.¬†QGIS.\n\nArcToolbox &gt; Conversion Tools &gt; Excel &gt; Excel to Tables\n\n\n\nFrom table of content, right click &gt; Display XY Data.\nX Field for Longitude and Y Field for Latitude, and make sure the coordinate system of input is set to GCS_WGS_1984.\n\n\n\nSave the table events to shapefile. From Table of Content, right click the table events &gt; Data &gt; Export Data. Save as idn_cli_rpg_201701d1_bmkg_p.shp\n\n\nExtract SPE value at station location\nLoad CHIRP data for Dekad 1, Jan 2017 and station data from the previous step.\n\nExtract RFE value at station location using ArcToolbox\nSpatial Analyst Tools &gt; Extraction &gt; Extract Values to Points\nPut observation data as Input point feature and CHIRP data as Input raster\nSave as CHIRP_Obs_point_extraction.shp\nAfter completed, open the attribute of CHIRP_Obs_point_extraction.shp by right click on the table of contents and Open Attribute Table.\nNew column RASTERVALU added next to Observation data ‚Äú201701D1‚Äù\nFrom ArcToolbox, go to Conversion Tools &gt; Excel &gt; Table to Excel. Convert CHIRP_Obs_point_extraction.shp table to spreadsheet.\n\nOpen the spreadsheet from previous step.\n\nCreate scatter chart with observation data as Vertical axis and CHIRP data as Horizontal axis.\nAdd Linear trendline and display equation on chart.\n\n\nRegression model rainfall - SPE\nUsing the same spreadsheet,\n\nAdd new column ‚ÄúResidual‚Äù next to CHIRP column.\nUsing the equation on the chart, calculate the residuals value. Residu = Observation-(1.0872*CHIRP-4.3614)\nAdd a new column again, called ‚ÄúObsMinRFE‚Äù. Calculate the difference between Observation and CHIRP data.\nCompare the result.\nYou can also create Residual scatter chart, to see distribution from the data.\n\n\nKriging of residuals\nCopy the data to new workbook, and save as new spreadsheet.\n\n\nConvert the new spreadsheet to table using ArcToolbox &gt; Conversion Tools &gt; Excel &gt; Excel to Tables.\nFrom table of content, right click &gt; Display XY Data.\nX Field for Longitude and Y Field for Latitude, and make sure the coordinate system of input is set to GCS_WGS_1984.\nSave the table events to shapefile. From Table of Content, right click the table events &gt; Data &gt; Export Data. Save as Residual.shp\n\nLoad Indonesia boundary file to ArcMap.\n\nDo Kriging Interpolation using Residual.shp as Input features and Residual column as Z value.\nArcToolbox &gt; Spatial Analyst Tools &gt; Interpolation &gt; Kriging\nKriging method: Ordinary\nOutput cell size: 0.05\nKlik Environments button. Find Processing Extent &gt; Mask &gt; Choose the Indonesia boundary ‚Äì idn_bnd_adm1_2013_bps_a.shp. And Snap raster &gt; idn_cli_CHIRP.2017.01.1.tif\n\n\n\nAfter the interpolation result appears, you need to clip the result to remove data outside the boundary. Spatial Analyst Tools &gt; Extraction &gt; Extract by Mask.\n\n\n\nPut residual.tif as Input raster, and idn_bnd_adm1_2013_bps_a.shp as Input raster or feature mask data\n\nBlending Observation and CHIRP\nFinal step is adding the residual to the SPE data.\n\nThere are two ways to do the calculation, using Plus tool from Spatial Analysts Tools &gt; Math &gt; Plus. And using Raster Calculator from Spatial Analysts Tools &gt; Map Algebra &gt; Raster Calculator.\nBelow is using Raster Calculator.\n\n\nFinal Result\n\nMore station data?\nPrevious blending analysis used 152 station data. What if you have more than 1000 station data. Can you imagine the result?\nDekad 1, Jan 2017. Data 1000 station https://cl.ly/lhvm\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20191108-global-gis-workshop-2019.html",
    "href": "blog/20191108-global-gis-workshop-2019.html",
    "title": "Global GIS Workshop 2019",
    "section": "",
    "text": "Right after following the innovation bootcamp in SF, I flew to Rome to attend WFPs Global GIS Workshop.\nOver 100 people from around the world came to WFP Headquarters in Rome for the Global GIS Workshop 2019. This event brings together GIS users, practitioners and analysts to exchange knowledge, ideas and best practices. This allows us to better understand the needs and gaps within GIS and build a stronger GIS community within WFP.\nOn day 1 we have a presentation about Geospatial Strategy from WFP Emergencies, Introduction and Overview of VAM Geospatial, Regional Bureau overview and Data Preparedness Exercise.¬†The objective of the exercise is to gain an understanding of the importance of data preparedness and role of GIS during the early phases of an emergency response following a fast onset disaster. And will be challenged by a disaster scenario, according to the likelihood of the event in the areas of concern of the RB.\nThe VAM HQ presented the various tools/products that been develop on for the past few years including seasonal monitor and country-level crop analysis. OSE presented on the services they provide to country offices, including flood extent.\nThe 3rd day was mostly with external partners including HDX, JRC, ECMWF, NASA, and Copernicus. They highlighted a lot of data and information that is available publicly ‚Äì inviting people in the audience to go out and get things on their own. Afternoon session was drone activities from ETC and they presented about use of drones in emergency response.\nDay 4, we also had a presentation by ESRI on some new spatial stats tools in ArcGIS Pro which were really impressive.¬†During the presentation, we provided an ArcGIS Pro Package containing scenarios driven by World Food Programme GIS analysts using a variety of spatial statistics tools. This coincides with content and material found in the User Conference workshops, Spatial Data Mining I: Essentials of Cluster Analysis, and Spatial Data Mining II: A Deep Dive into Space-Time Analysis. The utility of these exercises are analytical methods to explore and identify patterns relating to Taleban spring offensive conflicts that may prevent WFP programming from engaging in food distribution operations.¬†After lunch we have a Data Collection Exercise for Network Analysis using 3 mobile app data collection platforms: ESRI Quick Capture, ESRI Survey123 and KoboToolbox. All the participants are divided into 9 teams and have 9 routes with starting points in some train stations in Rome and the final meeting point is Colosseo. The objective of this exercise is to show how the network analysis could be relevant to face a possible emergency, planning properly a preparedness or a response scenario.\nLast day of the workshop was exciting, we discussed the result of the data collection exercise and what possible analysis we could do using the result. We have been able to estimate which IDPs are covered by the Food Shops we collected and which IDPs need to consider new Potential Shops using Network Analysis approach.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20110601-ilo-basic-operational-gis-for-road-assessment-part-3.html",
    "href": "blog/20110601-ilo-basic-operational-gis-for-road-assessment-part-3.html",
    "title": "ILO Basic Operational GIS for Road Assessment (Part 3)",
    "section": "",
    "text": "This the last part of 3 series post on ILO Basic Operational GIS for Road Assessment.\nYou can read first part here and second part here.\n9 ‚Äì 12 May 2011, Week 2: GIS\nBireuen:\nTraining was participated by 12 people as follows: 7 DBMCK staffs, 4 BAPPEDA staffs and 1 student from local university. The module and presentation were covering topics on Introduction to ArcMap on first day and Creating geographic map objects on second day. The purpose of the exercise on first day is to get familiar with the application and learn how to use the basic map tools in order to create a map using a data set over Aceh in ArcMap. On the second day, the purpose of the exercise is to learn how to create different geographic features represented as line, points and polygons.\nAt the end of this exercise the participant will be able to: Create a new project in ArcMap, Navigate around your map, View attribute information, Re-ordering layers, Different Views, Finding features, Measuring Distance, Working with coordinate system, Display attribute tables, Display and Edit attribute table, Select features, Symbolize data, Prepare a Layout, and Create new shapefile and map objects.\nPidie:\nTraining was participated by 17 people as follows: 9 DBMCK staffs, 6 BAPPEDA staffs and 2 DPKKD staffs. The training delivered same module and presentation as the training in Bireuen.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú714‚Äù] GIS training in Pidie [/caption]\n18 ‚Äì 19 May 2011, Week 3: GIS\nPidie: Training was participated by 12 people as follows: 5 DBMCK staffs, 5 BAPPEDA staffs and 2 DPKKD staffs. The module and presentation were covering topics on Working with Buffers and Joining Tables on first day and Assignment on Working with external data sources in the second day. The purpose of the exercise on first day is to get familiar with the ArcToolbox and the two different buffer tools used for geoprocessing. On the second day, the purpose of the exercise is to use different databases to combine data and visualize the result as unique values symbology.\nAt the end of this exercise the participant will be able to: Explore the data, Create geographic objects from x and y coordinates, Create a multiple buffer, Create a buffer from a specified distance, Joining tables, Select by attributes, Create another Symbology, and Display the result.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú714‚Äù] GIS training in Pidie [/caption]\n20 May 2011, Week 3: GPS\nPidie: Actually, training on the use of GPS in Pidie was conducted in the third week of April last month. But some staff in DBMCK requested to conduct GPS training again, then on the 20 May was held GPS training with same materials of the training. 7 peoples participated in this training.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú714‚Äù] GPS training in Pidie [/caption]\n23 ‚Äì 24 May 2011, Week 4: GIS\nBireuen: Training was participated by 5 people as follows: 1 DBMCK staffs, 3 BAPPEDA staffs and 1 student from local university. The training delivered same module and presentation as the training on 18 ‚Äì 19 May in Pidie.\n25 ‚Äì 26 May 2011, Week 4: GPS Field Training\nPidie: After getting the theory and practical session of the use of GPS, we then conducted the field survey. We split the the participants into two teams; all team will focus with survey area on roads that was rehabilitated by ILO in Delima, Pidie, Peukan Baro, Simpang Tiga, Glumpang Baro, Keumbang Tanjong, Sakti, Indra Jaya dan Padang Tiji sub-district.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1425‚Äù] GPS field training in Pidie [/caption]\nThe technical team has a good grip of the field condition, but they are less reliable in utilizing the maps for supporting the survey activities. There was a number of misdirection during the survey. Redirection (to obtain the right way) is guided by synchronizing the GPS with base maps and tabular data.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1336‚Äù] Roads that was rehabilitated by ILO (left to right: PD19, PD4 and PD21) [/caption]\nThe overall purpose of this activity is to develop the technical team capacity and comprehension to overcome the current and future problems. The survey is not only focused on road section position registration. Fruitful discussions on training materials and its application in the field were always been inserted during the survey. Considerable time and patience were spent to make a significant improvement.\nThe following picture shows the sub-district area in Delima, Pidie, Peukan Baro, Simpang Tiga, Glumpang Baro, Keumbang Tanjong, Sakti, Indra Jaya dan Padang Tiji, which had been surveyed by the all team. The red line shows the existing road network, the yellow point shows the point of road section identifier, and the green line shows the track which is traversed by a survey team that can later be converted into a road map.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú712‚Äù] Waypoint and Track from GPS field training in Pidie [/caption]\nOverall training was delivered in Bahasa Indonesia and discussed about GPS and GIS that can be useful specifically to provide support for the planning and budgeting of investments in road infrastructure. It is expected that by the end of the course, participants are able to describe basic functional and analytical capabilities of GPS and GIS, and perform data processing in order to visualize geographic data.\nAggregated Training Impact assessment results on GIS\nImpact Evaluation Methodology: Participants undertook a process of self-evaluation before and after the training, to essentially measure their level of self-confidence in carrying out specific tasks in which training was to be carried out. Participants were asked to indicate their level of skill/confidence for each task prior to and after the training. Choices offered were:\n\nI cannot perform the task at all\nI can perform the task but I still require assistance\nI can perform the task without any assistance\n\nEvaluation Results: This calculation is only carried to the participants that followed the training more than 80% of all the training that was conducted. Result indicated that 34.21% and 46.71% of the participants in Bireuen and Pidie can do task independently, while 49.12% and 53.29% of the participants in Bireuen and Pidie still need help in achieving the task due to their lack of confidence and the need to adapt towards the software. However, as a general comment, the participants have gained valuable computer knowledge and additional GIS and data management skills from the training. Request for a follow up training was raised by most of the participants at the closing ceremony.\nImpact assessment result in Bireuen\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1393‚Äù] Training impact assessment in Bireuen [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1837‚Äù] Training impact assessment in Bireuen [/caption]\nImpact assessment result in Pidie\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1393‚Äù] Training impact assessment in Pidie [/caption] \nAs identified during the training, local government offices already have a number of qualified human resources in the field of GPS and GIS. More training is required to increase the level of confidence among participants, who still need help in performing essential GPS and GIS basic tasks. It is expected that the next training will help to decrease the number of participants who need help and increase the number of participants who do not need help.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20231015-hourly-humidity-data.html",
    "href": "blog/20231015-hourly-humidity-data.html",
    "title": "Hourly Humidity Data",
    "section": "",
    "text": "Recently, I embarked on a journey to calculate humidity data from a myriad of sources. Throughout this process, I experimented with various methods, ranging from the saturation water vapour pressure using Teten‚Äôs formula (with parameters according to Buck) to saturation over ice from Alduchov and Eskridge, and finally to Clausius-Clapeyron.\nFor those in need of hourly humidity data spanning from 1 Jan 1950 to the present, there‚Äôs good news! You can seamlessly extract this information from ERA5-Land Hourly data via Google Earth Engine (GEE). The Specific and Relative Humidity is meticulously calculated based on three core parameters: T2m (Temperature at 2 meters), Dew Point, and Surface Pressure.\nInterested in exploring further? Check out my GEE script: https://code.earthengine.google.com/9b23f929939122fb1fdc8418d17c43f5\nBy the way, for those diving deep into the technicalities, the GEE script I‚Äôve shared leans on a simpler approach, kinda like a nod to the good ol‚Äô Magnus formula. So, it‚Äôs pretty straightforward and user-friendly\nI hope this proves beneficial to researchers, data scientists, and enthusiasts in the realm of climatology. If you have any suggestions, feedback, or improvements, please don‚Äôt hesitate to reach out.\nReference\nAlduchov, O. A., & Eskridge, R. E. (1996). Improved Magnus form approximation of saturation vapor pressure. Journal of Applied Meteorology, 35(4), 601-609.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031120-makalah-dasar-agronomi.html",
    "href": "blog/20031120-makalah-dasar-agronomi.html",
    "title": "Makalah Dasar Agronomi",
    "section": "",
    "text": "Makalah berikut merupakan tugas dari mata kuliah Dasar Agronomi di semester 3. Tugas ini saya kerjakan bersama 4 orang teman lainnya yang tergabung dalam kelompok 5 dengan topik bacaan 1.\n1.¬†Tita Krishna Murtie, G24101008 2.¬†Risyanto, G24101014 3.¬†Dini Oktavia Ambarwati, G24101025 4.¬†Utian Ayuba, G24101026 5.¬†Benny Istanto, G24101043\n\nJamur pathogen sebagai kontrol¬†‚Äòthrips‚Äô¬†pada bunga-bunga dalam rumahkaca\n\nWestern Flower Thrips (WFT) menyebabkan sejumlah tanaman pertanian hilang dengan memakan daun-daunan dan buah, berkembang biak dalam buah dan menyebarkan penyakit. Penggunaan pestisida berulang kali adalah satu-satunya cara untuk menurunkan populasi sampai pada tingkat yang wajar. Usaha pengendalian secara biologi telah dilakukan dengan menggunakan predator dan sebagian besar tidak berhasil. bagaimanapun, jamur entomopathognic dapat juga digunakan sebagai kontrol biologi untuk WFT. Laboratorium dan lahan percobaan menunjukkan formula umum Beauveria bassiana (GHA strain), dapat menginfeksi dan mengurangi sejumlah WFT dalam rumah kaca, ini menunjukkan bahwa hal itu berpotensi sebagai pestisida konvensional.\nWFT adalah salah satu hama terpenting dalam produksi pemotongan bunga di California. WFT hidup dari daun bunga, membahayakan dan menimbulkan kerusakan estetis sehingga tidak dapat dipasarkan untuk tanaman bunga, banyak semprotan pestisida digunakan khusus untuk thrips. Industri memperkirakan biaya pengendalian hama (tenaga kerja dan materi) mencapai 7,5%¬†dari biaya total keseluruhan. Ini sulit untuk mengukur jumlah yang rusak akibat thrips karena pemerintah tidak menyimpan data statistiknya.\nGangguan tersebut juga menyebabkan banyak kehilangan tanaman pertanian seperti tomat, lada, dan buah berbiji. Dengan memakan langsung daun-daunan dan buah, dan melalui¬†¬¢ovipotional¬¢¬†yang merugikan pada buah selain itu, WFT berperan sebagai vektor untuk¬†¬¢virus tomat¬¢¬†dan virus yang mematikan yang menyerang sejumlah tanaman dan sayuran.\nTaktik manajemen saat ini untuk WFT,¬†frinkliniella occidentalis, dalam produksi budidaya bunga mengendalikan secara utama pada penggunaan pestisida berulang-ulang. Dalam banyak situasi, penanam bunga menggunakan pestisida pada selang waktu 5 sampai 10 hari untuk mengurangi populasi pada tingkat standar. Penggunaan pestisida secara intensif seperti itu menyebabkan WFT anti pestisida berkembang pesat dalam rumah kaca. Usaha untuk mengurangi ketergantungan pada pengendalian secara kimia telah difokuskan pada pengendalian secara biologis dengan menggunakan kuman (mites) yang berifat predator dalam genus¬†amblyseiusdan¬†hypoaspis¬†dan serangga predator bergnus¬†orius.\nBagaimanapun juga pengendalian biologis itu sendiri tidak berhasil mengurangi populasi thrips dalam budidaya bunga sampai pada tingkat standar/normal. Tuntutan yang tinggi terhadap kualitas estetis dan masalah yang berhubungan dengan penyebaran virus, akan membuat pengendalian dengan predator itu sulit untuk berhasil. Karena harga komoditi itu berhubungan dengan kualitas estetis, penanam cenderung untuk tidak toleransi terhadap kerusakan.\nPotensi alternatif lain untuk pestisida tradisional adalah penggunaan jamur entomopathogenic yang mempengaruhi thrips dan arthropoda lainnya. Tentu saja terdapat jamur pathogen yang mematikan banyak spesies arthropoda. Ketika suhu dan RH optimal, sejumlah besar populasi serangga dapat terinfeksi oleh jamur, menyebabkan pengurangan yang berartipada ukuran populasi, hal ini disebut¬†epizootic.\nPeristiwa alami dari epizootics fungi telah berperan dalam usaha memanfaatkan jamur sebagai pengendali hama di lahan dan rumah kaca di seluruh dunia. Sekarang ini penggunaan secara umum telah dibatasi oleh kesulitan dan teknis, yaitu produksi yang berlebihan dan seluk beluk konidia (spora), formulasi dan bentuk tak tetap antara spesies jamur dan strains, sebanding dengan kebutuhan kondisi lingkungan untuk infeksi jamur. Baru-baru ini, bagaimanapun juga kemajuan dalam teknologi fermentasi dan formulasi, dan kemajuan dalam isolasi terhadap spesies dan strain yang terinfeksi telah membuat beberapa pabrik untuk memulai produksi komersial dari jamur untuk pengendalian hama.\nJamur pathogen memiliki beberapa ciri yang membuat mereka menjadi kandidat ideal sebagai pestisida kimia alternatif. Seringkali, jamur itu secara relatif berkelompok secara spesifik, tidak mebahayakan mamalia dan dapat diproduksi besar-besaran dalam media buatan dan dapat menginfeksi target populasi dalam jumlah yang tinggi. Selain itu, jamur dapat dengan mudah diformulasikan dan digunakan dengan peralatan penyemprotan yang standar.\nTemperatur dan RH yang tinggi diperlukan oleh kebanyakan jamur. Bagaimanapun penemuan spesies dan strain baru itu kelihatannya mampu untuk menginfeksi lebih luas pada kondisi lingkungan seperti biasa ditemukan di rumah kaca (60¬∞¬†to 85¬∞F, 50 to 160%¬†RH). Meskipun jamur memiliki perbedaan daur hidup, namun konidiospores (spora) lebih sering digunakan sebagai alat kontrol. Serangga bisa mendapatkan spora dari penyemprotan secara langsung dan dari kontak dari daun yang telah disemprot. Setelah menempel pada sekumpulan serangga yang rentan, spora menghasilkan kuman yang menembus suticle serangga. Hal ini memungkinkan jamur hidup dari tubuh serangga, akhirnya membunuh serangga itu.\nTemperatur yang hangat dan kelembapan yang relatif tinggi menjadikan rumah kaca sebagai lingkungan yang ideal untuk pengguanaan jamur pathogen. Disini, kami mempresentasikan hasil rancangan percobaan laboratorium dan rumah kaca untuk menilai potensi dari penggunaan produksi strain secara komersial dari¬†Beauveria bassiana¬†untuk pengendalian WFT dalam budidaya bunga di rumah kaca. Dua produk komersial yang mengandung jamur yang sekarang ini berharga yaitu Botani Gard (WP) atau Emulsifiable Oil (ES) yang diproduksi oleh perusahaan Mycotech, Montana dan Naturalis-O yang di produksi oleh Troy Biosciences.Inc di Lake Placid, Florida. Dalam laporan eksperimen ini, kami menggunakan Botani Gard Wp dan ES, formula jamur.\n\n\nLaboratorium dan lahan percobaan\nLahan percobaan. Kami mengadakan percobaan laboratorium untuk menilai keefektifan dari¬†B. bassiana¬†melawan segala macam umur WFT jantan dan betina dewasa pada daun bunga mawar dalam konsentrasi spora yang berbeda dibawah kendali suhu dan kelembapan. Konsentrasi 0.1, 0.45, 0.9, dan 1.8 gram spora (4.4¬†¬¥¬†1,010 spora/gram) per 100 ml air ditambah¬†¬¢spreading agent¬¢¬†(0.3%¬†v/v, Silwet L-77) yang telah dicoba pada daun mawar melawan WFT dan membandingkannya dengan WFT yang diobati dengan¬†¬†spreading agent saja. Selain itu, tes tersebut diadakan untuk membandingkan pengaruh tiga kelembapan (60%, 75%, dan 90%) pada mortalitas WFT dalam dua konsentrasi spora (0.9 dan 18 gr spora pr 100 ml air). WFT tersebut telah dibatasi dengan daun-daun mawar dalam kotak kecil dengan¬†¬¢petri dish¬¢¬†yang bersih. Setiap karton mewakili sebuah percobaan tiruan. Empat tiruan mengandung 20 sampai 50 WFT dewasa pada setiap konsentrasi dan kelembapan yang telah dibandingkan.\nKira-kira 0.6 ml suspensi spora¬†B. bassiana¬†telah digunakan untuk daun-daun mawar dan WFT dalam karton dengan menggunakan alat penyemprot laboratorium. Karton tersebut telah ditempatkan pada suhu kamar 78.8¬∞F (26¬∞C) pada salah satu dari RH tersebut. Suhu dan RH karton yang berlubang kira-kira mendekati suhu kamar (dalam 3.6¬∞F [2¬∞C] dan 5%¬†RH). Setiap 24 jam dalam seminggu, kami menghitung jumlah WFT yang mati disetiap karton. Perbdaan jumlah WFT yang mati akibat perlakuan itu telah di analisa oleh ANOVA dan perbandingan rata-rata WFT yang mati akibat perlakuan itu dilakukan dengan menggunakan Metode Dunnet¬¢s pada P = 0,05. Analisa¬†¬≤probit¬≤¬†telah digunakan untuk memperkirakan hubungan tingkat kematian.\nCaged rose trials.Test awal penilaian keampuhan¬†B. bassiana¬†WP melawan WFT betina dewasa pada tanaman berbunga telah dilakukan dalam¬†¬≤caged rose buds¬≤. Pada rumah kaca komersil, 16 pucuk mawar tiruan dalam¬†¬≤cages¬≤¬†telah digunakan dalam randomized complete block design. Kami menyemprot delapan pucuk mawar tiruan dengan¬†B. bassianaWP dengan 1 pon formula per 100 galon air ditambah¬†¬≤spreading agent¬≤¬†dan delapan lainyya disemprot dengan air ditambah¬†¬≤spreading agent¬≤¬†saja lalu dikeringkan. Pucuk bunga itu ditutup dengan¬†¬≤mylar tube cage¬≤¬†yang berlubang untuk ventilasi. Kami mengambil 12 sampai 15 WFT betina dari bunga anyelir dan melepaskannya pada setiap¬†¬≤cages¬≤.\nSetelah 7 hari,¬†¬≤cages¬≤¬†tersbut dibersihkan dari semak-semak bunga dan dikembalikan ke laboratorium. Lalu ditempatkan pada pendingin konvensional untuk membunuh WFT dalam¬†¬≤cages¬≤. Karena tingkat aktifitas WFT yang tinggi, kami perlu menonaktifkan WFT untuk mendapatkan perhitungan. Lalu kami memotong pucuk bunga dan mencatat WFT dewasa dan larvanya. WFT yang ditemukan dalam¬†¬≤cages¬≤¬†diemersikan dalam alkohol untuk membunuh spora pada kutikula serangga dan ditempatkan pada medium agar selektif untuk ditentukan tingkat infeksi jamur dalam WFT percobaan.\nComercial grenhouse trials\nPenelitian lapang telah dilakukan pada¬†¬†dua tempat: Watsonville dan Half Moon Bay. Percobaan tersebut membandingkan daya guna dari¬†B. bassiana¬†WP dan ES formula¬†B. bassiana.\nAnyelir. Watsonville telah melakukan percobaan melawan WFT pada rumahkaca seluas 40.000 square foot pada 3 kultivar anyelir:¬†¬¢Elegance¬¢,¬†¬¢Etna¬¢, dan¬†¬¢Bagatel¬¢. Test plots terdiri dari 3 percobaan dengan 6 replika, untuk setiap percobaan kira-kira 725 SF per plot dari total 13.050 SF. Enam plot telah dicoba dengan¬†B. bassiana¬†WP formula (1 lb/100 gal) dan 6 plot¬†¬†telah dicoba dengan¬†B. bassian¬†ES formula (2 qt/100 gal) dan 6 plot lainnya tanpa perlakuan apapun. Sejumlah WFT yang ada di anyelir telah dihitung terlebih dahulu saat percobaan dimulai. Dua aplikasinya telah dibuat dalam jangka 8 hari, yang pertama 28 November dan yang kedua 6 Desember 1995. untuk membandingkan berat jenis WFT yang di plot, kami mengambil 10-15 contoh bunga anyelir yang sepenuhnya terbuka dari setiap plot pada 6 dan 13 Desember.\nMawar. The Half Moon Bay telah melakukan percobaan melawan WFT dalam luasan 50.000 SF dengan bunga mawar rumahkaca komersial yaitu¬†¬¢Royalty¬¢¬†dan¬†¬¢Caramia¬¢. Percobaan terdiri dari 3 perlakuan, pertama¬†B. bassiana¬†WP ( 1 lb/100 gal), kedua¬†B. bassiana¬†ES ( 2 qt/100 gal), dan yang ketiga hanya menggunakan ES saja sebagai kontrol¬†¬†¬†¬†( 2 qt/100 gal).\nPercobaan telah dibentuk sebagai¬†¬¢randomized complete block design¬¢¬†menggunakan 12 test plot pada luasan sekitar 1185 SF ( untuk setiap plotnya). Setiap dua blok terdiri 6 plot, perlakuan secara acak telah diberikan kepada semua plot dalam setiap blok. Ada 4 replika tiap perlakuan. Satu baris penyangga/penahan telah dipelihara dalam tiap plot percobaan untuk meminimalkan arus/aliran dari penggunaan sempotan. Kami memantau jumlah WFT sebelum dan sesudah percobaan dengan 10 contoh mawar per plot. Penggunaan pada 3 percobaan telah dibuat dalam jangka waktu 7 hari dimulai pada 3 April 1996 dan berakhir 24 April 1996.\nKami mematikan contoh bunga anyelir dan mawar yang terpisah sendiri untuk mencegah pergerakan WFT antar sampel. Sampel dikembalikan ke laboratorium dan isimpan didalam pendingin konvensional sampai pemrosesan lebih lanjut. Kami mengambil WFT dari anyelir dan mawar dan mencatat total jumlahnya. Untuk percobaan pada mawar, 5%¬†klorin bleach digunakan untuk membunuh spora pada kutikula serangga dan menempatkan WFT pada medium agar selektif untuk menentukan tingkat infeksi jamur. Perbedaan WFT yang mati akibat perlakuan dalam¬†¬¢caged rose¬¢¬†dan percobaa komersial telah dianalisa oleh ANOVA dan perbandingan rata-rata WFT yang mati akibat perlakuan itu menggunakan test separasi Tukey pada P = 0.05.\nPengurangan jumlah thrips\nLaboratorium percobaan.Hasil percobaan laboratorium menyatakan bahwa penggunaan¬†B. bassiana¬†WP menyebabkan kematian WFT yang lebih besar pada semua konsentrasi dimana dibandingkan pada kontrol 78.8¬∞F (26¬∞C) dan pH 75%(tabel 1). Kematian WFT tidak melebihi 90%¬†untuk setiap single konsentrasi yang diujikan. Untuk kami memperkirakan konsentrasi optimal berhasil dengan kematian WFT 50%. Probit analysis menyatakan bahwa¬†B.bassiana¬†WP membunuh 50%¬†populasi WFT pada konsentrasi 0.42 gr/100 ml air (tabel 1). Konsentrasi ini sebanding dengan penggunaan 20 trillion spora per acre (1 lb¬†B. bassiana¬†WP). Penggandaan konsentrasi, pertama 0.9 gr/100 ml lalu 1.8 gr/100 ml, tidak menggandakan tingkat kematian, menunjukkan bahwa konsentrasi awal dari 0.45 gr/100 ml hasil produksi mendekati kematian maksimum pada suhu dan RH ini.\nUji menyatakan bahwa tingkat kematian yang signifikan terjadi pada semua tes RH; meskipun demikian tingkat kematian tersebut menunjukkan bahwa untuk meningkatkannya dengan meningkatkan kelembapannya. RH di California bergantung pada wilayah geografis dan tipe rumahkaca tetapi selama24 jam kisaran RH nya 50-100%. Hasil percobaan menguatkan bahwa RH adalah faktor penting yang mempengaruhi tingkat kontrol WFT tetapi penggunaan formula lainnya seharusnya secara signifikan mengurangi jumlah WFT dikondisi rumahkaca California.\nCaged rose trial. Hasil¬†¬¢caged trial¬¢¬†menguatkan bahwa penggunaan¬†B. bassiana¬†untuk WFT yang tersebar pada pucuk mawar¬†¬†dapat menghasilkan pengurangan berarti jumlah WFT, besarnya pengurangan tersebut sama dengan jumlah yang ditemukan di laboratorium pengujian.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180429-visiting-rohingya-refugees-in-coxs-bazaar.html",
    "href": "blog/20180429-visiting-rohingya-refugees-in-coxs-bazaar.html",
    "title": "Visiting Rohingya refugees in Cox‚Äôs Bazaar",
    "section": "",
    "text": "Two-weeks ago I visited Rohingya refugees camp in Cox‚Äôs Bazaar, Bangladesh to conduct the feasibility study of 72hrs approach related to the exposure of Rohingya refugee camps to the upcoming monsoon season and their high vulnerability in face of any flood and landslides call for adequate analysis that can inform humanitarian assistance in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBangladesh will entering cyclone season soon, simulating the impact based on existing cyclone path and combine with latest secondary data will help us understand the situation. Useful feedback and suggestion from team at Coxs also help me to prepare the analysis that already in my head; including viewshed analysis of the base transceiver stations (BTS) to predict the blank spot if the cyclone hit the tower; and travel time analysis to food distribution point will help on planning for the porter system for the logistics base, clinics, food and evouchers distribution points, as well as the areas that are less well covered by the current distribution points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI met my Indonesia colleague who works in here and have lunch+dinner together with some other colleague. My favorite food is Beef Khichuri at Poushee restaurant.\nI also took the time to visit some beach on the way back from Kutupalong to Cox‚Äôs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20211114-terraclimate-data-and-standardized-precipitation-evapotranspiration-index-spei.html",
    "href": "blog/20211114-terraclimate-data-and-standardized-precipitation-evapotranspiration-index-spei.html",
    "title": "TerraClimate and Standardized Precipitation-Evapotranspiration Index (SPEI)",
    "section": "",
    "text": "Last month, I have a chance to give a try TerraClimate data from Climatology Lab of University of California, Merced.\nTerraClimate is a dataset of monthly climate and climatic water balance for global terrestrial surfaces from 1958-2020. These data provide important inputs for ecological and hydrological studies at global scales that require high spatial resolution and time-varying data. All data have monthly temporal resolution and a 4-km (1/24th degree) spatial resolution.\nAnd here‚Äôs some of the datasets\n\nPrimary Climate Variables: Maximum temperature, minimum temperature, vapor pressure, precipitation accumulation, downward surface shortwave radiation, wind-speed\nDerived variables: Reference evapotranspiration (ASCE Penman-Montieth), Runoff, Actual Evapotranspiration, Climate Water Deficit, Soil Moisture, Snow Water Equivalent, Palmer Drought Severity Index, Vapor pressure deficit\n\n\n\n\nName\nDescription\nUnits\n\n\n\n\naet\nActual Evapotranspiration, monthly total\nmm\n\n\ndef\nClimate Water Deficit, monthly total\nmm\n\n\npet\nPotential evapotranspiration, monthly total\nmm\n\n\nppt\nPrecipitation, monthly total\nmm\n\n\nq\nRunoff, monthly total\nmm\n\n\nsoil\nSoil Moisture, total column - at end of month\nmm\n\n\nsrad\nDownward surface shortwave radiation\nW/m2\n\n\nswe\nSnow water equivalent - at end of month\nmm\n\n\ntmax\nMax Temperature, average for month\n¬∞C\n\n\ntmin\nMin Temperature, average for month\n¬∞C\n\n\nvap\nVapor pressure, average for month\nkPa\n\n\nws\nWind speed, average for month\nm/s\n\n\nvpd\nVapor Pressure Deficit, average for month\nkpa\n\n\nPDSI\nPalmer Drought Severity Index, at end of month\nunitless\n\n\n\nIf you are interested to download it, individual years data are accessible via this link: http://thredds.northwestknowledge.net:8080/thredds/catalog/TERRACLIMATE_ALL/data/catalog.html\nI am planning to use TerraClimate data to calculate Standardize Precipitation-Evapotranspiration Index (SPEI). SPEI requires monthly precipitation and potential evapotranspiration data. To calculate SPEI, I will use climate-indices python package which enables the user to calculate SPEI using any gridded netCDF dataset.\nIn my previous blog post, I have write some articles related to SPI calculation using various type of data. I suggest those article as prerequisite reading for you to get familiar with the python package and how to prepare the data before starting the SPEI calculation, as the SPEI step is almost similar.\nThere are certain requirements for input files that vary based on input type.\n\nPrecipitation and potential evapotranspiration unit must be written as millimeters, milimeter, mm, inches, inch or in.\nData dimension and order must be written as lat, lon, time (Windows machine required this order) or time, lat, lon (Works tested on Mac/Linux and Linux running on WSL).\nIf your study area are big, it‚Äôs better to prepare all the data following this dimension order: lat, lon, time as all the data will force following this order during SPEI calculation by NCO module. Let say you only prepare the data as is (leaving the order to lat, lon, time), it also acceptable but it will required lot of memory to use re-ordering the dimension, and usually NCO couldn‚Äôt handle all the process and failed.\n\n\nCalculate SPEI\nBefore starting the calculation, please make sure below points:\n\nYou are still inside climate\\_indices environment to start working on SPEI calculation.\nVariable name on precipitation --var\\_name\\_precip, usually TerraClimate data use ppt as name while other precipitation data like CHIRPS using precip and IMERG using precipitation as a variable name. To make sure, check using command ncdump -h file.nc then adjust it in SPEI script if needed.\nVariable name on potential evapotranspiration --var\\_name\\_pet, usually TerraClimate data use pet as name.\nPrecipitation and potential evapotranspiration unit must be written as millimeters, milimeter, mm, inches, inch or in.\nData dimension and order must be written as lat, lon, time (Windows machine required this order) or time, lat, lon (Works tested on Mac/Linux and Linux running on WSL).\n\nLet‚Äôs start the calculation! In your Terminal, run the following code.\nAbove code is example for calculating SPEI 1 to 72-months. It‚Äôs ok if you think you only need some of them. Example: you are interested to calculate SPEI 1 - 3-months or SPEI 12-months, then adjust above code into --scales 1 2 3 or --scales 12.\nThe above command will compute SPEI (both gamma and Pearson Type III distributions) from monthly precipitation dataset and potential evapotranspiration, and the calibration period used will be Jan-1958 through Dec-2020. The index will be computed at 1,2,3,6,9,12,18,24,36,48,60 and 72-month timescales. The output files will be &lt;out\\_dir&gt;/wld\\_cli\\_spei\\_gamma\\_xx.nc, and &lt;out\\_dir&gt;/wld\\_cli\\_spei\\_pearson\\_xx.nc.\nAfter the calculation process is completed, you can visualize it using Panoply or convert to GeoTIFF using GDAL for further processing.\n\n\nGlobal SPEI data and comparison with other data provider\nBelow is the example of global SPEI-12 as of December 2020\n\nSPEI Global Drought Monitor\nSantiago Begueria and friend from Spanish National Research Council who invented SPEI, released the SPEI Global Drought Monitor which offers near real-time information about drought conditions at the global scale, with a 1 degree spatial resolution and a monthly time resolution.\nLink for SPEI 12-month, December 2020 from Global Drought Monitor - https://spei.csic.es/map/maps.html#months=4#month=11#year=2020\n\nClimate Engine\nClimate Engine is a free web application powered by Google Earth Engine that can be used to create on-demand maps and charts from publicly available satellite and climate data using a standard web browser. Climate Engine allows users to analyze and interact with climate and earth observations for decision support related to drought, water use, agricultural, wildfire, and ecology.\nOne of the product that could generate easily using Climate Engine is SPEI and using TerraClimate data. Link https://climengine.page.link/yMtH\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20220102-hows-2021-rainfall-in-indonesia.html",
    "href": "blog/20220102-hows-2021-rainfall-in-indonesia.html",
    "title": "How‚Äôs 2021 rainfall in Indonesia?",
    "section": "",
    "text": "I frequently used the Global Precipitation Measurement (GPM) - next-generation of the Tropical Rainfall Measuring Mission (TRMM -¬†https://pmm.nasa.gov/TRMM) - data to support my working activities. Among many GPM products, the Integrated Multi-satellitE Retrievals for GPM (IMERG) is most interesting to the users since it delivers the ‚Äòbest‚Äô precipitation estimates by combining data obtained from all available microwave and infrared (IR) platforms of the GPM satellite constellation.\nIMERG is adjusted to GPCP monthly climatology zonally to achieve a bias profile that is considered reasonable. Multiple runs for different user requirements for latency and accuracy\n\n‚ÄúEarly‚Äù ‚Äì 4 hour (example application: flash flooding)\n‚ÄúLate‚Äù ‚Äì 14 hour (crop forecasting)\n‚ÄúFinal‚Äù ‚Äì 3 months (research)\n\nAt 21:43 WIB local time (UTC+7) - 1 Jan 2022, the last daily data for Late run IMERG in 2021 are available for download. It doesn‚Äôt take long, all the daily data for 2021 is already on my Mac mini and ready for further analysis.\nSo, this is what I got! Oh this post also as a warming-up to start first day of work in 2022 tomorrow! üòÜ\n\nAnnual rainfall\n \nThe annual rainfall total¬† for most areas in Indonesia ranging from normal to wet condition during 2021. Most areas of Bali, Nusa Tenggara, Sulawesi and Maluku experience 120-180% above normal precipitation. Wetness occurred specifically during the period of May - Dec.¬†Above maps using CHIRPS which has 40-years climatology data.\n\nAbove map is annual rainfall in 2021 based on IMERG daily data.\n\n\nMaximum daily rainfall\n\nMaximum daily rainfall varies across Indonesia. Far east in Papua, Timor Island and few scattered areas in Kalimantan, North Sulawesi maximum daily rainfall reaches &gt;200 mm. In the remaining areas of Indonesia, maximum daily rainfall ranging between 30 - 150 mm.\nThe biggest rainfall in in a day in 2021 in Indonesia occurred in Pulau Seraja - Kabupaten Natuna, Kepulauan Riau. Some of 527.85 mm of rain fell over 24 hours, on 2 September 2021 UTC+0 (from 2 September 2021, 7am to 3 September 2021, 6am.\n\nThere is a significant variation in the occurrence of maximum daily rainfall. In large part of Java, the highest intensity¬† rainfall occurred on February. West Java, coastal areas of South Sulawesi, and Kalimantan,¬† cape areas of Central Sulawesi experienced maximum rainfall on May. While in Nusa Tenggara, maximum daily rainfall occurred in April.\n\n\nConsecutive Dry Days\n\nEventhough 2021 is generally consider as La Nina years, and most of the areas experience high number of rainfall, prolonged dry spells (days without daily rainfall of 5 mm) were experienced in southern parts of the country. Extreme drought with more than 60 days without rainfall, specifically occurred in Southern area of Central and East Java, Yogyakarta, Bali and Nusa Tenggara. Data of the rainfall were recorded per pixel.\n\nOn average, rainy season ends on March. So April marks the onset of dry season. This is inline with above map, where southern part of the country (Java, Bali and Nusa Tenggara) experience the peak of prolonged drought in September.\n\n\nConsecutive Wet Days\nFollowing the prolonged dry season that resulted to an extreme drought, some areas also experience wetter condition i.e 6 to 20 days of wet spell. Moderate CWD (11-20 days) was detected in Java, while short CWD (6-10 days) occurred in other areas of Indonesia. Moderate to long number of dry spell was experienced scatteredly in small part of Central Java and Papua.\n\nEach areas experienced peak of wet condition in different periods as explained in below map.\n\n\n\nTotal Dry and Wet Days in a Year\n \nFor above maps, I am using 5 mm is a threshold of rainfall total for each day which can be categorised as rainy day and slightly different from what is used widely by meteorological bureau (1 mm).\nThe highest annual number of dry days is found in Sabu Raijua, East Nusa Tenggara, while for highest wet days is found in Weyland mountain, Dogiyai - Papua.\n\nDisclaimer:\nAll content related to climate within this blog post is merely based upon the most current available remote sensing data. As the climate phenomena is a dynamic situation, the current realities may differ from what is depicted in this post.\nGround check is necessary to ensure if satellite and field situation data are corresponding.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240124-drought-propagation-01.html",
    "href": "blog/20240124-drought-propagation-01.html",
    "title": "Drought Propagation",
    "section": "",
    "text": "Last month I did experiment test to analyze the propagation of Meteorological Drought (Standardized Precipitation Index - SPI) to Hydrological Drought (Standardized Streamflow Index - SSI) using Lagged Correlation at the pixel level with area of interest, Indonesia.\nTo download the full repository, you can ccess it via this link: https://github.com/bennyistanto/drought-propagation\n\nData\nI use the Standardized Precipitation Index (SPI) - as proxy for meteorological drought, and the (SSI - as proxy for hydrological drought.\nThe SPI use monthly gridded Satellite precipitation estimates from Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS).\nThe SSI use daily gridded River discharge in the last 24 hours from GloFAS-ERA5 operational global river discharge reanalysis 1979‚Äìpresent as a proxy for the streamflow time series infomation.\n\n\nFolder structure and files\nThere are 3 notebook along with support folder that required to run the analysis. Feel free to use your own preferences for this setting/folder arrangements.\n\nhyd # Files required to proceed the hydrological drought goes here.\nmet # Files required to proceed the meteorological drought goes here.\nprop # File required to proceed the propagation using lagged correlation goes here.\nsubset # In this folder I put idn_subset_chirps.nc file, a subset file to clip the input data to follow the area of interest. Basically this file are came from a shapefile polygon which has land attribute column with value = 1, then converting to raster based on land column, and set the cell size following our standard (I use 0.05 deg, because the SPI and SSI also has the same spatial resolution, 0.05 deg). After that, convert it to netCDF. All is done in ArcGIS Desktop.\n\nThe notebook\n\n1_Steps_to_Generate_SPI_Using_CHIRPS_Data.ipynb\n2_Steps_to_Generate_SSI_Using_GloFAS-ERA5_Data.ipynb\n3_Drought_Propagation_Met2Hyd_Using_CCA.ipynb\n\n\nThis is using Cross-Correlation for each pixel accross the entire time series, also employ noise filtering techniques like Singular Spectrum Analysis (SSA) which can help in isolate the underlying trends and patterns in our data before performing the CCA. This step is crucial for enhancing the signal-to-noise ratio in our datasets.\n\n\n\nApproach\nThe analysis using combination from various time scale [3, 6, 9, and 12-month] and Lag range from 1 - 12 month\ntime_scale_combinations = [\n    \"spi03_ssi03\", \"spi06_ssi03\", \"spi09_ssi03\", \"spi12_ssi03\",\n    \"spi03_ssi06\", \"spi06_ssi06\", \"spi09_ssi06\", \"spi12_ssi06\",\n    \"spi03_ssi09\", \"spi06_ssi09\", \"spi09_ssi09\", \"spi12_ssi09\",\n    \"spi03_ssi12\", \"spi06_ssi12\", \"spi09_ssi12\", \"spi12_ssi12\"\n]\n\nPreprocessing\nThe drought characteristics originally following the method proposed by Yevjevich in 1967 and has been employed to recognize the feature of droughts. The paper from Le, et al in 2019 provide better explanation about it: duration, severity, intensity, and interarrival.\n\n\n\nDrought\n\n\nMasking for Drought Event The drought condition is set when the SPI or SSI value negative, or less than -1.2. Focusing on drought conditions could be a more relevant approach for our analysis compare to using all SPI and SSI data, as it has dry and wet condition. By concentrating on these periods, we can potentially gain more insight into the correlation between meteorological and hydrological droughts.\nCalculate Drought Magnitude Compute the absolute cumulative values during drought events for both datasets. This gives a measure of drought magnitude, which may be more meaningful for correlation analysis than using raw SPI/SSI values.\nApplying Singular Spectrum Analysis (SSA) For noise filtering and trend extraction in drought magnitude data in SPI and SSI datasets. In drought propagation analysis, noise filtering with SSA is a critical step for data preparation. SSA effectively separates the underlying signal from the noise in climate datasets, such as SPI and SSI.\nSSA decomposes a time series into a sum of components:\n\\[X(t) = T(t) + S(t) + N(t)\\]\nWhere:\n\n\\(X(t)\\): Original time series\n\\(T(t)\\): Trend component\n\\(S(t)\\): Seasonal component\n\\(N(t)\\): Noise component\n\nThis process is crucial for enhancing the clarity and accuracy of the data, which in turn facilitates a more precise understanding of drought patterns and their progression.\n\n\nAnalysis\nCross-Correlation Analysis Especially when applied to data refined through SSA noise filtering, is pivotal in understanding drought propagation. This technique examines the relationship between different drought indicators across various time scales. By utilizing data filtered through SSA, which isolates the core signal from noise, Cross-Correlation Analysis can more accurately determine the time lag and intensity with which meteorological droughts (indicated by SPI) translate into hydrological droughts (indicated by SSI).\nThe cross-correlation coefficient \\(\\rho_{xy}(\\tau)\\) at lag \\(\\tau\\) is calculated as:\n\\[\\rho_{xy}(\\tau) = \\frac{\\sum((X_i - \\bar{X})(Y_{i+\\tau} - \\bar{Y}))}{\\sqrt{\\sum(X_i - \\bar{X})^2 \\sum(Y_i - \\bar{Y})^2}}\\]\nwhere:\n\n\\(X_i\\): Value of the first time series at time \\(i\\)\nYi: Value of the second time series at time i + œÑ\nœÑ: Time lag\nXÃÑ: Mean of the first time series\n»≤: Mean of the second time series\nN: Number of data points\n\nThis approach is essential for predicting the onset and progression of drought conditions, enabling timely decision-making and effective resource management to mitigate the adverse impacts of droughts.\nFrequency Analysis In the context of drought propagation analysis, frequency analysis plays a critical role in identifying the most prominent patterns of correlation between meteorological and hydrological drought indicators over time. By classifying cross-correlation values into distinct ranges (e.g., 0.0-0.1, 0.1-0.2, etc.) and analyzing these across different lag times, researchers can pinpoint the range that most frequently occurs.\nThis approach helps in understanding the typical strength of correlation and the temporal shift (lag) between the onset of meteorological drought and its subsequent impact on hydrological conditions. The most frequent range provides insights into the commonality of correlation strengths, while the corresponding lag sheds light on the typical delay between atmospheric changes and their effects on hydrological systems. We can also derive the maximum correlation value that can provides insight on which areas has the best correlation, and Lag time where the maximum correlation between SPI and SSI is observed.\n\n\n\nVisualisation\nThere are two map type that use to illustrate the results of the cross-correlation analysis between meteorological and hydrological droughts.\nLag Map This map displays the time lag (in months) between meteorological and hydrological droughts across the study area. It helps identify regions where hydrological responses to meteorological changes are immediate or delayed.\nStrength Map This map shows the strength of the correlation between meteorological and hydrological droughts. It highlights areas with a strong predictive relationship, indicating regions sensitive to meteorological changes.\nBelow some example of the individual Strength Map from various time scale combinations and lag.\n\nSPI 03 and SSI 03, Lag 1-month\n\n\n\nSM1\n\n\nSPI 06 and SSI 03, Lag 1-month\n\n\n\nSM2\n\n\nSPI 06 and SSI 03, Lag 3-month\n\n\n\nSM3\n\n\nSPI 12 and SSI 06, Lag 6-month\n\n\n\nSM4\n\n\n\nAnd below some example of the composite Strength and Lag Map from various time scale combinations.\n\nMost frequent correlation and Lag where the most frequent observe of SPI to SSI 3-month\n\n\n\nSM1\n\n\nMost frequent correlation and Lag where the most frequent observe of SPI to SSI 6-month\n\n\n\nSM2\n\n\nMost frequent correlation and Lag where the most frequent observe of SPI to SSI 9-month\n\n\n\nSM3\n\n\nMost frequent correlation and Lag where the most frequent observe of SPI to SSI 12-month\n\n\n\nSM4\n\n\nMaximum correlation and Lag where the maximum observe of SPI to SSI 3-month\n\n\n\nSM1\n\n\nMaximum correlation and Lag where the maximum observe of SPI to SSI 6-month\n\n\n\nSM2\n\n\nMaximum correlation and Lag where the maximum observe of SPI to SSI 9-month\n\n\n\nSM3\n\n\nMaximum correlation and Lag where the maximum observe of SPI to SSI 12-month\n\n\n\nSM4\n\n\n\n\n\nTo do\nNumber of Lag from 1-12 month in the existing simulation is good enough.\nAdding more time scale from 3, 6, 9, 12 to 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 and the combination, potentially produce more insight.\nTHIS WORK STILL IN PROGRESS\n\n\nLive testing\nYou can access the notebook via Binder\nhttps://mybinder.org/v2/gh/bennyistanto/drought-propagation/HEAD\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html",
    "href": "blog/20040103-menghitung-radiasi-matahari.html",
    "title": "Menghitung radiasi matahari",
    "section": "",
    "text": "Mata kuliah Mikrometeorologi membuat saya dan teman-teman melakukan pengamatan cuaca selama 24 jam di halaman Kampus IPB Baranangsiang, sangat menyenangkan karena waktu pengamatannya bertepatan dengan perayaan pergantian tahun 2003/2004.\nPraktikum pengukuran profil iklim mikro fluks uap air dan bahang pada permukaan rumput\nTujuan dari praktikum ini adalah:\nSelain melakukan pengamatan, praktikum kali ini juga mengasah kemampuan saya dan teman-teman dalam menerapkan ilmu yang kita pelajari pada mata kuliah Instrumentasi Meteorologi untuk membuat beberapa IoT Anemometer portable dengan interface USB-2, sehingga data yang dihasilkan bisa langsung tersimpan di komputer untuk dianalisa lebih lanjut. Saya bertanggung jawab untuk mengamati Radiasi Netto, merekam data dan menganalisanya. Selain itu saya juga harus membandingkan dengan nilai dugaan Radiasi menggunakan pendekatan rumus empirik.\nSlide di bawah ini adalah materi kuliah tentang radiasi matahari yang disampaikan Bu Tania June seminggu yang lalu, sebelum kita melakukan pengamatan lapangan."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#hasil-pengamatan",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#hasil-pengamatan",
    "title": "Menghitung radiasi matahari",
    "section": "Hasil pengamatan",
    "text": "Hasil pengamatan\nBerdasarkan hasil praktikum terlihat adanya perubahan radiasi netto pada siang dan malam hari. Satuan miliVolt (mV) menunjukkan bahwa pengukuran radiasi netto ini menggunakan Net Radiometer dan DVM (Digital Volt Meter), diukur setiap 30 menit. Nilai Radiasi Netto pada siang hari ditunjukkan oleh grafik di bawah ini:\nSelanjutnya saya harus membandingkan nilai radiasi hasil pengukuran dengan hasil dugaan menggunakan rumus empiris. Referensi saya adalah dokumen dari FAO no 56: Crop evapotranspiration - Guidelines for computing crop water requirements - FAO Irrigation and drainage paper 56 (http://www.fao.org/3/x0490e/x0490e00.htm#Contents): Chapter 3 yang membahas tentang data meteorologi, sub topik radiasi: http://www.fao.org/3/x0490e/x0490e07.htm#radiation\nMenurut dokumen di atas, konsep dari perhitungan radiasi adalah sebagai berikut:"
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-ekstraterestrial-ra",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-ekstraterestrial-ra",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi ekstraterestrial (Ra)",
    "text": "Radiasi ekstraterestrial (Ra)\nRadiasi yang mengenai permukaan tegak lurus terhadap sinar matahari di bagian atas atmosfer bumi, yang disebut konstanta matahari, adalah sekitar 0,082 MJ m-2 mnt-1. Intensitas lokal radiasi ditentukan oleh sudut antara arah sinar matahari ke permukaan atmosfer. Sudut ini akan berubah pada siang hari dan akan berbeda di lintang yang berbeda dan di musim yang berbeda. Radiasi matahari yang diterima di bagian atas atmosfer bumi pada permukaan horizontal disebut radiasi ekstraterestrial, Ra. Jika matahari berada tepat di atas kepala, sudut datangnya nol dan radiasi ekstraterestrial 0,0820 MJ m-2 min-1. Saat musim berubah, posisi matahari dan panjang hari juga berubah, oleh karena itu Ra juga berubah. Radiasi ekstraterestrial dengan demikian merupakan fungsi dari garis lintang, tanggal dan waktu hari.\n\nRa untuk periode harian\nRa untuk periode harian dalam setahun dan untuk lintang yang berbeda dapat diperkirakan dari konstanta matahari, deklinasi matahari, dan waktu dalam setahun dengan:\nRa = (24(60))/œÄ * Gsc¬†* dr¬†* [œâs¬†* sin(œÜ) * sin(Œ¥) + cos(œÜ) * cos(Œ¥) * sin(œâs)] (Persamaan 1)\ndimana :\n\nRa radiasi ekstraterestrial [MJ m-2 hari-1]\nGsc konstanta matahari = 0,0820 MJ m-2 menit-1\ndr jarak relatif terbalik Bumi-Matahari (Persamaan 4)\nœâs sudut waktu matahari terbenam, dalam radian (Persamaan 7 atau 8),\nœÜ latitude, dalam radian (Persamaan 3)\nŒ¥ sudut deklinasi matahari, dalam radian (Persamaan 5).\n\nDalam persamaan FAO Penman-Monteith tentang Evapotranspirasi, radiasi yang dinyatakan dalam MJ m-2 hari-1 dikonversi menjadi penguapan yang setara dalam mm hari-1 dengan menggunakan faktor konversi yang sama dengan kebalikan dari panas laten penguapan (1 / l = 0,408):\n\\[\\text{evaporasi [mm hari}^{-1}\\text{]} = 0.408 \\times \\text{Radiasi [MJ m}^{-2}\\text{ hari}^{-1}\\text{]} \\tag{2}\\]\nRa diekspresikan dalam persamaan di atas dalam MJ m-2 hari-1. Penguapan dalam mm hari-1 diperoleh dengan mengalikan Ra dengan 0,408 (Persamaan 2). Garis lintang, œÜ, dinyatakan dalam radian positif untuk belahan bumi utara dan negatif untuk belahan selatan. Konversi dari derajat desimal ke radian diberikan oleh:\n\\[[\\text{Radians}] = \\frac{\\pi}{180} \\times [\\text{decimal degrees}] \\tag{3}\\]\nJarak relatif terbalik Bumi-Matahari, dr, dan deklinasi matahari, d, dihitung dari:\n\\[d_r = 1 + 0.033 \\cos\\left(\\frac{2\\pi}{365} J\\right) \\tag{4}\\]\n\\[\\delta = 0.409 \\sin\\left(\\frac{2\\pi}{365} J - 1.39\\right) \\tag{5}\\]\ndimana J adalah tanggal dalam Julian (https://landweb.modaps.eosdis.nasa.gov/browse/calendar.html), antara 1 (1 Januari) dan 365 atau 366 (31 Desember). J dapat ditentukan untuk setiap hari (D) bulan (M) melalui:\nJ = INTEGER (275 M / 9 - 30 + D) - 2\nIF (M &lt;3) THEN J = J + 2\nIF (‚Äútahun kabisat‚Äù and (M&gt; 2)) THEN J = J + 1 (Persamaan 6)\nSudut waktu matahari terbenam, œâs, ditentukan dari:\n\\[\\omega_s = \\arccos[-\\tan(\\phi) \\tan(\\delta)] \\tag{7}\\]\nKarena fungsi arccos tidak tersedia dalam semua bahasa komputer, sudut waktu matahari terbenam juga dapat dihitung menggunakan fungsi arctan:\n\\[\\omega_s = \\frac{\\pi}{2} - \\arctan\\left[\\frac{-\\tan(\\phi) \\tan(\\delta)}{X^{0.5}}\\right] \\tag{8}\\]\ndimana:\n\\[X = 1 - [\\tan(\\phi)]^2 [\\tan(\\delta)]^2 \\tag{9}\\]\ndan \\(X = 0.00001\\) jika \\(X \\leq 0\\)\n\n\nRa untuk periode per jam atau yang lebih pendek\nUntuk periode per jam atau lebih pendek, sudut waktu matahari pada awal dan akhir periode harus dipertimbangkan ketika menghitung Ra:\nRa = (12(60))/œÄ * Gsc¬†* dr¬†* [(œâ2 -¬†œâ1)¬†* sin(œÜ) * sin(Œ¥) + cos(œÜ) * cos(Œ¥) * (sin(œâ2)-sin(œâ1))] (Persamaan 10)\ndimana :\n\nRa radiasi ekstraterestrial per jam atau periode yang lebih pendek [MJ m-2 jam-1],\nGsc konstanta matahari = 0,0820 MJ m-2 menit-1,\ndr jarak relatif terbalik Bumi-Matahari (Persamaan 4),\nŒ¥ sudut deklinasi matahari, dalam radian (Persamaan 5),\nœÜ latitude, dalam radian (Persamaan 3),\nœâ1 sudut waktu matahari terbit, dalam radian (Persamaan 11),\nœâ2 sudut waktu matahari terbenam, dalam radian (Persamaan 12),\n\nSudut waktu matahari terbit dan tenggelam dihitung menggunakan persamaan berikut:\n\\[\\omega_1 = \\omega - \\frac{\\pi t_1}{24} \\tag{11}\\]\n\\[\\omega_2 = \\omega + \\frac{\\pi t_1}{24} \\tag{12}\\]\ndimana\n\nœâ sudut waktu matahari pada tengah hari atau periode lebih pendek, dalam radian,\nt1 panjang periode perhitungan [jam]: misal. 1 untuk periode per jam atau 0,5 untuk periode 30 menit.\n\nSudut waktu matahari di titik tengah periode adalah:\n\\[\\omega = \\frac{\\pi}{12} [(t + 0.06667 (L_z - L_m) + S_c) - 12] \\tag{13}\\]\ndimana\n\nt waktu jam standar di tengah periode [jam]. Misalnya untuk periode antara 14.00 dan 15.00 jam, t = 14.5,\nLz lokasi bujur dari titik tengah dari zona waktu lokal [derajat Bujur Barat Greenwich]. Misalnya, Lz = 75, 90, 105 dan 120¬∞ untuk zona waktu wilayah Timur, Tengah, Pegunungan Rocky dan Pasifik (Amerika Serikat) dan Lz = 0¬∞ untuk Greenwich, 330¬∞ untuk Kairo (Mesir), dan 255¬∞ untuk Bangkok (Thailand),\nLm bujur dari lokasi pengukuran [derajat Bujur Barat Greenwich],\nSc Koreksi musiman untuk waktu matahari [jam].\n\nTentu saja, jika w &lt; -ws atau w &gt; ws dari Persamaan 13 menunjukkan bahwa matahari berada di bawah cakrawala sehingga, menurut definisi, Ra adalah nol.\nKoreksi musiman untuk waktu matahari adalah:\n\\[S_c = 0.1645 \\sin(2b) - 0.1255 \\cos(b) - 0.025 \\sin(b) \\tag{14}\\]\n\\[b = \\frac{2\\pi(J-81)}{364} \\tag{15}\\]\ndimana J adalah hari ke-n dalam setahun."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#lama-penyinaran-dalam-jam-n",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#lama-penyinaran-dalam-jam-n",
    "title": "Menghitung radiasi matahari",
    "section": "Lama penyinaran, dalam jam (N)",
    "text": "Lama penyinaran, dalam jam (N)\nDihitung menggunakan persamaan berikut:\nN = 24/œÄ *¬†œâs (Persamaan 16)\ndi mana œâs adalah sudut waktu matahari terbenam dalam radian yang diberikan oleh Persamaan 7 atau 8."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-rs",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-rs",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi matahari (Rs)",
    "text": "Radiasi matahari (Rs)\nJika radiasi matahari, Rs, tidak terukur, dapat dihitung dengan rumus Angstrom yang menghubungkan radiasi matahari dengan radiasi ekstraterestrial dan durasi sinar matahari relatif:\n\\[R_s = (a_s + b_s \\frac{n}{N}) R_a \\tag{17}\\]\ndimana\n\nRs radiasi matahari atau gelombang pendek [MJ m-2 hari-1],\nn durasi aktual sinar matahari [jam],\nN durasi maksimum yang mungkin dari sinar matahari atau panjang hari [jam],\nn/N durasi sinar matahari relatif [-],\nRa radiasi ekstarterrestrial [MJ m-2 hari-1],\nas sebagai konstanta regresi, menyatakan fraksi radiasi ekstraterestrial yang mencapai bumi pada hari-hari mendung (n = 0),\nas+bs sebagai fraksi radiasi ekstraterestrial yang mencapai bumi pada hari-hari cerah (n = N).\n\nRs dinyatakan dalam persamaan di atas dalam MJ m-2 hari-1. Penguapan setara yang sesuai dalam mm hari-1 diperoleh dengan mengalikan Rs dengan 0,408 (Persamaan 2). Bergantung pada kondisi atmosfer (kelembaban, debu) dan deklinasi matahari (garis lintang dan bulan), nilai Angstrom as dan bs akan bervariasi. Ketika tidak ada data radiasi matahari aktual yang tersedia dan tidak ada kalibrasi telah dilakukan untuk parameter as dan bs yang ditingkatkan, nilai yang direkomendasikan untuk as = 0,25 dan bs = 0,50.\nRadiasi ekstraterestrial, Ra, dan panjang hari atau durasi maksimum yang mungkin dari sinar matahari, N, diberikan oleh Persamaan 10 dan 16. Durasi aktual sinar matahari, n, direkam dengan perekam sinar matahari Campbell Stokes."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-ketika-langit-cerah-rso",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-ketika-langit-cerah-rso",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi matahari, ketika langit cerah (Rso)",
    "text": "Radiasi matahari, ketika langit cerah (Rso)\nPerhitungan radiasi ketika langit-cerah, Rso, ketika n = N, diperlukan untuk menghitung radiasi gelombang panjang netto.\nUntuk dekat permukaan laut atau ketika nilai kalibrasi untuk as dan bs tersedia:\n\\[R_{so} = (a_s + b_s) R_a \\tag{18}\\]\ndimana\n\nRso radiasi matahari ketika langit cerah [MJ m-2 hari-1],\nas + bs fraksi radiasi ekstraterestrial yang mencapai bumi pada hari-hari langit cerah (n = N).\n\nKetika nilai kalibrasi untuk as dan bs tidak tersedia:\n\\[R_{so} = (0.75 + 2 \\times 10^{-5}z) R_a \\tag{19}\\]\ndimana\n\nz ketinggian stasiun di atas permukaan laut [m]."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-atau-gelombang-pendek-netto-rns",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-matahari-atau-gelombang-pendek-netto-rns",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi matahari atau gelombang pendek netto (Rns)",
    "text": "Radiasi matahari atau gelombang pendek netto (Rns)\nRadiasi gelombang pendek netto yang dihasilkan dari keseimbangan antara radiasi matahari yang masuk dan yang dipantulkan diberikan oleh:\n\\[R_{ns} = (1-\\alpha) R_s \\tag{20}\\]\ndimana\n\nRns radiasi matahari atau gelombang pendek netto [MJ m-2 hari-1],\na koefisien refleksi albedo atau kanopi, yang bernilai 0.23 untuk referensi hipotesis tanaman rumput [tidak mempunyai unit],\nRs radiasi matahari yang masuk [MJ m-2 hari-1]."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-gelombang-panjang-netto-rnl",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-gelombang-panjang-netto-rnl",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi gelombang panjang netto (Rnl)",
    "text": "Radiasi gelombang panjang netto (Rnl)\nTingkat emisi energi gelombang panjang sebanding dengan suhu absolut permukaan yang dipangkatkan empat. Hubungan ini diekspresikan secara kuantitatif oleh hukum Stefan-Boltzmann. Namun, fluks energi bersih yang meninggalkan permukaan bumi kurang dari yang dipancarkan dan diberikan oleh hukum Stefan-Boltzmann karena penyerapan dan radiasi ke bawah dari langit. Uap air, awan, karbon dioksida, dan debu adalah penyerap dan penghasil radiasi gelombang panjang. Konsentrasi mereka harus diketahui ketika menilai fluks keluar netto. Karena kelembaban dan kekeruhan memainkan peran penting, hukum Stefan-Boltzmann dikoreksi oleh dua faktor ini ketika memperkirakan - fluks keluar radiasi gelombang panjang. Dengan demikian diasumsikan bahwa konsentrasi peredam lain adalah konstan:\n\\[R_{nl} = \\sigma\\left[\\frac{(T_{max} + 273.16)^4 + (T_{min} + 273.16)^4}{2}\\right] (0.34 - 0.14\\sqrt{e_a}) \\left[1.35 \\frac{R_s}{R_{so}} - 0.35\\right] \\tag{21}\\]\ndimana\n\nRnl radiasi gelombang panjang keluar netto [MJ m-2 hari-1],\nœÉ Konstanta Stefan-Boltzmann [4,903 10-9 MJ K-4 m-2 hari-1],\nTmax, dalam Kelvin, suhu absolut maksimum selama periode 24 jam [K = ¬∞C + 273,16],\nTmin, dalam Kelvin, suhu absolut minimum selama periode 24 jam [K = ¬∞C + 273,16],\nea tekanan uap aktual [kPa],\nRs/Rso Radiasi gelombang pendek relatif (terbatas pada ‚â§ 1.0),\nRs dihitung dari (Persamaan 17) radiasi matahari [MJ m-2 hari-1],\nRso dihitung dari (Persamaan 18 atau 19) radiasi langit cerah [MJ m-2 hari-1].\n\nRata-rata suhu udara maksimum dan suhu udara minimum yang dipangkatkan empat biasanya digunakan dalam persamaan Stefan-Boltzmann untuk periode waktu 24 jam. Istilah (0.34-0.14‚àöea) menyatakan koreksi untuk kelembaban udara, dan akan lebih kecil jika kelembaban meningkat. Efek kekeruhan diungkapkan oleh (1.35 Rs / Rso - 0.35). Istilah ini menjadi lebih kecil jika kekeruhan meningkat dan karenanya Rs berkurang. Semakin kecil ketentuan koreksi, semakin kecil fluks keluar radiasi gelombang panjang. Perhatikan bahwa istilah Rs/Rso dalam Persamaan 39 harus dibatasi sehingga Rs / Rso ‚â§ 1.0.\nDi mana pengukuran radiasi pendek dan gelombang panjang masuk dan keluar selama langit cerah dan mendung tersedia, kalibrasi koefisien dalam Persamaan 39 dapat dilakukan."
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-netto-rn",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#radiasi-netto-rn",
    "title": "Menghitung radiasi matahari",
    "section": "Radiasi netto (Rn)",
    "text": "Radiasi netto (Rn)\nRadiasi netto (Rn) adalah perbedaan antara radiasi gelombang pendek netto masuk (Rns) dan radiasi gelombang panjang netto keluar (Rnl):\n\\[R_n = R_{ns} - R_{nl} \\tag{22}\\]"
  },
  {
    "objectID": "blog/20040103-menghitung-radiasi-matahari.html#script-menggunakan-bahasa-basic",
    "href": "blog/20040103-menghitung-radiasi-matahari.html#script-menggunakan-bahasa-basic",
    "title": "Menghitung radiasi matahari",
    "section": "Script menggunakan bahasa BASIC",
    "text": "Script menggunakan bahasa BASIC\nSelanjutnya untuk memudahkan perhitungan radiasi harian selama 1 tahun, beberapa persamaan diatas saya tulis menggunakan bahasa BASIC (kodenya dapat dilihat dibagian bawah tulisan). Dan untuk perhitungan 1 waktu, saya buat user interface sederhana menggunakan Visual Basic 6 seperti gambar dibawah.\n\n\n\nGambar 1. UI program menghitung radiasi matahari.\n\n\nGambar 1. UI program menghitung radiasi matahari."
  },
  {
    "objectID": "blog/20210901-batch-download-30-minutes-rainfall-data-from-multiple-coordinates-and-dates.html",
    "href": "blog/20210901-batch-download-30-minutes-rainfall-data-from-multiple-coordinates-and-dates.html",
    "title": "Batch download 30-minutes rainfall data from multiple coordinates and dates",
    "section": "",
    "text": "The need for the availability of 30-minutes rainfall time series data are very crucial, especially when we are working on the study of extreme rainfall induced landslide.\nDuring 2018, landslide dominated natural disasters occurred in Central Java. The Regional Disaster Management Agency (BPBD) of Central Java Province recorded that there were about 2,000 landslides in this area. Most landslide is preceeded by continuous extreme rainfall for few days, as most of the area isn‚Äôt located near ground weather station, therefore rainfall records are often not available.\nIn this post, I have talk about how we can extract half-hourly IMERG rainfall in single location and date. Now I will explain on how to batch download 30-minutes rainfall data from multiple coordinates and dates.\nFor this example, I am still using Landslide event as the case, and Google Earth Engine as a tool for downloading the data.\n\nData Source\n\nHalf hourly IMERG at Earth Engine Data Catalogue - https://developers.google.com/earth-engine/datasets/catalog/NASA_GPM_L3_IMERG_V06\nLandslide event in Magelang, Central Java - Indonesia during 2018. Compiled by Department of Environmental Geography, Faculty of Geography - Universitas Gadjah Mada.\n\n\n\nAvailable in CSV format with column structure: ID, Lon, Lat, Day, DD, MM, YYYY, TimeWIB.\nExample of landslide event data accessible via this linkÔªø, and below picture. There are 10 landslide event that happen during 2018.\n\n\n\n\nScript\nThe script below describe how to extract 30-minute rainfall from NASA GPM-IMERG, based on points location and convert it into CSV file using Google Earth Engine code editor.\nFull GEE script available via this link https://code.earthengine.google.com/b7ccc8291d23770301e96a22bd1be2c8\n\n\nOutput\n30-minutes of rainfall that occurred 10-days before landslide. Generated using GEE, CSV output is accessible via this link.\n\n\n\nAbout\nI worked with my 2 other colleagues: Ridwan Mulyadi from WFP-VAM and Guruh Samodra from UGM. This activity is part of research and development of threshold for extreme rainfall that could trigger a landslide event in Indonesia. Reference: https://bennyistanto.github.io/erm/ls/#extreme-rainfall-triggered-landslide-alert\nSource: https://github.com/bennyistanto/landslide-rainfall\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20250228-word-clock.html",
    "href": "blog/20250228-word-clock.html",
    "title": "Word Clock",
    "section": "",
    "text": "Ever wondered what time it would be if clocks could speak? This Word Clock translates the current time into natural language phrases that we use in everyday conversation. Instead of looking at hands or digits, you simply read the highlighted words to tell the time!\n‚ÄúIT IS TWENTY MINUTES PAST FOUR‚Äù or ‚ÄúIT IS QUARTER TO SEVEN‚Äù - just like you‚Äôd say it to a friend.\nThe clock updates every minute, automatically highlighting the words that form the correct time phrase. Notice how at different times, different word combinations light up to create readable sentences.\nIt‚Äôs a fun, more human way to experience time passing. Enjoy watching the words change as the minutes tick by!\nInspiration\nInspired by the Word Clock sold by Walmart - The Word Clock - Shows The Time In A Sentence\n\nSee it at Observable - https://observablehq.com/@bennyistanto/word-clock\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me.",
    "section": "",
    "text": "Hi! my name is Benny Istanto, an agricultural meteorologist by training and a certified GIS Professional (GISP). With over 15 years of experience working with the United Nations and various international organizations, specializes in integrating advanced GIS modeling with climate technology to drive international development initiatives.\nI create maps, play with spatial data and satellite imagery. In addition to technical activities, I enjoy listening to Iwan Fals, Led Zeppelin, and MR.BIG, as well as other similar music genres. During my free time, I practice pencak silat, hike, cycle, draw, and occasionally post on social media.\nMy formal education background was actually Meteorology, I got my Bachelor for this major in 2006 from IPB. However my interest has been growing much broader since I was a student. I love GIS, Remote Sensing and computer programming and decided to work on those fields. I‚Äôve been very lucky to get various working opportunities. Got the chances to work with governments, non-government and international organizations in disaster risk reduction, climate adaptation and mitigation and some other interesting fields.\nFrom my work I‚Äôve got chances to travel, live in some interesting places, learn interesting cultures, eat lots of delicious (or sometimes weird) foods! I love what I do and I am eager to keep continuing this path. People says that the more we get involved, the more interesting this is. The more I learn, the more I realized how much I do not know.\nFrom June 2012 - August 2021, I was an Earth Observation and Climate Analyst at the United Nations World Food Programme based in Indonesia, and played the role of geospatial and climate technologist for Indonesia Country Office and Regional Bureau Bangkok (RBB) for Asia and Pacific, covering 17 country offices and 5 oversight countries. During my service with WFP, my two-others colleagues and I started working on an award-winning tech innovation project to solve hunger called VAMPIRE (Vulnerability Analysis and Monitoring Platform for Impact of Regional Events) in 2015, now rebranding as PRISM. In my last 4 years within WFP, I have been working on the process of transforming satellite-based data product into actionable and life-saving insight for hydro-meteorological hazards early-warning early-action.\nCurrently, I work as a Climate Geographer at Geospatial Operations Support Team (GOST) of Development Economic Data Group (DECDG) at The World Bank, Washington DC.\nI run a ‚Äúcasual weekend project‚Äù called Climate Social Responsibility (CSR), to provide satellite-based climate and vegetation products for free, i.e.¬†rainfall and anomaly, dry and wet spell, standardized precipitation index, and crop phenology.\nDisclaimer: All content on this website does not represent the views of my (current or previous) employer.\nAcknowledgement: Development of this website is supported by my cheering team El and beloved-wife Kiki Kartikasari."
  },
  {
    "objectID": "about.html#background.",
    "href": "about.html#background.",
    "title": "About me.",
    "section": "Background.",
    "text": "Background.\n\n\nEducation.\nBogor Agricultural University, Bogor, Indonesia (09/2001 - 03/2006)\nSarjana Sains (B.Sc.) in Meteorology with a focus on agriculture meteorology and climatology, computation and agriculture simulation model, satellite meteorology, with minor in geographic information system and remote sensing.\n\nSupervisors: Idung Risdiyanto (Bogor Agricultural University), M. Rokhis Khomarudin (Indonesian National Institute of Aeronautics and Spaces)\nBachelor thesis: Software development to determine the Canadian Forest Fire Weather Index System using remote sensing satellite. (in Bahasa Indonesia). Source: IPB Scientific Repository\n\n\nHarvard University on edX, T.H. Chan School of Public Health, United States (01/2022 - 10/2024)\n\nProfessional Certificate of Data Science. View Credential\nCapstone Projects: HarvardX PH125.9x Data Science Capstone\n\n\n\nAwards.\nJacub Rais award\nIndonesian Society for Remote Sensing (MAPIN), December 2006.\nThe Best Author in 15th Annual Scientific gathering and 4th Congress of Indonesian Society for Remote Sensing. Bandung, Indonesia. Paper entitled ‚ÄúSoftware development for forest fire early warning system in Indonesia‚Äù. (in Bahasa Indonesia)\n\nThe WFP GIS Community Award\nWorld Food Programme, worldwide\nThe Best GIS Officer in the 1st WFP GIS Community Award. Rome, Italy. June 2015.\n\nInnovative New Solution to Hunger\nWorld Food Programme, worldwide\nVAMPIRE (Vulnerability Analysis Monitoring Platform for the Impact of Regional Events) won the 2017 WFP Innovation Challenge. Rome, Italy. January 2018."
  },
  {
    "objectID": "about.html#expertise.",
    "href": "about.html#expertise.",
    "title": "About me.",
    "section": "Expertise.",
    "text": "Expertise.\n\n\nDomains.\n\nAtmospheric science, agriculture meteorology and climatology, climate risk\nGIS, remote-sensing and agriculture related spatial modeling\nClimate, geo-statistics and other scientific data processing\nImpact science, early-warning and anticipatory-action\nCapacity development on geospatial and climate analytics\n\n\n\nTools.\n\nESRI ArcGIS Family: Desktop, Pro, Server, Online, Portal\nFOSS4G: QGIS, GDAL/OGR, SAGA, Whitebox, CDO, NCO\nData Catalogue: GeoNode, CKAN, JKAN\nProgramming: Google Earth Engine, bash, python, arcpy, jupyter or ESRI notebook\nDesign: Photoshop/GIMP and Illustrator/CorelDRAW/Inkscape, Avenza\nDocumentation: Markdown, Material for MkDocs, Jupyter Book, Quarto"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About me.",
    "section": "???",
    "text": "???\nIt all begins with a question.\nI‚Äôm medium-active on Telegram, you can discuss or post question about gis and remote sensing at @gis_id Telegram channel, and I will endeavour to answer them. Don‚Äôt hesitate to contact me if you have something to say: my email is below, you can text me too via WhatsApp or via message box below. Find more about me in this website and get in touch, happy surfing!"
  },
  {
    "objectID": "about.html#contact.",
    "href": "about.html#contact.",
    "title": "About me.",
    "section": "Contact.",
    "text": "Contact.\n\n\nGet in Touch\n benny@istan.to\n+1 (940) 604-3852\nCurrently living in Bogor, Indonesia\nConnect with me:\n\n GitHub\n LinkedIn\n Website\n\n\n  \n    \n  \n\n\n\nSend a Message\n\n  \n    Name\n    \n  \n\n  \n    Email\n    \n  \n\n  \n    Message\n    \n  \n\n  Send Message"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWord Clock\n\n\n\nData Science\n\nGeneral\n\n\n\nA creative visualization that transforms time into natural language - experience time as spoken words rather than digits or clock hands\n\n\n\nBenny Istanto\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nxkcd style for Country map\n\n\n\nGeneral\n\nGIS\n\n\n\nSource: &lt;https://gist\n\n\n\nBenny Istanto\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\nxkcd style for LSEQM illustration\n\n\n\nGeneral\n\nClimate\n\n\n\nSource: &lt;https://gist\n\n\n\nBenny Istanto\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSkip PEARSON fitting on climate-indices python package\n\n\n\nRemote Sensing\n\nClimate\n\n\n\nSource: &lt;https://gist\n\n\n\nBenny Istanto\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\nUtilizing CUDA\n\n\n\nGeneral\n\n\n\nThis week I try to utilize CUDA on my desktop, to support the upcoming activities on heavy geospatial and climate analytics\n\n\n\nBenny Istanto\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing Thinkpad T14 Gen 2 AMD\n\n\n\nGeneral\n\n\n\nI bought a Thinkpad T14 Gen 2 AMD (released on August 2022) end of December 2023, it‚Äôs second hand with mint condition and standard specification (AMD Ryzen‚Ñ¢ 7 PRO 5850U‚Ä¶\n\n\n\nBenny Istanto\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrought Propagation\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nLast month I did experiment test to analyze the propagation of Meteorological Drought (Standardized Precipitation Index - SPI) to Hydrological Drought (Standardized‚Ä¶\n\n\n\nBenny Istanto\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirmware upgrade on Thuraya SatSleeve for iPhone\n\n\n\nGeneral\n\n\n\n\n\n\n\nBenny Istanto\n\n\nJan 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nHourly Humidity Data\n\n\n\nRemote Sensing\n\nClimate\n\n\n\n\n\n\n\nBenny Istanto\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nA certified GISP\n\n\n\nGeneral\n\n\n\n\n\n\n\nBenny Istanto\n\n\nAug 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonthly mosaic of modified Radar Vegetation Index\n\n\n\nRemote Sensing\n\n\n\n\n\n\n\nBenny Istanto\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nParsing BMKG‚Äôs daily climate data\n\n\n\nClimate\n\n\n\nTo replicate below code, please download daily climate data from BMKG Data Online &lt;https://dataonline\n\n\n\nBenny Istanto\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\nSPI-based drought characteristics\n\n\n\nRemote Sensing\n\nGIS\n\nClimate\n\n\n\n\n\n\n\nBenny Istanto\n\n\nAug 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nFourier regression model to generate monthly to daily temperature data\n\n\n\nResearch\n\nClimate\n\n\n\n1 Introduction In the sphere of meteorology, the significance of statistical models in comprehending and forecasting diverse weather patterns is incontestable\n\n\n\nBenny Istanto\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression analysis with dummy variables\n\n\n\nData Science\n\nClimate\n\n\n\nThis exercise aims to determine the best reduced model (RM) in regression analysis with dummy variables from annual rainfall data and altitude data in three different regions\n\n\n\nBenny Istanto\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-1 modified Radar Vegetation Index\n\n\n\nRemote Sensing\n\n\n\nThe Sentinel-1 modified Radar Vegetation Index (RVI) based on Google Earth Engine (GEE) script below originally developed by my friend Jose Manuel Delgado Blasco (Scholar‚Ä¶\n\n\n\nBenny Istanto\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond-order Markov chain model to generate time series of occurrence and rainfall\n\n\n\nResearch\n\nClimate\n\n\n\n1 Introduction In the realm of meteorological studies, the use of statistical models is pivotal for understanding and predicting various weather phenomena\n\n\n\nBenny Istanto\n\n\nMay 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of climate change in cities\n\n\n\nResearch\n\nClimate\n\n\n\nA new World Bank report is launched, in which I had the opportunity to contribute the analysis\n\n\n\nBenny Istanto\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuzzy Inference System (FIS) for Flood Risk Assessment\n\n\n\nData Science\n\nClimate\n\n\n\n1 Implementation In the implementation phase of this analysis, we utilized Python and the Scikit-Fuzzy library to develop a fuzzy logic-based flood risk assessment model\n\n\n\nBenny Istanto\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising the WRF output\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\n1 Introduction The Weather Research and Forecasting (WRF) model is a powerful numerical weather prediction system used to simulate atmospheric phenomena at various scales\n\n\n\nBenny Istanto\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring dry and wet season in South Sudan\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nI am excited to share some news with you all\n\n\n\nBenny Istanto\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPyCPT for Subseasonal Forecasts in Indonesia\n\n\n\nClimate\n\nRemote Sensing\n\nResearch\n\n\n\n1 Introduction PyCPTis a Python library for statistical analysis and forecasting of climate data\n\n\n\nBenny Istanto\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimental climatological rainfall zone\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nClimatological rainfall zones represent distinct areas with different rainfall patterns\n\n\n\nBenny Istanto\n\n\nMar 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nPyCPT config and notebook\n\n\n\nClimate\n\nRemote Sensing\n\nResearch\n\n\n\nPyCPT is a Python interface and enhancement for the command line version of the International Research Institute for Climate and Society‚Äôs Climate Predictability Tool (CPT)‚Ä¶\n\n\n\nBenny Istanto\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit Hydrographs\n\n\n\nClimate\n\n\n\nUnit hydrographs are a fundamental tool in the analysis of floods and their impacts on watersheds\n\n\n\nBenny Istanto\n\n\nFeb 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSBR Market Day 2023: When Creativity and Demon Slayer Colors Led to Success\n\n\n\nGeneral\n\n\n\nMy fourth-grade daughter recently participated in her school‚Äôs market day as part of the IB Primary Years Programme at Sekolah Bogor Raya, and it turned into an inspiring‚Ä¶\n\n\n\nBenny Istanto\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nInstall the WRF Model in WSL2\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nUPDATE: 25 Feb 2024 Old version: &lt;https://gist\n\n\n\nBenny Istanto\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive statistics analysis using climate data\n\n\n\nRemote Sensing\n\nClimate\n\n\n\nClimate is a complex system that is affected by a variety of factors, including atmospheric composition, solar radiation, and ocean currents\n\n\n\nBenny Istanto\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistory of climate modeling\n\n\n\nResearch\n\nClimate\n\n\n\n\n\n\n\nBenny Istanto\n\n\nJan 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeat wave duration index\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nThe World Meteorological Organization (WMO), defines a heat wave as five or more consecutive days of prolonged heat in which the daily maximum temperature is higher than the‚Ä¶\n\n\n\nBenny Istanto\n\n\nAug 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing Thinkpad T480\n\n\n\nGeneral\n\n\n\nI never owned a Thinkpad before, but I used Thinkpad since 2008, T400 version from my office, and every 2-3 years got upgrade to the latest\n\n\n\nBenny Istanto\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch task execution in Google Earth Engine Code Editor\n\n\n\nRemote Sensing\n\n\n\nHave you ever had a problem when you wanted to download the entire data of an ImageCollection from Google Earth Engine (GEE), but you were lazy and bored when you had to‚Ä¶\n\n\n\nBenny Istanto\n\n\nMar 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMODIS LST Explorer\n\n\n\nRemote Sensing\n\nClimate\n\n\n\nRecently I worked on utilising daily MODIS LST (MOD11A1) to calculate number of day in a year with LST exceeding 35degC\n\n\n\nBenny Istanto\n\n\nFeb 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow‚Äôs 2021 rainfall in Indonesia?\n\n\n\nRemote Sensing\n\nClimate\n\n\n\nI frequently used the Global Precipitation Measurement (GPM) - next-generation of the Tropical Rainfall Measuring Mission (TRMM -¬†&lt;https://pmm\n\n\n\nBenny Istanto\n\n\nJan 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean annual temperature and number of hot days in a year\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nThis week I got request from my colleagues to calculate mean annual temperature and number of hot days in a year using MODIS Daily Land Surface Temperature (LST) data from‚Ä¶\n\n\n\nBenny Istanto\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerraClimate and Standardized Precipitation-Evapotranspiration Index (SPEI)\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nLast month, I have a chance to give a try TerraClimate data from Climatology Lab of University of California, Merced\n\n\n\nBenny Istanto\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch download 30-minutes rainfall data from multiple coordinates and dates\n\n\n\nRemote Sensing\n\nResearch\n\n\n\nThe need for the availability of 30-minutes rainfall time series data are very crucial, especially when we are working on the study of extreme rainfall induced landslide\n\n\n\nBenny Istanto\n\n\nSep 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpgrade the MBP MC724 Early 2011\n\n\n\nGeneral\n\n\n\n\n\n\n\nBenny Istanto\n\n\nAug 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nQGIS auto mapping\n\n\n\nGIS\n\n\n\nThis week I learned a new thing, create a layout and automatically export to PNG files from various datasets using QGIS\n\n\n\nBenny Istanto\n\n\nAug 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nFarewell WFP!\n\n\n\nGeneral\n\n\n\nAs you may have heard, I‚Äôm moving on to the World Bank HQ after 9 wonderful years at the UN WFP in Indonesia\n\n\n\nBenny Istanto\n\n\nAug 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize daily weather forecast from GFS using Google Earth Engine\n\n\n\nClimate\n\nRemote Sensing\n\nResearch\n\n\n\n\n\n\n\nBenny Istanto\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nNinth year and the 2020 Nobel Peace Prize Laureate\n\n\n\nGeneral\n\n\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù]\n\n\n\nBenny Istanto\n\n\nJun 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nHP CLJ Pro MFP M181fw supply error message\n\n\n\nGeneral\n\n\n\nI bought printer HP Color Laserjet Pro MFP M181fw and a set spare original HP 204A catridges not long after I started to work from home due to COVID-19 pandemic situation\n\n\n\nBenny Istanto\n\n\nMay 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get daily rainfall forecast data from GFS? (Part 2)\n\n\n\nClimate\n\nRemote Sensing\n\nResearch\n\n\n\nFew days ago I wrote a post on how to get GFS data using GRIB filter and NCAR/UCAR Research Data Archive\n\n\n\nBenny Istanto\n\n\nApr 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get daily rainfall forecast data from GFS? (Part 1)\n\n\n\nClimate\n\nRemote Sensing\n\nResearch\n\n\n\nLast week, Tropical Cyclone Seroja hit eastern part of Indonesia and Timor-Leste\n\n\n\nBenny Istanto\n\n\nApr 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nToday - a year ago\n\n\n\nGeneral\n\n\n\n\n\n\n\nBenny Istanto\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate SPI using monthly rainfall data in GeoTIFF format\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nNOTES - 1 Mar 2022 I have write a new and comprehensive guideline on SPI, you can access via this link &lt;https://bennyistanto\n\n\n\nBenny Istanto\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup your machine to be GIS ready!\n\n\n\nRemote Sensing\n\nGeneral\n\nGIS\n\nClimate\n\n\n\n\n\n\n\nBenny Istanto\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoTIFF to NetCDF file with time dimension enabled and CF-Compliant\n\n\n\nResearch\n\nClimate\n\n\n\nUPDATE: as of 4 January 2024 1\n\n\n\nBenny Istanto\n\n\nDec 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Reason For Being\n\n\n\nGeneral\n\n\n\nAccording to Wikipedia, Ikigai (Áîü„ÅçÁî≤Êñê) (pronounced [[iki…°ai]](https://en\n\n\n\nBenny Istanto\n\n\nNov 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Map Challenge, 2020\n\n\n\nRemote Sensing\n\nGIS\n\n\n\nFor the month of November, Topi Tjukanov announced a 30 Day Map Challenge on Twitter\n\n\n\nBenny Istanto\n\n\nNov 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa Ni√±a and Indonesia context\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nGeneral sensitivity of rainfall in Indonesia to sea surface temperature (SST) changes in NINO-4 region\n\n\n\nBenny Istanto\n\n\nOct 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nKriging and IDW interpolation in GEE\n\n\n\nRemote Sensing\n\nGIS\n\nClimate\n\n\n\nI have automatic weather station coordinates from BMKG, along with example data on precipitation accumulation for 1 - 10 Jan 2017 in csv format with column structure: Lon‚Ä¶\n\n\n\nBenny Istanto\n\n\nAug 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of dry and wet-spell\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nNow, it‚Äôs easy to get statistics information on AVERAGE, MINIMUM and MAXIMUM number of consecutive dry and wet days for a certain period using Google Earth Engine platform\n\n\n\nBenny Istanto\n\n\nJul 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate SPI using CHIRPS data\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nNOTES - 1 Mar 2022 I have write a new and comprehensive guideline on SPI, you can access via this link &lt;https://bennyistanto\n\n\n\nBenny Istanto\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate SPI using IMERG data\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nNOTES - 1 Mar 2022 I have write a new and comprehensive guideline on SPI, you can access via this link &lt;https://bennyistanto\n\n\n\nBenny Istanto\n\n\nJul 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonthly weather data from ERA5 and FLDAS\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nSince 2015, considerable amount of new data (high resolution satellite-based product combined with social media data) developed using new technology through artificial‚Ä¶\n\n\n\nBenny Istanto\n\n\nJun 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n30-minutes rainfall and landslide research\n\n\n\nRemote Sensing\n\nClimate\n\nGIS\n\nResearch\n\n\n\nJemblung Landslide - Banjarnegara 2014 Landslide occured on 12 Dec 2014 in Jemblung of Sampang village, Karangkobar sub-district, Banjarnegara district, Central Java (109\n\n\n\nBenny Istanto\n\n\nMay 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nVulnerable groups in COVID-19\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù]\n\n\n\nBenny Istanto\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical flood occurrence\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\nInspired from Dr\n\n\n\nBenny Istanto\n\n\nApr 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid-19 and WFH\n\n\n\nGeneral\n\n\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú958‚Äù]\n\n\n\nBenny Istanto\n\n\nMar 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n72-hours assessment workshop in Vanuatu\n\n\n\nGeneral\n\nTravel\n\n\n\nTrip to Port Vila - Vanuatu last week was my first trip to Pacific Islands\n\n\n\nBenny Istanto\n\n\nMar 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy harvesting and planting area 2019\n\n\n\nRemote Sensing\n\nResearch\n\n\n\nBPS (Statistics of Indonesia) release: paddy harvested area of 2019 was estimated around 10,68 million Ha\n\n\n\nBenny Istanto\n\n\nFeb 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nJakarta flood 2020\n\n\n\nRemote Sensing\n\nGIS\n\nClimate\n\n\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù]\n\n\n\nBenny Istanto\n\n\nJan 2, 2020\n\n\n\n\n\n\n\n\n\n\n\nDry and Start of Rainy Season in Timor-Leste 2019/2020\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\nClimate\n\n\n\nFor the last 6 weeks, I am working with my colleagues in Bangkok and Dili to produces climate monitoring bulletin related to dry season in the last 2019 and start of rainy‚Ä¶\n\n\n\nBenny Istanto\n\n\nDec 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit to DLR and WFP Innovation Accelerator\n\n\n\nResearch\n\nTravel\n\n\n\nShortly after finishing workshop in Rome, I flew to Munich to attend a workshop in 29 October 2019 with Earth Observation Centre at Deutsches Zentrum f√ºr Luft- und Raumfahrt‚Ä¶\n\n\n\nBenny Istanto\n\n\nNov 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal GIS Workshop 2019\n\n\n\nGIS\n\nTravel\n\n\n\nRight after following the innovation bootcamp in SF, I flew to Rome to attend WFPs Global GIS Workshop\n\n\n\nBenny Istanto\n\n\nNov 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nSF Innovation Bootcamp 2019\n\n\n\nResearch\n\nTravel\n\n\n\nLast month I had the opportunity to attend 2019 WFP Innovation Accelerator Bootcamp in San Francisco, CA, USA\n\n\n\nBenny Istanto\n\n\nNov 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation in humanitarian crisis meeting\n\n\n\nTravel\n\nGeneral\n\n\n\nLast week I went to Rome for a workshop and a task force meeting\n\n\n\nBenny Istanto\n\n\nSep 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnical engagement workshop in Beijing\n\n\n\nGeneral\n\nTravel\n\n\n\nLast week I wen to Beijing for a technical engagement workshop on emergency preparedness and response at National Disaster Reduction Centre of China (NDRCC) office\n\n\n\nBenny Istanto\n\n\nJul 27, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nPixel-wise regression between rainfall and sea surface temperature\n\n\n\nData Science\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nGeneral sensitivity of rainfall to sea surface temperature (sst) changes in NINO region Climate variability directly influences many aspects of food and nutrition security‚Ä¶\n\n\n\nBenny Istanto\n\n\nJul 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer in Mongolia\n\n\n\nGeneral\n\nTravel\n\n\n\nI visited Mongolia again last week during Summer season, it was nice\n\n\n\nBenny Istanto\n\n\nMay 31, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurface buffer\n\n\n\nGIS\n\n\n\nThe current state of a volcano is extremely variable with time and it has no meaning to define fixed risk zones around volcanoes (e\n\n\n\nBenny Istanto\n\n\nApr 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme winter in Mongolia\n\n\n\nGeneral\n\nTravel\n\n\n\nLast week, I had the opportunity to visit Mongolia\n\n\n\nBenny Istanto\n\n\nJan 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nDSSAT training in Chiang Mai, 2018\n\n\n\nTravel\n\nResearch\n\nClimate\n\n\n\nLast month, I attended training on DSSAT and MWCropDSS: Efficient and precision agricultural resource utilization under changes with simulation models and GIS in Chiang Mai‚Ä¶\n\n\n\nBenny Istanto\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nFood accessibility for Rohingya refugees\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\nFollowing my visit to Rohingya camps in Cox‚Äôs Bazaar - Bangladesh two-months ago, I try to apply accessibility modeling to help understand on how well covered is Kutupalong‚Ä¶\n\n\n\nBenny Istanto\n\n\nJun 6, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisiting Rohingya refugees in Cox‚Äôs Bazaar\n\n\n\nTravel\n\nGeneral\n\n\n\nTwo-weeks ago I visited Rohingya refugees camp in Cox‚Äôs Bazaar, Bangladesh to conduct the feasibility study of 72hrs approach related to the exposure of Rohingya refugee‚Ä¶\n\n\n\nBenny Istanto\n\n\nApr 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelegram group for GIS user in Indonesia\n\n\n\nRemote Sensing\n\nGIS\n\n\n\n\n\n\n\nBenny Istanto\n\n\nApr 10, 2018\n\n\n\n\n\n\n\n\n\n\n\nClimate expertise in the humanitarian field\n\n\n\nGeneral\n\nClimate\n\n\n\nLast month I was invited by my alma mater to an academic workshop which aims to (i) evaluate the curriculum for the period (2013-2017), (ii) develop a curriculum that is‚Ä¶\n\n\n\nBenny Istanto\n\n\nApr 2, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAMPIRE won 2017 Innovation Challenge\n\n\n\nGeneral\n\n\n\nToday we celebrate the VAMPIRE project, won the 2017 WFPs Innovation Challenge\n\n\n\nBenny Istanto\n\n\nFeb 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap Mapathon in Atambua\n\n\n\nGeneral\n\n\n\nMy office frequently conduct an OpenStreetMap (OSM) Mapathon series, to improve the quality and speed of disaster preparedness, response and recovery in Indonesia through‚Ä¶\n\n\n\nBenny Istanto\n\n\nDec 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap GeoWeek 2017\n\n\n\nGeneral\n\n\n\nNovember 12‚Äì18 is OSM Geography Awareness Week\n\n\n\nBenny Istanto\n\n\nNov 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlending Satellite Precipitation and Gauge Observations\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nBlending of point and grid data I have some station data\n\n\n\nBenny Istanto\n\n\nAug 2, 2017\n\n\n\n\n\n\n\n\n\n\n\nList of free satellite-based products and geospatial data on internet\n\n\n\nRemote Sensing\n\nGIS\n\n\n\nThis post updated regularly, so if you couldn‚Äôt find what you are looking for, comeback later or send me an email\n\n\n\nBenny Istanto\n\n\nMay 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\nStanding crowd density, 2 Desember 2016\n\n\n\nGIS\n\n\n\nLagi rame tentang angka 7 juta :) Masih terkait dengan metode perhitungan yang cocok untuk standing crowd density pada 2 Desember di Monas dan sekitarnya, saya mencoba‚Ä¶\n\n\n\nBenny Istanto\n\n\nDec 5, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatellite-based monitoring of growing season\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\nClimate and vegetation data sourced from remote sensing satellites are widely used for agricultural monitoring\n\n\n\nBenny Istanto\n\n\nSep 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow long will I live?\n\n\n\nData Science\n\n\n\nWhat‚Äôs my place in the world population\n\n\n\nBenny Istanto\n\n\nAug 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016 GIS for Sustainable World Conference\n\n\n\nGIS\n\nTravel\n\n\n\nEarly this month I had the opportunity to attend the¬†2016 GIS for a Sustainable World Conference held on May 2‚Äì4, 2016 in Campus Biotech, Geneva, Switzerland\n\n\n\nBenny Istanto\n\n\nMay 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo fun Aerial Bold!\n\n\n\nRemote Sensing\n\nGeneral\n\n\n\nHave you heard about Aerial Bold\n\n\n\nBenny Istanto\n\n\nApr 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015 ESRI User Conference\n\n\n\nGIS\n\nTravel\n\n\n\nSan Diego Convention Center 20 ‚Äì 24 July 2015, San Diego, CA, USA\n\n\n\nBenny Istanto\n\n\nAug 1, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\nOSM use case: accessibility mapping\n\n\n\nGIS\n\nResearch\n\n\n\nOpenStreetMap (OSM) is a collaborative project to create a free, editable map of the world\n\n\n\nBenny Istanto\n\n\nDec 16, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\nKelas Inspirasi Bogor 2\n\n\n\nGeneral\n\n\n\nLast week, on 9 Sep 2014 I participated in Kelas Inspirasi Bogor - part of Indonesia Mengajar activities\n\n\n\nBenny Istanto\n\n\nSep 15, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\nMenginstall ‚ÄúGDAL/OGR for Python‚Äù di Windows\n\n\n\nGIS\n\n\n\nBeberapa bulan terakhir saya banyak menggunakan GDAL/OGR untuk melakukan clip dan translate data satelit\n\n\n\nBenny Istanto\n\n\nSep 11, 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\nCloudless satellite image\n\n\n\nRemote Sensing\n\n\n\nI found excellent Image-compositing scripts for filtering weather out of satellite images &lt;https://github\n\n\n\nBenny Istanto\n\n\nApr 15, 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\nMBTiles map\n\n\n\nGIS\n\n\n\nAfter few months playing around with TileMill and TileStream, I have created few MBTiles map using data from my previous and current project\n\n\n\nBenny Istanto\n\n\nApr 2, 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\nMenggunakan fungsi ArcPy melalui Python non-ArcGIS\n\n\n\nGIS\n\n\n\nArcGIS menginstal versi Python-nya sendiri yang tidak terdaftar secara resmi di sistem Windows tetapi pada kenyataannya dapat digunakan seperti layaknya Python melalui‚Ä¶\n\n\n\nBenny Istanto\n\n\nJun 12, 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst day at WFP\n\n\n\nGeneral\n\n\n\nToday is my first day working at World Food Programme, Indonesia Country Office and based in Jakarta\n\n\n\nBenny Istanto\n\n\nJun 11, 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\nBanjir Tangse di akhir Februari 2012\n\n\n\nGeneral\n\n\n\nTangse adalah sebuah kecamatan di Kabupaten Pidie, Aceh, Indonesia\n\n\n\nBenny Istanto\n\n\nMar 4, 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst try using TileMill and TileStream\n\n\n\nGIS\n\n\n\nSince it was announced through Development Seed‚Äôs blog, I am really excited to try TileMill and TileStream\n\n\n\nBenny Istanto\n\n\nAug 10, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nILO Basic Operational GIS for Road Assessment (Part 3)\n\n\n\nGIS\n\n\n\nThis the last part of 3 series post on ILO Basic Operational GIS for Road Assessment\n\n\n\nBenny Istanto\n\n\nJun 1, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nILO Basic Operational GIS for Road Assessment (Part 2)\n\n\n\nGIS\n\n\n\nThis the second part of 3 series post on ILO Basic Operational GIS for Road Assessment\n\n\n\nBenny Istanto\n\n\nMay 1, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nILO Basic Operational GIS for Road Assessment (Part 1)\n\n\n\nGIS\n\n\n\nThis the first part of 3 series post on ILO Basic Operational GIS for Road Assessment\n\n\n\nBenny Istanto\n\n\nApr 28, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nBack to Aceh (again)\n\n\n\nGIS\n\n\n\nStarting from 1 February 2011, I am back to Aceh again\n\n\n\nBenny Istanto\n\n\nFeb 10, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google to disseminate information during 2010 Tsunami in Mentawai Islands\n\n\n\nGeneral\n\n\n\nInformation Management in Emergency Situation ¬†During post-disaster emergency response, the role of information management is highly important\n\n\n\nBenny Istanto\n\n\nDec 15, 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010 Mentawai Tsunami and Earthquake\n\n\n\nGIS\n\n\n\n25 October 2010 - 21:42, that night I was in my room (Agus Salim stadium complex) working on map requested by Ignacio Leon - Head of OCHA Indonesia\n\n\n\nBenny Istanto\n\n\nNov 1, 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\nWest Sumatra Earthquake Response 2009\n\n\n\nGIS\n\n\n\nAlmost 5-months I have been working for UNOCHA as GIS Officer to support earthquake response that happen in West Sumatra, 30 Sep 2009\n\n\n\nBenny Istanto\n\n\nApr 20, 2010\n\n\n\n\n\n\n\n\n\n\n\nWeb Map Interface\n\n\n\nGIS\n\n\n\nThis training was part of ADB‚Äôs ETESP Package 39 Part 2 project and was carried out last month, which was attended by staff from Aceh‚Äôs Forestry and Plantation Office\n\n\n\nBenny Istanto\n\n\nNov 21, 2009\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandmark survey in TLS\n\n\n\nGIS\n\n\n\nSince May 2009 I am working for UNFPA as Census Mapping Supervisor in Timor-Leste supporting Direksaun Jeral de Estatistika (DNE) conducting landmark survey as preparation‚Ä¶\n\n\n\nBenny Istanto\n\n\nOct 5, 2009\n\n\n\n\n\n\n\n\n\n\n\n\n\nPotential soil loss\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\nDuring my assignment with ADB project called Earthquake and Tsunami Emergency Support Project (ETESP) - Package 38, I am working on development of potential soil loss map\n\n\n\nBenny Istanto\n\n\nDec 25, 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy‚Äôs growth and development model, and economic value of farming system\n\n\n\nGIS\n\nResearch\n\nClimate\n\n\n\nAgriculture is one of the important sectors in Indonesia so that the various efforts related to development in agriculture continues to be done, especially paddy\n\n\n\nBenny Istanto\n\n\nNov 30, 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel simulasi penyakit busuk daun tanaman kentang\n\n\n\nResearch\n\nClimate\n\n\n\nUser interface model simulasi penyakit busuk daun (late blight) pada tanaman kentang (Solanum tuberosum L\n\n\n\nBenny Istanto\n\n\nNov 9, 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\nGIS for immunization activities\n\n\n\nGIS\n\n\n\nIn the last 4-months I have been working for USAID funded project as a GIS Consultant on Immunization for 64 districts in 7 provinces in Indonesia\n\n\n\nBenny Istanto\n\n\nDec 15, 2007\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Coordinate Register System\n\n\n\nRemote Sensing\n\n\n\nImage registration is the process of transforming different sets of data into one coordinate system\n\n\n\nBenny Istanto\n\n\nFeb 10, 2007\n\n\n\n\n\n\n\n\n\n\n\n\n\nJacub Rais Award\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\n\n\nMinggu lalu saya mengikuti Pertemuan Ilmiah Tahunan XV dan Kongres IV Masyarakat Ahli Penginderaan Jauh Indonesia (MAPIN), yang diadakan di Bandung, 13 - 14 Desember 2006\n\n\n\nBenny Istanto\n\n\nDec 21, 2006\n\n\n\n\n\n\n\n\n\n\n\n\n\nSistem Peringkat Bahaya Kebakaran\n\n\n\nRemote Sensing\n\nGIS\n\nResearch\n\nClimate\n\n\n\nLAPAN secara rutin melakukan pemantauan Sistem Peringkat Bahaya Kebakaran (SPBK/FDRS - Fire Danger Rating System) berbasis data satelit\n\n\n\nBenny Istanto\n\n\nNov 9, 2006\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical random search\n\n\n\nResearch\n\nClimate\n\nData Science\n\n\n\n\n\n\n\nBenny Istanto\n\n\nAug 12, 2006\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Person Decision Making\n\n\n\nResearch\n\nClimate\n\nData Science\n\n\n\nProblem\n\n\n\nBenny Istanto\n\n\nJul 23, 2006\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to calculate the Fire Weather Index in Indonesia\n\n\n\nResearch\n\nClimate\n\n\n\nFWI Excel program add-in is a quick and easy way to calculate the fire weather index into a table in Microsoft Excel, hereinafter known as XLFWI add-ins\n\n\n\nBenny Istanto\n\n\nJul 20, 2006\n\n\n\n\n\n\n\n\n\n\n\nSarjana Meteorologi\n\n\n\nGeneral\n\n\n\nHari ini saya dinyatakan lulus program sarjana Meteorologi - FMIPA IPB, setelah tadi siang saya menerima Surat Keterangan Lulus (SKL)\n\n\n\nBenny Istanto\n\n\nMar 10, 2006\n\n\n\n\n\n\n\n\n\n\n\n\n\nMagang di TISDA, BPPT\n\n\n\nGIS\n\nResearch\n\nClimate\n\n\n\nPada tanggal 27 Juni 2004, saya memulai magang di Laboratorium Teknologi Geosystem, Pusat Pengkajian dan Penerapan Teknologi Inventarisasi Sumber Daya Alam (P3-TISDA), BPPT‚Ä¶\n\n\n\nBenny Istanto\n\n\nJun 10, 2005\n\n\n\n\n\n\n\n\n\n\n\n\n\nTermodinamika dalam pertanian rumahkaca\n\n\n\nResearch\n\nClimate\n\n\n\nMakalah berikut merupakan tugas dari mata kuliah Termodinamika di semester 4, dibimbing oleh Hanedi Darmasetiawan\n\n\n\nBenny Istanto\n\n\nMar 27, 2005\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatelit Geostasioner\n\n\n\nRemote Sensing\n\nResearch\n\nClimate\n\n\n\nMengamati bumi melalui satelit Geostasioner (GOES EAST, METEOSAT, IODC, GMS dan GOES WEST) tanggal 25 Oktober 2004, Jam 18\n\n\n\nBenny Istanto\n\n\nJan 8, 2005\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel biomassa dan neraca air\n\n\n\nResearch\n\nClimate\n\n\n\nModel ini merupakan tugas akhir mata kuliah Model Simulasi Pertanian di Semester 6\n\n\n\nBenny Istanto\n\n\nJul 8, 2004\n\n\n\n\n\n\n\n\n\n\n\n\n\nPendugaan deret hari kering\n\n\n\nResearch\n\nClimate\n\n\n\nMakalah berikut merupakan tugas dari mata kuliah Klimatologi Pertanian di semester 6, dibimbing oleh Rizaldi Boer\n\n\n\nBenny Istanto\n\n\nMay 22, 2004\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel pendugaan biomassa tanaman padi\n\n\n\nResearch\n\nClimate\n\n\n\nIni adalah salah satu tugas praktikum dari mata kuliah Model Simulasi Pertanian yang diberikan oleh Pak Handoko dan Pak Yon Sugiarto\n\n\n\nBenny Istanto\n\n\nApr 27, 2004\n\n\n\n\n\n\n\n\n\n\n\n\n\nMenghitung radiasi matahari\n\n\n\nResearch\n\nClimate\n\n\n\nMata kuliah Mikrometeorologi membuat saya dan teman-teman melakukan pengamatan cuaca selama 24 jam di halaman Kampus IPB Baranangsiang, sangat menyenangkan karena waktu‚Ä¶\n\n\n\nBenny Istanto\n\n\nJan 3, 2004\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnak UMPTN memang beda!\n\n\n\nGeneral\n\n\n\nDi kampus lagi sering ledek-ledakan antara mahasiswa yang masuk IPB lewat jalur USMI dan UMPTN\n\n\n\nBenny Istanto\n\n\nDec 10, 2003\n\n\n\n\n\n\n\n\n\n\n\n\n\nMekanika - gerak dalam dua dimensi\n\n\n\nResearch\n\nClimate\n\n\n\nMakalah berikut merupakan tugas dari mata kuliah Mekanika di semester 4, dibimbing oleh Hanedi Darmasetiawan\n\n\n\nBenny Istanto\n\n\nNov 25, 2003\n\n\n\n\n\n\n\n\n\n\n\n\n\nMakalah Dasar Agronomi\n\n\n\nResearch\n\nClimate\n\n\n\nMakalah berikut merupakan tugas dari mata kuliah Dasar Agronomi di semester 3\n\n\n\nBenny Istanto\n\n\nNov 20, 2003\n\n\n\n\n\n\n\n\n\n\n\n\n\nPengukuran Suhu\n\n\n\nClimate\n\nResearch\n\n\n\nMakalah berikut merupakan tugas dari mata kuliah Metode Observasi dan Instrumentasi Cuaca di semester 3\n\n\n\nBenny Istanto\n\n\nOct 28, 2003\n\n\n\n\n\n\n\n\n\n\n\n\n\nMenduga distribusi ukuran butir hujan\n\n\n\nClimate\n\nData Science\n\n\n\nMinggu kemarin kita melakukan praktikum mata kuliah Hidrometeorologi tentang pendugaan distribusi ukuran butir hujan pada berbagai kondisi atmosfer, awal atau akhir hujan\n\n\n\nBenny Istanto\n\n\nOct 16, 2003\n\n\n\n\n\n\n\n\n\n\n\n\n\nHalo!\n\n\n\nGeneral\n\n\n\nHari ini saya memulai sebuah blog, dan ini tulisan pertama saya\n\n\n\nBenny Istanto\n\n\nOct 8, 2003\n\n\n\n\n\n\nNo matching items\n\n  \n\n Back to top"
  },
  {
    "objectID": "index.html#spotlight.",
    "href": "index.html#spotlight.",
    "title": "Hi, I am Benny",
    "section": "Spotlight.",
    "text": "Spotlight.\nMy name is visualized through a collection of Landsat satellite imagery, where each letter represents distinct Earth features captured from space. This artistic composition combines various natural patterns, including rivers, lakes, mountains, and landscapes photographed by the Landsat program - NASA and USGS‚Äôs 50+ year Earth observation mission. From meandering rivers to desert patterns, each letter showcases the diverse and beautiful textures of our planet‚Äôs surface as seen from orbit and created using NASA‚Äôs ‚ÄòYour Name in Landsat‚Äô interactive tool - https://science.nasa.gov/specials/your-name-in-landsat/, which transforms Earth observation data into a personalized geographic alphabet."
  },
  {
    "objectID": "works/projects.html",
    "href": "works/projects.html",
    "title": "Projects",
    "section": "",
    "text": "It all begins with an idea, then it becomes something else.\nFeatured projects demonstrating expertise in earth observation, climate analysis, and data science."
  },
  {
    "objectID": "works/projects.html#selected-projects",
    "href": "works/projects.html#selected-projects",
    "title": "Projects",
    "section": "Selected Projects",
    "text": "Selected Projects\n\n\n\n  \n  \n    2003\n    Festival Film Independen Indonesia\n  \n\n\n\n  \n  \n    2004 - now\n    Logo\n  \n\n\n\n  \n  \n    2005\n    Surface Energy Balance\n  \n\n\n\n  \n  \n    2006\n    GIS for Forest Fire\n  \n\n\n\n  \n  \n    2007\n    Sistem Informasi Spasial Kebakaran Hutan dan Lahan\n  \n\n\n\n  \n  \n    2008\n    F/OSS for Forest Fire Monitoring\n  \n\n\n\n  \n  \n    2008\n    Aceh Forest Information System\n  \n\n\n\n  \n    2009\n    Cluster Atlas: West Sumatra Earthquake Response\n  \n\n\n\n  \n    2011\n    Aceh District Road Network\n  \n\n\n\n  \n    2014\n    Province Infographic\n  \n\n\n\n  \n    2015\n    Food Security and Vulnerability Atlas\n  \n\n\n\n  \n    2016\n    VAMPIRE\n  \n\n\n\n  \n    2021\n    Extreme Rainfall Monitoring\n  \n\n\n\n\n‚Üê Back to Works"
  },
  {
    "objectID": "works/experiences.html",
    "href": "works/experiences.html",
    "title": "Experiences",
    "section": "",
    "text": "With limited skills I have, so far I‚Äôve been working on information management and coordination for humanitarian activities, GIS and remote sensing, crop simulation models, agriculture meteorology and climate risk analytics.\nFind out some information about the projects that I‚Äôve been involved with‚Ä¶"
  },
  {
    "objectID": "works/experiences.html#international-development-agencies",
    "href": "works/experiences.html#international-development-agencies",
    "title": "Experiences",
    "section": "International Development Agencies",
    "text": "International Development Agencies"
  },
  {
    "objectID": "works/experiences.html#government-of-indonesia",
    "href": "works/experiences.html#government-of-indonesia",
    "title": "Experiences",
    "section": "Government of Indonesia",
    "text": "Government of Indonesia"
  },
  {
    "objectID": "works/projects/2021-extreme-rainfall-monitoring.html",
    "href": "works/projects/2021-extreme-rainfall-monitoring.html",
    "title": "Extreme Rainfall Monitoring",
    "section": "",
    "text": "Year\n\n\n2021"
  },
  {
    "objectID": "works/projects/2021-extreme-rainfall-monitoring.html#overview",
    "href": "works/projects/2021-extreme-rainfall-monitoring.html#overview",
    "title": "Extreme Rainfall Monitoring",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2016-vampire.html",
    "href": "works/projects/2016-vampire.html",
    "title": "VAMPIRE",
    "section": "",
    "text": "Year\n\n\n2016"
  },
  {
    "objectID": "works/projects/2016-vampire.html#overview",
    "href": "works/projects/2016-vampire.html#overview",
    "title": "VAMPIRE",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2006-gisforestfire.html",
    "href": "works/projects/2006-gisforestfire.html",
    "title": "GISForestFire",
    "section": "",
    "text": "This software is used to identify fire danger level in Sumatra - Indonesia. The hazard level is determined using the Canadian Forest Fire Weather Index System, which is generated through analysis of remote sensing data.\nGISForestFire is part of my final research and as one of the requirements to get bachelor of science degree at Department of Geophysics and Meteorology, Faculty of Mathematics and Natural Science, Bogor Agricultural University."
  },
  {
    "objectID": "works/projects/2006-gisforestfire.html#project-background",
    "href": "works/projects/2006-gisforestfire.html#project-background",
    "title": "GISForestFire",
    "section": "Project Background",
    "text": "Project Background\nForest fire is one of the main types of hazards in Indonesia. This study was aimed to develop an application system to determine the Canadian Forest Fire Weather Index System (FWI) from various combinations of remote sensing satellite data based on numerical and spatial models.\nThis application is expected to provide forest fire danger rating information for a certain area based on numerical-spatial FWI value at a certain time. FWI was calculated from air temperature (Ta), relative humidity (RH), wind speed (u), and rainfall (Ra). These were derived from satellite images (remote sensing data)."
  },
  {
    "objectID": "works/projects/2006-gisforestfire.html#methodology",
    "href": "works/projects/2006-gisforestfire.html#methodology",
    "title": "GISForestFire",
    "section": "Methodology",
    "text": "Methodology\nThree main processes were involved:\n\nProgram Development - Designing a program to prepare the input for the software\nAlgorithm Development - Script development of the software\nEarly Warning System - Development of the software for forest fire early warning\n\nThe designed program stimulated weather conditions based on surface temperature data (Ts) that was extracted from AVHRR/NOAA to derive Ta and RH. Wind data was generated using thermal wind equation which relates vertical shear of the geostrophic wind to horizontal temperature gradient, then corrected using extracted wind data from TXLAPS. Rainfall data was extracted from QMORPH (CPC MORPHing Technique)."
  },
  {
    "objectID": "works/projects/2006-gisforestfire.html#software-interface-results",
    "href": "works/projects/2006-gisforestfire.html#software-interface-results",
    "title": "GISForestFire",
    "section": "Software Interface & Results",
    "text": "Software Interface & Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/projects/2011-district-road-network.html",
    "href": "works/projects/2011-district-road-network.html",
    "title": "District Road Network",
    "section": "",
    "text": "Year\n\n\n2011"
  },
  {
    "objectID": "works/projects/2011-district-road-network.html#overview",
    "href": "works/projects/2011-district-road-network.html#overview",
    "title": "District Road Network",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2004-now-logo.html",
    "href": "works/projects/2004-now-logo.html",
    "title": "Logo Design",
    "section": "",
    "text": "My friends and colleagues at work often asked me for help designing logos. Below are a few examples of branding and visual identity work I‚Äôve created over the years."
  },
  {
    "objectID": "works/projects/2004-now-logo.html#logo-portfolio",
    "href": "works/projects/2004-now-logo.html#logo-portfolio",
    "title": "Logo Design",
    "section": "Logo Portfolio",
    "text": "Logo Portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "",
    "text": "Research and development of remote sensing applications for fire monitoring using Free and Open Source Software for Geospatial (FOSS4G) at the EU-funded South Sumatra Forest Fire Management Project (SSFFMP) in 2008."
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html#project-objectives",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html#project-objectives",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "Project Objectives",
    "text": "Project Objectives\nThe Terms of Reference specified comprehensive evaluation and implementation of open-source geospatial software:\n\nIdentify potential open-source applications for fire information systems\nInvestigate and explore capabilities of identified applications\nDevelop manuals for fire monitoring and land cover classification\nCompare strengths and weaknesses with proprietary solutions\nTrain counterparts in application usage"
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html#software-evaluation",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html#software-evaluation",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "Software Evaluation",
    "text": "Software Evaluation\nDesktop GIS open-source applications evaluated for fire information system capabilities:\nAssessed Applications: GRASS, Quantum GIS, uDig, gvSIG, SAGA, JUMP Family, ILWIS, KOSMO, MapWindow GIS\nImplementation Criteria: 1. Operating System Support - UNIX, Linux, Windows, MacOS compatibility 2. Performance - Speed and efficiency 3. Data Format Support - Comprehensive format compatibility 4. Interoperability - OGC Specifications compliance"
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html#selected-solutions",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html#selected-solutions",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "Selected Solutions",
    "text": "Selected Solutions\nKOSMO - Selected for daily hotspot monitoring (vector data model applications)\nILWIS - Selected for fire-prone area mapping (raster data model applications)\nArcExplorer Java Edition for Education (AEJEE) - Free GIS software (not open-source) used for supplementary tasks\nThe selection balanced functional requirements for daily monitoring operations and fire risk assessment with the practical considerations of software accessibility and intellectual property compliance."
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html#outcomes",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html#outcomes",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "Outcomes",
    "text": "Outcomes\nThe project demonstrated that open-source GIS software provides sufficient capability for daily hotspot monitoring and fire-prone area analysis. The research emphasized the importance of:\n\nPromoting open-source applications to address software piracy concerns\nReducing barriers to legal software usage\nEnhancing human resource capacity in FOSS4G technologies\nSupporting sustainable fire information system development"
  },
  {
    "objectID": "works/projects/2008-floss-for-forest-fire-monitoring.html#software-interface-results",
    "href": "works/projects/2008-floss-for-forest-fire-monitoring.html#software-interface-results",
    "title": "F/OSS for Forest Fire Monitoring",
    "section": "Software Interface & Results",
    "text": "Software Interface & Results\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/consulting.html",
    "href": "works/consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "I provide specialized consultancy bridging climate science and geospatial technology. I help organizations and individuals harness GIS, remote sensing, and information management for precision agriculture, natural resource management, and atmospheric resilience."
  },
  {
    "objectID": "works/consulting.html#ready-to-get-started",
    "href": "works/consulting.html#ready-to-get-started",
    "title": "Consulting",
    "section": "Ready to Get Started?",
    "text": "Ready to Get Started?\nSchedule a consultation or reach out to discuss your project needs.\n\nüìÖ Schedule Appointment\n‚úâÔ∏è Send Email"
  },
  {
    "objectID": "works/projects/2015-fsva.html",
    "href": "works/projects/2015-fsva.html",
    "title": "FSVA",
    "section": "",
    "text": "Year\n\n\n2015"
  },
  {
    "objectID": "works/projects/2015-fsva.html#overview",
    "href": "works/projects/2015-fsva.html#overview",
    "title": "FSVA",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2014-province-infographic.html",
    "href": "works/projects/2014-province-infographic.html",
    "title": "Province Infographic",
    "section": "",
    "text": "Year\n\n\n2014"
  },
  {
    "objectID": "works/projects/2014-province-infographic.html#overview",
    "href": "works/projects/2014-province-infographic.html#overview",
    "title": "Province Infographic",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2007-siske.html",
    "href": "works/projects/2007-siske.html",
    "title": "SISKe",
    "section": "",
    "text": "New software for hotspot and forest fire monitoring was developed to facilitate Forest Information System (FIS) operators in their regular tasks. SISKe is a robust yet user-friendly application for fire monitoring and early warning systems, developed by the South Sumatra Forest Fire Management Project (SSFFMP) in 2006-2007, and freely available not only for stakeholders in South Sumatra but also at the national level."
  },
  {
    "objectID": "works/projects/2007-siske.html#key-features",
    "href": "works/projects/2007-siske.html#key-features",
    "title": "SISKe",
    "section": "Key Features",
    "text": "Key Features\nDistinguished from other forest fire software with Bahasa Indonesia as the main language interface, SISKe offers several unique capabilities:\n\n1. Weather Data Management\nComprehensive management of meteorological data (rainfall, air temperature, relative humidity, wind speed and direction) from multiple sources including BMKG, Manggala Agni, and plantation company weather stations. Features include simple queries, statistical information, and export functionality to Microsoft Excel.\n\n\n2. Forest Fire Early Warning System\nIntegrated with the weather data management module to calculate fire hazard levels at specific locations:\n\nFire Weather Index (FWI) - Canadian forest fire danger rating system\nKeetch-Byram Drought Index (KBDI) - Soil moisture deficit index\n\n\n\n3. Forest Fire Monitoring and Analysis\nHotspot Frequency Analysis\nIntersection features with administrative boundaries or land cover to generate hotspot summaries by region or land cover type, with monitoring capabilities by day, month, or custom periods.\nFire-prone Areas Analysis - Weighted Overlay\nInspired by ESRI‚Äôs weighted overlay methodology, custom algorithms enable users to perform multi-criteria analysis using existing SISKe database layers or their own datasets through the embedded translation tools.\nFire-prone Areas Analysis - Fire Danger Rating System\nConnected with the weather data management module and featuring advanced IDW interpolation, users can calculate Fire Weather Index across entire regions.\n\n\n4. Layout and Printing\nStreamlined layout design and print preview module for report generation."
  },
  {
    "objectID": "works/projects/2007-siske.html#software-interface-results",
    "href": "works/projects/2007-siske.html#software-interface-results",
    "title": "SISKe",
    "section": "Software Interface & Results",
    "text": "Software Interface & Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/projects/2003-ffii.html",
    "href": "works/projects/2003-ffii.html",
    "title": "Festival Film Independen Indonesia",
    "section": "",
    "text": "I was in a team consisted of second year students from ITB, ITENAS, IKJ and IPB, produced the movie in 2003 and we participated the Festival Film Independen Indonesia hosted by SCTV.\nAnd involved in the production of this following movie, mainly in property and set construction but also as one of the casts (Si Amrik).\n\n\n\nProducer & Director\n\n\nd‚ÄôJALMA 5 and Hendro Poerwandito\n\n\n\n\nLocation\n\n\nJakarta - Bogor - Sukabumi, Indonesia\n\n\n\n\nFestival\n\n\nFestival Film Independen Indonesia - SCTV\n\n\n\n\nProduction Year\n\n\n2003\n\n\n\n\nTeam\n\n\nHendro Poerwandito, Fery Widiantoro, Farid Hamdan, Dhian Wijanarko, Heri Sucahyono, Benny Istanto and Niken Eka Priarnani\n\n\n\n\n\n\nMovie cover of ‚ÄúMerah Putih Dipundakku\n\n\n\n\nWe‚Äôre talking about idealism dude! It‚Äôs all about fighting non-stop while worrying about getting food in your belly!\n\n\n‚Üê Back to Projects\n\n\n\n\n Back to top"
  },
  {
    "objectID": "works/projects/2009-cluster-atlas.html",
    "href": "works/projects/2009-cluster-atlas.html",
    "title": "Cluster Atlas",
    "section": "",
    "text": "Year\n\n\n2009"
  },
  {
    "objectID": "works/projects/2009-cluster-atlas.html#overview",
    "href": "works/projects/2009-cluster-atlas.html#overview",
    "title": "Cluster Atlas",
    "section": "Overview",
    "text": "Overview\nBrief description of the project goes here.\n\n‚Üê Back to Projects\n\nThis is a placeholder page. Content will be added soon."
  },
  {
    "objectID": "works/projects/2005-surface-energy-balance.html",
    "href": "works/projects/2005-surface-energy-balance.html",
    "title": "Surface Energy Balance",
    "section": "",
    "text": "I was involved in research activities at Division of Climatology and Environment Application (BAKL), Center for Atmospheric and Climate Sciences - LAPAN in 2005, on the phenomenon of urban heat islands.\nUsing Landsat satellite data, several surface energy balance parameters are extracted to obtain information related to changing in land use which affecting surface temperatures increase in urban and rural areas."
  },
  {
    "objectID": "works/projects/2005-surface-energy-balance.html#research-overview",
    "href": "works/projects/2005-surface-energy-balance.html#research-overview",
    "title": "Surface Energy Balance",
    "section": "Research Overview",
    "text": "Research Overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/projects/2008-aceh-fis.html",
    "href": "works/projects/2008-aceh-fis.html",
    "title": "Aceh Forest Information System",
    "section": "",
    "text": "The Asian Development Bank‚Äôs Earthquake and Tsunami Emergency Support Project (ETESP) packages 38 and 39-P2 developed a comprehensive Forestry Information System (FIS) with detailed user guides for the Agency for Forestry and Plantations (DisHutBun - Aceh). The system integrated multiple Forest Management Modules (FMM) addressing high-priority forest management processes during 2008-2010."
  },
  {
    "objectID": "works/projects/2008-aceh-fis.html#system-components",
    "href": "works/projects/2008-aceh-fis.html#system-components",
    "title": "Aceh Forest Information System",
    "section": "System Components",
    "text": "System Components\nThe FIS components support comprehensive forest management:\n\nQuantification and Classification of the Aceh Forest Resource Base\nLand Suitability Assessment for Plantation Forests and Tree Crops\nEnvironmental Sensitivity Mapping\nForest Fire Risk Management for Protection, Land Rehabilitation, and Social Forestry\nRemote Sensing Applications at the Watershed Level\nTechnical and Graphical Information Products for modular, scalable FIS"
  },
  {
    "objectID": "works/projects/2008-aceh-fis.html#user-guides",
    "href": "works/projects/2008-aceh-fis.html#user-guides",
    "title": "Aceh Forest Information System",
    "section": "User Guides",
    "text": "User Guides\nComprehensive documentation categorizes user guides by management structure, describing representative uses, essential outputs, and training requirements:\n\nArcCatalog and Metadata\nArcGIS Server\nCoordinate Conversion\nDegraded Land\nEnvironment Sensitivity\nForest Disturbance\nForest Fire Risk\nInstallation and Configuration\nLand Availability for Forest Production\nLand Cover\nLand Suitability\nNon-Spatial Database\nTimber Volume Estimated"
  },
  {
    "objectID": "works/projects/2008-aceh-fis.html#potential-soil-loss-model",
    "href": "works/projects/2008-aceh-fis.html#potential-soil-loss-model",
    "title": "Aceh Forest Information System",
    "section": "Potential Soil Loss Model",
    "text": "Potential Soil Loss Model\nAs GIS Modeler, I developed the potential soil loss model based on the RUSLE (Revised Universal Soil Loss Equation) methodology.\n\n\n\n\n\n\nRUSLE Model Implementation\nThe potential soil loss model is based on RUSLE (Revised Universal Soil Loss Equation), which accounts for erosional factors without considering sediment deposition. Annual erosion is calculated by combining spatially explicit raster maps:\nE = R √ó K √ó LS √ó C √ó P\nWhere: - E - Annual Erosion - R - Rainfall Erosivity Factor - K - Soil Erodibility Factor - LS - Slope Factor - C - Vegetative Cover Factor - P - Management Factor\n\n\nData Sources and Methodology\nSlope Factor (LS): Calculated from SRTM30 Digital Elevation Model\nVegetative Cover (C): Derived from Landsat land cover dataset classified according to IGBP (International Geosphere-Biosphere Programme) scheme\nSoil Erodibility (K): Calculated from USDA global soil sub-order dataset (NRCS), incorporating: - Organic matter content - Percentage of silt and fine sand - Percentage of sand - Soil texture and permeability\nRainfall Erosivity (R): Calculated from interpolated daily global rainfall data (CMORPH - NOAA) using ILWIS application methodology\n\n\nModel Specifications\n\nOutput Scale: 1:250,000\nNote: Higher resolution input data would improve calculation accuracy of potential soil loss\n\n\n‚Üê Back to Projects"
  },
  {
    "objectID": "works/maps-and-infographics.html",
    "href": "works/maps-and-infographics.html",
    "title": "Maps & Infographics",
    "section": "",
    "text": "I am having fun playing with geospatial and remote sensing data. Have a look at selected maps/infographics that I have ever made!"
  },
  {
    "objectID": "works/maps-and-infographics.html#section",
    "href": "works/maps-and-infographics.html#section",
    "title": "Maps & Infographics",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n2024. The World Bank. Biodiversity Evaluation. QGIS 3.22.9\n\n\n\n\n\n\n\n2024. The World Bank. GEEST Level of Enablement Women. QGIS 3.22.9"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-1",
    "href": "works/maps-and-infographics.html#section-1",
    "title": "Maps & Infographics",
    "section": "2023",
    "text": "2023\n\n\n\n\n\n2023. The World Bank. Myanmar crop harvest status. QGIS 3.22.9\n\n\n\n\n\n\n\n2023. The World Bank. Myanmar EVI ratio anomaly. QGIS 3.22.9\n\n\n\n\n\n\n\n2023. The World Bank. Syrian Arab Republic - Standardize Precipitation-Evapotranspiration Index. QGIS 3.22.9\n\n\n\n\n\n\n\n2023. The World Bank. Syrian Arab Republic - Temperature anomaly. QGIS 3.22.9\n\n\n\n\n\n\n\n2023. The World Bank. Syrian Arab Republic - Vegetation Condition Index. QGIS 3.22.9"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-2",
    "href": "works/maps-and-infographics.html#section-2",
    "title": "Maps & Infographics",
    "section": "2022",
    "text": "2022\n\n\n\n\n\n2022. The World Bank. Ukraine population 2021. QGIS 3.22.9\n\n\n\n\n\n\n\n2022. The World Bank. Ukraine power plant. QGIS 3.22.9\n\n\n\n\n\n\n\n2022. The World Bank. Ukraine general administration. QGIS 3.22.9\n\n\n\n\n\n\n\n2022. The World Bank. Ukraine land cover and mine incidents. QGIS 3.16.8"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-3",
    "href": "works/maps-and-infographics.html#section-3",
    "title": "Maps & Infographics",
    "section": "2021",
    "text": "2021\n\n\n\n\n\n2021. WFP. Tropical Cyclone Seroja. ArcGIS 10.8"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-4",
    "href": "works/maps-and-infographics.html#section-4",
    "title": "Maps & Infographics",
    "section": "2020",
    "text": "2020\n\n\n\n\n\n2020. WFP. Facebook population movement. ArcGIS 10.7\n\n\n\n\n\n\n\n2020. WFP. COVID-19 data preparedness. ArcGIS 10.7\n\n\nFeatured on ESRI Map Gallery\n\n\n\n\n\n2020. WFP. Travel time to nearest market. ArcGIS 10.7\n\n\n\n\n\n\n\n2020. WFP. Vegetation anomaly. ArcGIS 10.7"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-5",
    "href": "works/maps-and-infographics.html#section-5",
    "title": "Maps & Infographics",
    "section": "2019",
    "text": "2019\n\n\n\n\n\n2019. WFP. Rainfall forecast. ArcGIS 10.7\n\n\n\n\n\n\n\n2019. WFP. Snow water equivalent. ArcGIS 10.7\n\n\n\n\n\n\n\n2019. WFP. Wet-spell. ArcGIS 10.7"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-6",
    "href": "works/maps-and-infographics.html#section-6",
    "title": "Maps & Infographics",
    "section": "2018",
    "text": "2018\n\n\n\n\n\n2018. WFP. Disaster snapshot. ArcGIS 10.4, Google Slide\n\n\n\n\n\n\n\n2018. WFP. Central Sulawesi Earthquake Response. ArcGIS 10.4\n\n\n\n\n\n\n\n2018. WFP. General logistics and planning. ArcGIS 10.4\n\n\n\n\n\n\n\n2018. WFP. Travel time. ArcGIS 10.4\n\n\nFeatured on WFP Story\n\n\n\n\n\n2018. WFP. VAMPIRE drought alert. ArcGIS 10.4, Google Slide\n\n\n\n\n\n\n\n2018. WFP. Dry-spell. ArcGIS 10.4"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-7",
    "href": "works/maps-and-infographics.html#section-7",
    "title": "Maps & Infographics",
    "section": "2017",
    "text": "2017\n\n\n\n\n\n2017. WFP. VAMPIRE flood alert. ArcGIS 10.4, Google Slide\n\n\n\n\n\n\n\n2017. WFP. Dry-spell. ArcGIS 10.4"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-8",
    "href": "works/maps-and-infographics.html#section-8",
    "title": "Maps & Infographics",
    "section": "2016",
    "text": "2016\n\n\n\n\n\n2016. WFP. Highlight activities. ArcGIS 10.4. CorelDRAW 12\n\n\n\n\n\n\n\n2016. WFP. Highlight activities. ArcGIS 10.4. CorelDRAW 12\n\n\n\n\n\n\n\n2016. WFP. Pidie earthquake. ArcGIS 10.3\n\n\n\n\n\n\n\n2016. WFP. Rainfall anomaly. ArcGIS 10.3\n\n\n\n\n\n\n\n2016. WFP. Vegetation health index. ArcGIS 10.3"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-9",
    "href": "works/maps-and-infographics.html#section-9",
    "title": "Maps & Infographics",
    "section": "2015",
    "text": "2015\n\n\n\n\n\n2015. WFP. Facebook population. ArcGIS 10.2.2"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-10",
    "href": "works/maps-and-infographics.html#section-10",
    "title": "Maps & Infographics",
    "section": "2014",
    "text": "2014\n\n\n\n\n\n2014. WFP. Province infographic. ArcGIS 10.2.2, Illustrator"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-11",
    "href": "works/maps-and-infographics.html#section-11",
    "title": "Maps & Infographics",
    "section": "2013",
    "text": "2013\n\n\n\n\n\n2013. WFP. Food for Assets. ArcGIS 10.2.2\n\n\n\n\n\n\n\n2013. WFP and BNPB. Jakarta flood. ArcGIS 10.2.2"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-12",
    "href": "works/maps-and-infographics.html#section-12",
    "title": "Maps & Infographics",
    "section": "2012",
    "text": "2012\n\n\n\n\n\n2012. ILO. Road condition. ArcGIS 10\n\n\n\n\n\n\n\n2012. ILO. LRB factsheet. CorelDRAW 12"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-13",
    "href": "works/maps-and-infographics.html#section-13",
    "title": "Maps & Infographics",
    "section": "2010",
    "text": "2010\n\n\n\n\n\n2010. UNOCHA. Mentawai Tsunami Response. ArcGIS 9.3\n\n\n\n\n\n\n\n2010. UNOCHA. Mentawai Tsunami Response. ArcGIS 9.3"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-14",
    "href": "works/maps-and-infographics.html#section-14",
    "title": "Maps & Infographics",
    "section": "2009",
    "text": "2009\n\n\n\n\n\n2009. UNOCHA. West Sumatra Earthquake Response. ArcGIS 9.3"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-15",
    "href": "works/maps-and-infographics.html#section-15",
    "title": "Maps & Infographics",
    "section": "2007",
    "text": "2007\n\n\n\n\n\n2007. MCC International. Immunization project. ArcView GIS 3.3\n\n\n\n\n\n\n\n2007. Ministry of Forestry. Ciliwung Cisadane Flood Project. ArcView GIS 3.3"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-16",
    "href": "works/maps-and-infographics.html#section-16",
    "title": "Maps & Infographics",
    "section": "2006",
    "text": "2006\n\n\n\n\n\n2006. SSFFMP. Land cover analysis. ILWIS 3.4\n\n\n\n\n\n\n\n2006. SSFFMP. Hotspot monitoring. ArcExplorer Java Edition\n\n\n\n\n\n\n\n2006. LAPAN. Fire Danger Rating System. ArcView GIS 3.3"
  },
  {
    "objectID": "works/maps-and-infographics.html#section-17",
    "href": "works/maps-and-infographics.html#section-17",
    "title": "Maps & Infographics",
    "section": "2004",
    "text": "2004\n\n\n\n\n\n2004. IPB. Satellite meteorology assignment. ArcView 3.3, ERMapper 6.4\n\n\n\n\n\n‚Üê Back to Works"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Content coming soon‚Ä¶"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Content coming soon‚Ä¶"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\nContent coming soon‚Ä¶"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\nContent coming soon‚Ä¶"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\nContent coming soon‚Ä¶"
  },
  {
    "objectID": "cv.html#certifications",
    "href": "cv.html#certifications",
    "title": "Curriculum Vitae",
    "section": "Certifications",
    "text": "Certifications\nContent coming soon‚Ä¶\n\nThis is a placeholder page. CV content will be added soon."
  },
  {
    "objectID": "csr.html",
    "href": "csr.html",
    "title": "CSR",
    "section": "",
    "text": "Don‚Äôt get confused with the common CSR - Corporate Social Responsibility! I‚Äôm introducing Climate Social Responsibility here.\nAs a teenager, I dreamt about a future career in technology that could provide actionable, life-saving insights. This led me to the Department of Geophysics and Meteorology at IPB University, where environment and climate issues captured my interest. During my final two years of undergraduate studies, I discovered the beauty of geospatial applications for climate research - combining my passion for science with visual communication through maps and data visualization.\nI understand that meteorological bureaus in each country have the absolute authority to provide climate-related data and information under government regulations. This repository serves as complementary information and an alternative perspective to ground observations - my contribution as a graduate of agricultural meteorology to the fields I‚Äôm passionate about."
  },
  {
    "objectID": "csr.html#overview",
    "href": "csr.html#overview",
    "title": "CSR",
    "section": "",
    "text": "Don‚Äôt get confused with the common CSR - Corporate Social Responsibility! I‚Äôm introducing Climate Social Responsibility here.\nAs a teenager, I dreamt about a future career in technology that could provide actionable, life-saving insights. This led me to the Department of Geophysics and Meteorology at IPB University, where environment and climate issues captured my interest. During my final two years of undergraduate studies, I discovered the beauty of geospatial applications for climate research - combining my passion for science with visual communication through maps and data visualization.\nI understand that meteorological bureaus in each country have the absolute authority to provide climate-related data and information under government regulations. This repository serves as complementary information and an alternative perspective to ground observations - my contribution as a graduate of agricultural meteorology to the fields I‚Äôm passionate about."
  },
  {
    "objectID": "csr.html#about-the-data",
    "href": "csr.html#about-the-data",
    "title": "CSR",
    "section": "About the Data",
    "text": "About the Data\nData and information play crucial roles in decision-making processes. This repository focuses on geographical data related to location and position on Earth. Despite limited availability of open geographical data on the internet, such information is vital for visualization, planning, and decision support systems at all administrative levels - from local to global.\nThis data catalogue serves as a repository for climate and vegetation datasets. I continuously update this collection with the latest selected data, promoting data sharing and open-source principles to encourage more data exploration and analysis.\n\nTechnical Specifications\nVector data: ESRI Shapefile format\nRaster data: GeoTIFF format (with TFW world file when applicable)\nGeographic Coordinate System: EPSG:4326, WGS84 datum\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\nProjected Coordinate System: EPSG:3395 WGS84 / World Mercator\n+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\n\n\nDisclaimer\nAll users downloading data from this repository are highly encouraged to read about data sources and processing methods in each dataset‚Äôs documentation. All data is provided as a public service. I am not responsible for any issues relating to accuracy, content, design, or usage. If you discover errors or omissions, please report them for future updates."
  },
  {
    "objectID": "csr.html#available-datasets",
    "href": "csr.html#available-datasets",
    "title": "CSR",
    "section": "Available Datasets",
    "text": "Available Datasets\nYou can find specific products below. I will continue to upload new data, so please check this page regularly for updates.\n\n\n\n\n  \n  \n    1958-2024\n    TerraClimate SPI\n  \n\n\n\n  \n  \n    1958-2024\n    TerraClimate SPEI"
  },
  {
    "objectID": "blog/20230306-pycpt-config-and-notebook.html",
    "href": "blog/20230306-pycpt-config-and-notebook.html",
    "title": "PyCPT config and notebook",
    "section": "",
    "text": "PyCPT is a Python interface and enhancement for the command line version of the International Research Institute for Climate and Society‚Äôs Climate Predictability Tool (CPT), for seasonal and sub-seasonal skill assessment and forecast experiments.\nThis notes is describing on how to use PyCPT s2sv1.92 and seav1.92 for Subseasonal and Seasonal Forecasting in Indonesia"
  },
  {
    "objectID": "blog/20230306-pycpt-config-and-notebook.html#installation",
    "href": "blog/20230306-pycpt-config-and-notebook.html#installation",
    "title": "PyCPT config and notebook",
    "section": "1 Installation",
    "text": "1 Installation\nThis section will explain on how to install the PyCPT inside Windows Subsystem for Linux (WSL) 2. This step-by-step guide was tested using Windows 11 with WSL2 - Ubuntu 22 enabled running on Thinkpad T480 2019, i7-8650U 1.9GHz, 64 GB 2400 MHz DDR4.\n\n1.1 Anaconda Python3\n\nDownload and install Anaconda Python on your WSL Ubuntu Linux.\nGo to https://repo.anaconda.com/archive/ to find the list of Anaconda releases\nSelect the release you want. I have a 64-bit computer, so I chose the latest release ending in x86_64.sh. If I had a 32-bit computer, I‚Äôd select the x86.sh version. If you accidentally try to install the wrong one, you‚Äôll get a warning in the terminal. I chose Anaconda3-2022.10-Linux-x86_64.\nFrom the terminal run wget https://repo.anaconda.com/archive/[YOUR VERSION]. Example:\nwget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64\nAfter download process completed, Run the installation script: bash Anaconda[YOUR VERSION].sh\nbash Anaconda3-2022.10-Linux-x86_64.sh\nRead the license agreement and follow the prompts to press Return/Enter to accept. Later will follow with question on accept the license terms, type yes and Enter. When asks you if you‚Äôd like the installer to prepend it to the path, press Return/Enter to confirm the location. Last question will be about initialize Anaconda3, type yes then Enter.\nClose the terminal and reopen it to reload .bash configs. It will automatically activate base environment.\nDeactivate base environment then set to false the confirguration of auto activate the base environment by typing\nconda deactivate && conda config --set auto_activate_base false\nTo test that it worked, which python in your Terminal. It should print a path that has anaconda in it. Mine is /home/bennyistanto/anaconda3/bin/python. If it doesn‚Äôt have anaconda in the path, do the next step.\nManually add the Anaconda bin folder to your PATH. To do this, I added \"export PATH=/home/bennyistanto/anaconda3/bin:$PATH\" to the bottom of my ~/.bashrc file.\nYou should use Anaconda Virtual Environments. This step must only be done the first time. Once the environment has been created there is no need to do it again.\nFirst, open your Terminal (in your Ubuntu Linux on WSL), create the Python environment:\nconda create -n cpt\nProceed with y\nThe environment created can now be ‚Äòactivated‚Äô and ready to install the supporting packages:\nconda activate cpt\n  conda install -c conda-forge xarray matplotlib scipy cartopy netcdf4 numpy pandas subprocess\n\n\n\n1.2 CPT\nDownload the 16.5.8 version of CPT from https://academiccommons.columbia.edu/doi/10.7916/d8-em5q-0f07. Use the source code tarfile CPT.16.5.8.tar.gz\n\n1.2.1 Prerequisites:\nI f you don‚Äôt have GCC and GFortran, please install it before continuing to install the CPT\nsudo apt install gcc gfortran make git\n\n\n1.2.2 Install CPT\n\nNavigate to your home directory by typing cd ~ in your terminal\nUntar the CPT tar file into CPT/XX.X.X\ntar xvzf CPT.16.5.8.tar.gz\nBuild CPT\ncd CPT/XX.X.X\n  make distclean\n  make\nInstall CPT: Determine where you want to install CPT, making sure this folder is in your PATH\nmake INSTALL_DIR=~/CPT/16.5.8 install\nUpdate your PATH and add CPT_BIN_DIR\nEdit the file ~/.bashrc, adding the following lines to the file:\nexport PATH=$PATH:~/CPT/16.5.8/bin\n  export CPT_BIN_DIR=~/CPT/16.5.8/bin\nClone PyCPT\nUse git to download from the Bitbucket Git Repository into your home directory You will need to install a GIT client if you want to keep the PyCPT source up to date.\nuse git clone to get iri-pycpt from bitbucket https://bitbucket.org/py-iri/iri-pycpt.git\ncd ~/CPT\n  git clone https://bitbucket.org/py-iri/iri-pycpt.git"
  },
  {
    "objectID": "blog/20230306-pycpt-config-and-notebook.html#supporting-files",
    "href": "blog/20230306-pycpt-config-and-notebook.html#supporting-files",
    "title": "PyCPT config and notebook",
    "section": "2 Supporting Files",
    "text": "2 Supporting Files\nThere are one authentication file that required to be available and also two modified py files that need to be copied and replaced the existing files in the PyCPT folder.\n\n2.1 Authentication File\nONLY needed for the s2s version\nCreate the file .IRIDLAUTH in the main PyCPT folder. It must contain only one line with the Data Library S2S key (104 characters) obtained via this link. Please do not share your key.\n\nIf you have file .IRIDLAUTH in your PyCPT folder, don‚Äôt forget to set the permission.\nchmod 644 ~/CPT/iri-pycpt/.IRIDLAUTH\n\n\n\n2.2 PyCPT python function\nThere are some minor revision in both py functions. You need to copy and replace the pycpt_functions.py and pycpt_functions_seasonal.py\n\n\n2.3 Notebook Files\nI provide four ipynb files (2 Subseasonal and 2 Seasonal Forecasts), with study case for Indonesia. Put these files inside PyCPT folder."
  },
  {
    "objectID": "blog/20230306-pycpt-config-and-notebook.html#run-the-forecasts",
    "href": "blog/20230306-pycpt-config-and-notebook.html#run-the-forecasts",
    "title": "PyCPT config and notebook",
    "section": "3 Run the Forecasts",
    "text": "3 Run the Forecasts\nMake sure you are inside the PyCPT folder or navigate your Terminal to cd ~/CPT/iri-pycpt.\n\nRun the Jupyter Notebook.\njupyter notebook\nTry one of the notebook I have provided.\nSeasonal Forecasts\n\n]PyCPT_seav1.9.2-IDN_CCA_PRCP_16N_18S_85E_150E_CHIRPS__PRCP_11N_13S_90E_145E_NCEP-CFSv2_NASA-GEOSS2S_Mar2023.ipynb](https://gist.github.com/bennyistanto/3c87cca14fec4ce2bb35203b37593135/raw/0d964a18f93ac376302f9b8ca3c3fbe839fa4a5f/PyCPT_seav1.9.2-IDN_CCA_PRCP_16N_18S_85E_150E_CHIRPS__PRCP_11N_13S_90E_145E_NCEP-CFSv2_NASA-GEOSS2S_Mar2023.ipynb)\nPyCPT_seav1.9.2-IDN_PCR_PRCP_16N_18S_85E_150E_CHIRPS__PRCP_11N_13S_90E_145E_NCEP-CFSv2_NASA-GEOSS2S_Mar2023.ipynb\nSubseasonal Forecasts\nPyCPT_s2sv1.9.2-Istanto_GFM1622_CCA.ipynb\nPyCPT_s2sv1.9.2-Istanto_GFM1622_PCR.ipynb\n\n\nFeel free to adjust the study area, model, etc."
  },
  {
    "objectID": "blog/20230306-pycpt-config-and-notebook.html#references",
    "href": "blog/20230306-pycpt-config-and-notebook.html#references",
    "title": "PyCPT config and notebook",
    "section": "4 References",
    "text": "4 References\n\nhttps://bitbucket.org/py-iri/iri-pycpt/src/master/\nhttps://iri-pycpt.github.io/\nhttps://iri.columbia.edu/our-expertise/climate/tools/cpt/"
  },
  {
    "objectID": "blog/20220319-batch-task-execution-in-google-earth-engine-code-editor.html",
    "href": "blog/20220319-batch-task-execution-in-google-earth-engine-code-editor.html",
    "title": "Batch task execution in Google Earth Engine Code Editor",
    "section": "",
    "text": "Have you ever had a problem when you wanted to download the entire data of an ImageCollection from Google Earth Engine (GEE), but you were lazy and bored when you had to click on each available date to save it on Google Drive?\nAre you looking for a solution on how to do it in more efficient way? Then, this blog post is for you!\nThis week I am working on analysis of phenological metrics for food crops, I need a timeseries vegetation index data and I will use MODIS Vegetation Indices (VI) data. What I usually do is download the data through NASA AppEEARS, then pre-process it (remove pixel with bad quality based on Pixel Reliability layer) using GIS software.\nNow I would like to use GEE to download the cleaned (pre-process) MODIS VI, I will try to get MOD13Q1 (Terra) and MYD13Q1 (Aqua), and combine the two 16-day composites into a synthethic 8-day composite containing data from both Aqua and Terra.\nOk, let start!\nI need to define the geographic domain, I will use Ukraine as an example.\nThen I need to import the data and define the time range. For the reference, the data is available at GEE Data Catalog:\n\nTerra - https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD13Q1?hl=en\nÔªøAqua - https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MYD13Q1?hl=en\n\nFor this example, I will try to download all data in 2021. As Terra and Aqua has 23 data for each year (16-day), the total data for both Aqua and Terra will be 46.\nAs I will pre-process the VI using GEE, I need to create a function to apply the QA Bitmask, so at the end I will get cleaned (pixel with good quality only) VI data. Bitmask information are available from above link, or you can check below.\nI am using bitwiseExtract function from Daniel Wiell\nExample: A mask with ‚ÄúGood data, use with confidence‚Äù would be ‚ÄúbitwiseExtract(image, 0, 1).eq(0)‚Äù\nNow, let‚Äôs apply the SummaryQA and DetailedQA. As example in below code, I will use ‚ÄúGood data, use with confidence‚Äù and ‚ÄúVI produced with good quality‚Äù.\nNext, let‚Äôs filter the data based on preferred time range and select EVI, SummaryQA and DetailedQA layer for both Aqua and Terra.\nRemoved bad pixel by mask using above function from the collection of images.\nCombine Aqua and Terra into single Image Collection and sorted from the earliest date, then I will have VI with 8-day temporal resolution.\nAnd below is the result from ‚Äúprint‚Äù command.\n\nCreate a symbology, and test the output as expected or not by add the last map to map display.\nTo download the data, usually I use standard code like below.\nTo batch export for all data in the ImageCollection, use below code.\nUsing above code, you will have 46 unsubmitted tasks that required you to click all the RUN button. I know, clicking 46 button is not a big deal. But, by doing this frequently or you need to download more than 500 data, it‚Äôs very boring.\n\nHow to export all the data without clicking on RUN button? Thanks to Dongdong Kong and yzq.yang for creating the script.\nBelow is the step-by-step:\n\nRun your Google Earth Engine code;\nWait until all the tasks are listed (the Run buttons are shown);\nClick two computer keys fn and f12 at the same time and bring up console; or find it via: menubar Develop &gt; Show JavaScript Console (Safari), and menubar View &gt; Developer &gt; JavaScript Console (Chrome)\nCopy and paste below code into the console, then Enter;\nWait until the code sends messages to the server to run all the tasks (you wait and relax); wait until GEE shows the dialogue window asking you to click ‚ÄúRun‚Äù; (below picture)\nPlease bear in mind, the time execution is depend on the image size you will export. Example, 1 year data for Ukraine on MODIS VI takes a minute below picture to popup.\n\n\nCopy and paste below code into the console, then Enter;\nKeep your computer open until all the tasks (Runs) are done (you probably need to set your computer to never sleep).\n\nYou can close your browser or computer after all the Tasks running and submitted to server.\nLink to code\nNOTES:\nThe preferred and more stable way to do batch exports is using the Python API. Or you can try geemap and the example notebook from Qiusheng Wu.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20201008-la-nina-and-indonesia-context.html",
    "href": "blog/20201008-la-nina-and-indonesia-context.html",
    "title": "La Ni√±a and Indonesia context",
    "section": "",
    "text": "General sensitivity of rainfall in Indonesia to sea surface temperature (SST) changes in NINO-4 region.\nIndonesia is affected by La Ni√±a which is associated with a rapidly falling sea surface temperature (SST) in NINO-4 region, negatively affect rainfall in most areas of the country.\nRead this post for general introduction and how to do the analysis.\nAccording to NOAA, SST values in the Ni√±o 3.4 region may not be the best choice for determining La Ni√±a episodes but, for consistency, the index has been defined by negative anomalies in this area. A better choice might be the Ni√±o 4 region, since that region normally has SSTs at or above the threshold for deep convection throughout the year.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú580‚Äù] Source: http://www.bom.gov.au/climate/influences/images/map-indices.png [/caption]\nThe map demonstrates the change in rainfall associated with a 1¬∞C decrease in SST in NINO-4 region, as a signal for moderate La-Nina. Dark blue areas represent more than 50mm increase in monthly rainfall. Yet changes in monthly rainfall is not extreme, only about 10 - 50 mm for most areas in the country.\n\nData\n\n119 years (1901-2019) monthly rainfall data used in the analysis came from Climate Research Unit - University of East Anglia, and downloaded from Center for Environmental Data Analysis, UK (https://catalogue.ceda.ac.uk/uuid/89e1e34ec3554dc98594a5732622bce9), and\nSST anomaly in NINO and Indian Ocean region region from National Oceanic and Atmospheric Administration (NOAA) ERSL Physical Sciences Laboratory (https://psl.noaa.gov/gcos_wgsp/Timeseries/).\n\nSimple regression applied to indicate the correlation between rainfall anomaly in each area to anomaly of SST in the Pacific Ocean which represent ENSO signals.\nY = aX + b, where:\nY = Rainfall anomaly\nb = Y intercept\na = Slope\nX = SST anomaly\nHow about other NINO region? You can check below maps.\n\n\n\n\n\n\n\n\n\n\n\nCRU rainfall above has 50km/pixel spatial resolution, and probably couldn‚Äôt capture well local phenomena. If we have data with higher spatial resolution we could do similar approach to get sensitivity rainfall due to SST changes.\nFor example, below map used CHIRPS rainfall from CHC University of California - Santa Barbara, with 0.05 deg ~ 5.6 km spatial resolution and shorter time period 1981 - 2019.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20040427-model-pendugaan-biomassa-tanaman-padi.html",
    "href": "blog/20040427-model-pendugaan-biomassa-tanaman-padi.html",
    "title": "Model pendugaan biomassa tanaman padi",
    "section": "",
    "text": "Ini adalah salah satu tugas praktikum dari mata kuliah Model Simulasi Pertanian yang diberikan oleh Pak Handoko dan Pak Yon Sugiarto.\nPendugaan biomassa tanaman padi yang dilakukan mencakup dua tahapan proses yang saling mempengaruhi satu sama lain. Struktur neraca air yang dimodelkan mengasumsikan CH dan faktor irigasi sebagai sumber air. Dua model pendugaan Biomassa dan Neraca Air Lahan ini kemudian digabungkan dalam satu model yang berkaitan satu sama lain.\nSeperti yang telah dikemukakan diatas bahwa pemodelan ini merupakan penggabungan antara pendugaan biomassa tanaman dan neraca air lahan. Oleh karena itu ada parameter masukan yang dubutuhkan. Untuk sub model perkembangan dan sub model pertumbuhan dibutuhkan parameter masukan yang dapat kita impor dari file input oleh pengguna model yaitu data iklim suhu udara rataan harian (dalam ¬∞C) dan radiasi surya (dalam MJ M-2) yang dalam hal ini data inputnya adalah data iklim Stasiun Gunung Medan selama periode setahun. Dan untuk neraca air lahan dibutuhkan data iklim curah hujan, radiasi dan kelembaban relatif."
  },
  {
    "objectID": "blog/20040427-model-pendugaan-biomassa-tanaman-padi.html#user-interface-menggunakan-visual-basic",
    "href": "blog/20040427-model-pendugaan-biomassa-tanaman-padi.html#user-interface-menggunakan-visual-basic",
    "title": "Model pendugaan biomassa tanaman padi",
    "section": "User interface menggunakan Visual BASIC",
    "text": "User interface menggunakan Visual BASIC\nSelanjutnya untuk memudahkan perhitungan pendugaan biomassa dengan periode tertentu, beberapa persamaan diatas saya tulis ulang menggunakan Visual BASIC 6 (kodenya dapat dilihat dibagian bawah tulisan), dilengkapi dengan user interface sederhana seperti gambar dibawah."
  },
  {
    "objectID": "blog/20210125-calculate-spi-using-monthly-rainfall-data-in-geotiff-format.html",
    "href": "blog/20210125-calculate-spi-using-monthly-rainfall-data-in-geotiff-format.html",
    "title": "Calculate SPI using monthly rainfall data in GeoTIFF format",
    "section": "",
    "text": "NOTES - 1 Mar 2022\nI have write a new and comprehensive guideline on SPI, you can access via this link https://bennyistanto.github.io/spi/\n\nThese last few months, I have tried a lot of difference formulation to calculate Standardized Precipitation Index (SPI) based on rainfall data in netCDF format, check below blog post as a background:\n\nSPI using IMERG\nSPI using CHIRPS\n\nThe reason why I use rainfall in netCDF format in above blog post because the software to calculate SPI: climate-indices python package will only accept single netCDF as input, and the SPI script will read the netCDF input file based on time dimension.\nConverting raster files into netCDF is easy using GDAL or other GIS software, but to make the time dimension enabled netCDF output and CF-Compliant is another story. Using NCO, I can easily rename any variable with ncrename and add attributes to the content with ncatted to fit the desired data structure. Yet, I think this is too much work if every time I update the data, I should update the attribute too.\nThen I found this solution.\nNow, I am able to convert bunch of GeoTIFF in a folder into a single netCDF file with time dimension enabled and CF-Compliant and can be accepted as input using climate-indices software.\nLets try!\nThis step-by-step guide was tested using Macbook Pro, 2.9 GHz 6-Core Intel Core i9, 32 GB 2400 MHz DDR4, running on macOS Big Sur 11.1\n\n0. Working directory\nFor this test, I am working at /Users/bennyistanto/Temp/CHIRPS/SPI/Java directory. I have some folder inside this directory:\n\nDownload\nFitting\nInput_nc\nInput_TIF\nOutput_nc\nOutput_TEMP\nOutput_TIF\nScript\nSubset\n\nFeel free to use your own preferences for this setting.\n\n\n1. Software requirement\nThe installation and configuration described below are performed using a bash/zsh shell on macOS. Windows users will need to install and configure a bash shell in order to follow the instruction below. Try to use¬†Windows Subsystem for Linux¬†for this purpose.\n1.1. If you are new to using Bash refer to the following lessons with Software Carpentry:¬†http://swcarpentry.github.io/shell-novice/\n1.2. If you don‚Äôt have¬†Homebrew, you can install it by pasting below code in your macOS terminal.\n\nbin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n\n1.3. Install wget (for downloading data). Use Hombrew to install it by pasting below code in your macOS terminal.\n\nbrew install wget\n\n1.4. Download and install¬†Panoply Data Viewer¬†from¬†NASA GISS¬†on your machine:¬†macOS,¬†Windows,¬†Linux.\n1.5. Download and install Anaconda Python on your machine:¬†macOS,¬†Windows,¬†Linux.\n1.6. Or you can use Miniconda:¬†macOS,¬†Windows,¬†Linux\n\n\n2. Configure the python environment\nThe code for calculating SPI is written in Python 3. It is recommended to use either the¬†Miniconda3¬†(minimal Anaconda) or Anaconda3 distribution. The below instructions will be Anaconda specific (although relevant to any Python¬†virtual environment), and assume the use of a bash shell.\nA new Anaconda¬†environment¬†can be created using the¬†conda¬†environment management system that comes packaged with Anaconda. In the following examples, I‚Äôll use an environment named climate_indices (any environment name can be used instead of climate_indices) which will be created and populated with all required dependencies through the use of the provided setup.py file.\n2.1. First, create the Python environment:\n\nconda create -n climate_indices python=3.7\n\n2.2. The environment created can now be ‚Äòactivated‚Äô:\n\nconda activate climate_indices\n\n2.3. Install¬†climate-indices¬†package. Once the environment has been activated then subsequent Python commands will run in this environment where the package dependencies for this project are present. Now the package can be added to the environment along with all required modules (dependencies) via¬†pip:\n\npip install climate-indices\n\n2.4. Install Climate Data Operator (CDO) from Max-Planck-Institut f√ºr Meteorologie using conda\n\nconda install -c conda-forge cdo\n\n2.5. Install netCDF Operator (NCO) using conda\n\nconda install -c conda-forge nco\n\n2.6. Install jupyter and other package using conda\n\nconda install -c conda-forge jupyter numpy datetime gdal netCDF4\n\n\n\n3. Rainfall data acquisition\nSPI requires monthly rainfall data, and there are many source providing global high-resolution gridded monthly rainfall data:\n\nCHIRPS\nIMERG\nFLDAS\nTerraClimate\nCRU\n\nFor this step-by-step guideline, I will use CHIRPS monthly data in GeoTIFF format and the case study is Java island - Indonesia.\nWhy CHIRPS? It is produced at 0.05 x 0.05 degree spatial resolution, make it CHIRPS is the highest gridded rainfall data, and long-term historical data from 1981 ‚Äì now.\n3.1. Navigate to Download folder in the working directory. Download using¬†wget¬†all CHIRPS monthly data in GeoTIFF format from Jan 1981 to Dec 2020 (this is lot of data ~7GB zipped files, and become 27GB after extraction, please make sure you have bandwidth and unlimited data package):\n\nwget -r https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/\n\n3.2. Gunzip all the downloaded files\n\ngunzip *.gz\n\n3.3. Download the Java boundary shapefile http://on.istan.to/365PSyH. And save it in Subset directory then unzip it.\n3.4. Still in your Download directory, Clip your area of interest using Java boundary and save it to Input_TIF directory. I will use gdalwarp command from GDAL to clip all GeoTIFF files in a folder.\n\nfor i in `find *.tif`; do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline ../Subset/java_bnd_chirps_subset.shp $i ../Input_TIF/java_cli_$i; done\nif you are using Windows, you can follow below script.\nfor %i IN (*.tif) do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline ../Subset/java_bnd_chirps_subset.shp %i ../Input_TIF/java_%i\nIf you have limited data connection or lazy to download ~7GB and process ~27GB data, you can get pre-processed clipped data for Java covering Jan 1981 to Dec 2020, with file size ~6.8MB. Link: https://on.istan.to/3iLu68v\n\n3.4. Download python script/notebook that I use to convert GeoTIFF in a folder to single netCDF, save it to Script folder.\n\nPython script: https://gist.github.com/bennyistanto/ff16dc08cc06b5a13740323db41dc8f3 or https://s3.benny.istan.to/script/tiff2nc.py\nJupyter notebook: https://github.com/wfpidn/VAMscript/blob/master/notebook/CHIRPS-GeoTIFF_to_netCDF_Java.ipynb or https://s3.benny.istan.to/script/CHIRPS-GeoTIFF_to_netCDF_Java.ipynb\n\nYou MUST adjust the folder location (replace /path/to/directory/ with yours) in line 13 and 114.\nIf you are using other data source (I assume all the data in WGS84), you need to adjust few code in:\n\nLine 31: folder location\nLine 40: start of the date\nLine 44: output name\nLine 53: date attribute\nLine 85-88: bounding box\nLine 110: output filename structure\nLine 114: folder location\nLine 120-122: date character location in a filename\n\n3.5. After everything is set, then you can execute the translation process\n\nUsing Python in Terminal, navigate to your Script directory, type python tiff2nc.py\n\n\n\nWait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc. You will find this file inside Input_TIF folder. Move it to Input_nc folder.\nUsing Jupyter, make sure you still inside conda climate_indices environment, type in Terminal jupyter notebook\n\n\n\nNavigate to your notebook directory (where you put *.ipynb file), run Cell by Cell until completed. Wait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc. You will find this file inside Input_TIF folder. Move it to Input_nc folder.\nYou also can get this data: java_cli_chirps_1months_1981_2020.nc via this link http://on.istan.to/3iRDl7e\n\n\n\n4. Calculate SPI\nThe climate-indices software enables the user to calculate SPI using any gridded netCDF dataset, However, there are certain specifications for input files that vary based on input type.\n\nPrecipitation unit must be written as ‚Äòmillimeters‚Äô, ‚Äòmilimeter‚Äô, ‚Äòmm‚Äô, ‚Äòinches‚Äô, ‚Äòinch‚Äô or ‚Äòin‚Äô\nData dimension and order must be written as ‚Äòlat‚Äô, ‚Äòlon‚Äô, ‚Äòtime‚Äô or ‚Äòtime‚Äô, ‚Äòlat‚Äô, ‚Äòlon‚Äô\n\n4.1. I have try to make script in Step 3.4. are follow above specification. To make sure everything is correct, in your Terminal - navigate to your directory where you save java_cli_chirps_1months_1981_2020.nc file: Input_nc. Then type ncdump -h java_cli_chirps_1months_1981_2020.nc\n\nFrom above picture, I can say:\n\nTime dimension is enabled, 480 is total months from Jan 1981 to Dec 2020\nData dimension and order are following the specification ‚Äòtime‚Äô, ‚Äòlat‚Äô, ‚Äòlon‚Äô\nThe unit is in ‚Äòmm‚Äô\n\nSo, everything is correct and I am ready to calculate SPI. Make sure in your Terminal still inside climate_indices environment.\nOther requirements and options related to the indices calculation, please follow https://climate-indices.readthedocs.io/en/latest/#indices-processing\n4.2. In order to pre-compute fitting parameters for later use as inputs to subsequent SPI calculations I can save both gamma and Pearson distribution fitting parameters to NetCDF, and later use this file as input for SPI calculations over the same calibration period.\nIn your Terminal, run the following code.\n$ spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/java_cli_chirps_1months_1981_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/Java/Output_nc/java_CHIRPS --multiprocessing all --save_params /Users/bennyistanto/Temp/CHIRPS/Java/Fitting/java_CHIRPS_fitting.nc --overwrite\n\nThe above command will compute SPI (standardized precipitation index, both gamma and Pearson Type III distributions) from an input precipitation dataset (in this case, CHIRPS precipitation dataset). The input dataset is monthly rainfall accumulation data and the calibration period used will be Jan-1981 through Dec-2020. The index will be computed at 1,2,3,6,9,12,24,36,48,60 and 72-month timescales. The output files will be &lt;out_dir&gt;/java_CHIRPS_spi_gamma_xx.nc, and &lt;out_dir&gt;/java_CHIRPS_spi_pearson_xx.nc.\nThe output files will be:\n\n1-month: /Output_nc/java_CHIRPS_spi_gamma_01.nc\n2-month: /Output_nc/java_CHIRPS_spi_gamma_02.nc\n3-month: /Output_nc/java_CHIRPS_spi_gamma_03.nc\n6-month: /Output_nc/java_CHIRPS_spi_gamma_06.nc\n9-month: /Output_nc/java_CHIRPS_spi_gamma_09.nc\n12-month: /Output_nc/java_CHIRPS_spi_gamma_12.nc\n24-month: /Output_nc/java_CHIRPS_spi_gamma_24.nc\n36-month: /Output_nc/java_CHIRPS_spi_gamma_36.nc\n48-month: /Output_nc/java_CHIRPS_spi_gamma_48.nc\n60-month: /Output_nc/java_CHIRPS_spi_gamma_60.nc\n72-month: /Output_nc/java_CHIRPS_spi_gamma_72.nc\n1-month: /Output_nc/java_CHIRPS_spi_pearson_01.nc\n2-month: /Output_nc/java_CHIRPS_spi_pearson_02.nc\n3-month: /Output_nc/java_CHIRPS_spi_pearson_03.nc\n6-month: /Output_nc/java_CHIRPS_spi_pearson_06.nc\n9-month: /Output_nc/java_CHIRPS_spi_pearson_09.nc\n12-month: /Output_nc/java_CHIRPS_spi_pearson_12.nc\n24-month: /Output_nc/java_CHIRPS_spi_pearson_24.nc\n36-month: /Output_nc/java_CHIRPS_spi_pearson_36.nc\n48-month: /Output_nc/java_CHIRPS_spi_pearson_48.nc\n60-month: /Output_nc/java_CHIRPS_spi_pearson_60.nc\n72-month: /Output_nc/java_CHIRPS_spi_pearson_72.nc\n\nParallelization will occur utilizing all CPUs.\nFor small area of interest, the calculation will fast and don‚Äôt take much time. Below is example if you processed bigger area:\n\nMonthly IMERG data, global coverage 180W - 180E, 60N - 60S, 0.1 deg spatial resolution. It takes almost 9-hours to process SPI 1-72 months.\n\n\nOutput gamma and pearson file https://on.istan.to/2MhVnTP\nFitting file http://on.istan.to/3c6ZLjq\n\n\n5. Updating procedure when new data is available\nWhat if the new data is coming (Jan 2021)? Should I re-run again for the whole periods, 1981 to date? That‚Äôs not practical as it requires large storage and time processing if you do for bigger coverage (country or regional analysis).\nUpdating SPI up to SPI72, I should have data at least 6 years back (2014) from the latest (Jan 2021). In order to avoid computation for the whole periods (1981-2021), I will process data data only for year 2014 to 2021.\nAfter that, I continue the process following Step 3.4.\nStep 4.2 demonstrates how distribution fitting parameters can be saved as NetCDF. This fittings NetCDF can then be used as pre-computed variables in subsequent SPI computations. Initial command computes both distribution fitting values and SPI for various month scales.\nThe distribution fitting variables are written to the file specified by the ‚Äô--save_params‚Äô option.\nThe second command also computes SPI but instead of computing the distribution fitting values it loads the pre-computed fitting values from the NetCDF file specified by the ‚Äô--load_params‚Äô option.\nSee below code:\n$ spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/java_cli_chirps_1months_2014_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/Java/Output_nc/java_CHIRPS --multiprocessing all --load_params /Users/bennyistanto/Temp/CHIRPS/Java/Fitting/java_CHIRPS_fitting.nc\n\n\n\n6. Visualized the result using¬†Panoply\nLet see the result.\n6.1. From the Output_nc directory, right-click file java_CHIRPS_spi_gamma_3_month.nc (you can download this file from this link:¬†https://on.istan.to/2MhVnTP) and Open With Panoply.\n\n6.2. From the Datasets tab select spi_gamma_3_month and click Create Plot\n\n6.3. In the Create Plot window select option Georeferenced Longitude-Latitude.\n6.4. When the Plot window opens:\n\nArray tab: Change the time into 469 to view data on Jan 2020\nScale tab: Change value on Min -3, Max 3, Major 6, Color Table CB_RdBu_09.cpt\nMap tab: Change value on Center on Lon 110.0 Lat -7.5, then Zoom in the map through menu-editor Plot &gt; Zoom Plot In few times until Indonesia appear proportionally. Set grid spacing 2.0 and Labels on every grid lines.\nOverlays tab: Change Overlay 1 to MWDB_Coasts_Countries_1.cnob\n\n\n\n\n7. Convert the result to GeoTIFF\nConversion of the result into GeoTIFF format, CDO required the variable should be in¬†\"time, lat, lon\", while the output from SPI:¬†java_CHIRPS_spi_xxxxx_x_month.nc¬†in¬†\"lat, lon, time\", you can check this via¬†ncdump -h file.nc\n\nNavigate your Terminal to folder Output_nc\nLet‚Äôs re-order the variables into¬†time,lat,lon¬†using¬†ncpdq¬†command from NCO and save the result to folder Output_TEMP\n\nncpdq -a time,lat,lon java_CHIRPS_spi_gamma_3_month.nc ../Output_TEMP/java_CHIRPS_spi_gamma_3_month_rev.nc\n\nNavigate your Terminal to folder Output_TEMP\nCheck result and metadata to make sure everything is correct.\n\nncdump -h java_CHIRPS_spi_gamma_3_month_rev.nc\n\nThen convert all SPI value into GeoTIFF with¬†time¬†dimension information as the¬†filename¬†using CDO and GDAL.\nNavigate your Terminal to folder Output_TEMP and save the result to folder Output_TIF\n\nfor t in `cdo showdate java_CHIRPS_spi_gamma_3_month_rev.nc`; do\ncdo seldate,$t java_CHIRPS_spi_gamma_3_month_rev.nc dummy.nc\ngdal_translate -of GTiff -a_ullr 105.05 -5.05 116.25 -8.8 -a_srs EPSG:4326 -co COMPRESS=LZW -co PREDICTOR=1 dummy.nc ../Output_TIF/$t.tif\ndone\n\n\nFINISH\nCongrats, now you are able to calculate SPI based on monthly rainfall in GeoTIFF format.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20091005-landmark-survey-in-tls.html",
    "href": "blog/20091005-landmark-survey-in-tls.html",
    "title": "Landmark survey in TLS",
    "section": "",
    "text": "Since May 2009 I am working for UNFPA as Census Mapping Supervisor in Timor-Leste supporting Direksaun Jeral de Estatistika (DNE) conducting landmark survey as preparation before 2010 Population and Housing Census.\nMy main tasks was\n\nMentor team leaders and survey technicians, developing their skills for collecting geographic data methodically and systematically.\nBuilding their understanding of the purpose for which the data is being collected.\nWork with team leaders to manage the large volume of maps, survey forms, supplies and materials, and to manage the database of landmark locations and attributes as it grows in size during the course of the survey.\n\nBeside above tasks, I also go around visiting every house of ‚Äòchefe aldeia‚Äô (head of sub village) in 3 districts (Dili+Atauro, Oecusse and Baucau). Interestingly, in rural areas, the distance from chefe aldeias house to another is very far. My team sometimes have difficulties to mention how far is the distance (in km or in hour). If they said the distance is 3 hills, it means I need to walk and can‚Äôt use car or motorbike to go. üòÇüò©\nBut all of that was great fun and sometimes scary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary and finding during Landmark Survey\n\nTraining in Landmark Survey\nThere was a two weeks preparation training preceding the landmark survey. The training concerned to the basic concept of Geographic Information System (GIS), Global Positioning System (GPS) operation and census mapping application. The participants were also trained to conceive and use a number of basic supporting forms. The training was attended by 11 survey technical officers and 5 team leaders. During the training, there were two sample/practice surveys, a two-days survey in Dili and a three-days survey outside of Dili.\n\n\nProblems faced during the field\nUsed base maps are actually providing appropriate information, but there are some fatal error which causing the missteps during the LS such as: different scale of maps, unmatched positions, and also some conditions do not match the reality, i.e layer of: Aerial photo, Road, Toponimy. The maps were also does not have the coordinate system.\nHandheld Garmin GPS eTrex H is suitable for the beginner, as it is easy to use, small, lightweight, low cost and able to make quick and reliable satellite fixes. But it has a low accuracy, moreover with the various topographic condition of Timor-Leste. Noted landmarks position was mostly taken on the hillsides and under the shade trees. The accurate waypoints are difficult to be retrieved under those conditions by using Garmin eTrex H. Furthermore; this is equipped by the monochrome display which usually confusing to mark off the Track and existing base maps.\nThe technical team has a good grip of the field condition, but they are less reliable in utilizing the maps for supporting the survey activities. There was a number of misdirection during the survey. Redirection (to obtain the right way) is guided by synchronizing the GPS with base maps in laptop. In this case, Dili district - with its complex condition - required an extra concern.\nAt the beginning, the technical team always took the waypoints incorrectly, due to the limitation of their understanding on mapping basic concept and census application. Considerable time and patience were spent to make a significant improvement. The output of Dili district still needs a major revision, since it is the first location and there are a number of errors. The revision should be supported by accurate and good quality map. The most challenging problem during the survey was various topographic condition of Timor Leste, which required an excellent physical condition.\nThe overall purpose of this activity is to develop the technical team capacity and comprehension to overcome the current and future problems. The survey is not only focused on landmark position registration. Fruitful discussions on training materials and its application in the field were always been inserted during the survey.\nNSD Agency was highly supporting the LS by conducting the approach to Chefe Suco and Chefe Aldeia. The LS was successfully conducted without any major problems either technical or administrative and also good cooperation among the related agencies and team.\nRevision to the output has not been fully completed, due to the limitation of mapping expert in NSD.\n\n\nSuggestion from United Nations Security Operation Centre\nDuring the LS outside of Dili district area, United Nation Security Operation Center (UN SOC) suggested the team to always bring UN standard safety equipment, including: single communication radio, vehicle communication radio and vehicle GPS. UN staff is only allowed to conduct the trip to the area outside of DIli district using UN vehicle and UN flight. UN staff is not allowed to trip by public transportation i.e NAKROMA watercraft.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230822-parsing-bmkgs-daily-climate-data.html",
    "href": "blog/20230822-parsing-bmkgs-daily-climate-data.html",
    "title": "Parsing BMKG‚Äôs daily climate data",
    "section": "",
    "text": "To replicate below code, please download daily climate data from BMKG Data Online https://dataonline.bmkg.go.id/home, just a heads up, if you haven‚Äôt already registered on the portal, you might want to do so. It‚Äôs a necessary step before you can download any data.\nThen go to Climate Data &gt; Daily Data, choose the Station Type, Parameter, Province, Regency, Station Name and the Date Period and click Process button. You will get the data in *.xlsx format.\nYou can get one of the data example from this link: https://docs.google.com/spreadsheets/d/1xbBWeHhiMNs8IehHbsrMV9yeZlcu8GqR/edit?usp=sharing&ouid=104182606454912191559&rtpof=true&sd=true\nIn above example, I tried to get daily precipitation data for all station from 1 Jun 2000 - 31 Dec 2021. I would like to use it to correct the value and distribution daily IMERG data using bias correction method that currently I have develop.\nUnfortunately, too many missing value.\nI should find alternative daily timeseries precipitation data, probably gridded data will suit my objectives.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20160810-how-long-will-i-live.html",
    "href": "blog/20160810-how-long-will-i-live.html",
    "title": "How long will I live?",
    "section": "",
    "text": "What‚Äôs my place in the world population? How long will I live?\n\nPopulation.io aims to make demography ‚Äì the study of human populations ‚Äì accessible to a wider audience. Below is the summary:\n\nI was older than 59.6% of all people in the world and 62.8% in Indonesia.\nMy next milestone is 5 Sep 2022 then I‚Äôll be 5th billionth person to be alive in the world.\nI share a birthday with about 298,427 people around the world and that approximately 12,300 people were born in the same hour.\nThe website estimate that I will live until 21 Jan 2061 if I was an average world citizen. Whereas in Indonesia it would be until 17 Nov 2055, age 73.6 years.\nOr maybe I could live even longer if I moved to Japan, they estimate I will live until age 86.1 years or 12.5 years longer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200311-covid-19-and-wfh.html",
    "href": "blog/20200311-covid-19-and-wfh.html",
    "title": "Covid-19 and WFH",
    "section": "",
    "text": "[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú958‚Äù] Source: https://www.forbes.com/sites/judystone/2020/01/11/wuhan-coronavirus-outbreak-shows-the-importance-of-sound-science-sleuthing-and-cooperation/?sh=768f34351add [/caption]\nI was in Vanuatu when President Joko Widodo announced the first two confirmed cased in Indonesia. Yesterday, I returned to the office after a week-long mission and only spent a few hours there to wrap up some pending work. Then the Country Director told me to start working from home tomorrow.\nI think this is only temporary, maybe for a week or two, so I didn‚Äôt bring my equipment home. We‚Äôll see how it goes.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20201101-30-day-map-challenge.html",
    "href": "blog/20201101-30-day-map-challenge.html",
    "title": "30 Day Map Challenge, 2020",
    "section": "",
    "text": "For the month of November, Topi Tjukanov announced a 30 Day Map Challenge on Twitter.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú960‚Äù] Source: https://github.com/tjukanovt/30DayMapChallenge [/caption]\nThe idea is to create (and publish) maps based on different themes on each day of the month using the hashtag¬†#30DayMapChallenge, you can prepare the maps beforehand, but the main idea is to publish maps from specific topics on specific days listed below. Just include a picture of the map when you post to Twitter with the hashtag.\nYou don‚Äôt have to sign up anywhere to participate. There are no restrictions on the tools, technologies and the data you use in your maps. Doing less than 30 is also fine (and actually doing all 30 is really hard!).\nAs I don‚Äôt have Twitter account, so I would like to publish through this blogpost and update it every day.\nHappy mapping!\n\nDay 30 of #30DayMapChallenge ‚Äì A map\nThis is ‚ÄúA Map‚Äù text entirely made up of satellite imagery. Each letter would be a real world feature from a bird‚Äôs eye view. A building in the shape of an ‚ÄúA‚Äù, a lake in the shape of a ‚ÄúM‚Äù, a tree in the shape of a ‚ÄúP‚Äù..\nData. Google Satellite. Tools: Preview.app on macOS\n\nDay 29 of #30DayMapChallenge ‚Äì Globe\nH3 and tesselations on the sphere, and multiple levels of granularity.\nData. H3 Uber. Tools: Observable\n\nDay 28 of #30DayMapChallenge ‚Äì Non geographic map\nScatterplots, summary of earthquake that happen in the past week.\nData. USGS. Tools: Plotly\n\nDay 27 of #30DayMapChallenge ‚Äì Big or small data\nWorld tile grid map.\nData. Natural Earth. Tools: D3\n \nDay 26 of #30DayMapChallenge ‚Äì Map with a new tool\nAccording to infographic of the History of Open Source GIS from Makepath blog, xarray-spatial is categorized as a new tool, launched in 2020. Below map is example on how xarray-spatial could help finding shortest path using A* from start location to destination location.\nData. OSM. Tools: xarray-spatialÔªø\n\nDay 25 of #30DayMapChallenge ‚Äì COVID-19\nElderly population and access to hospital.\nData. BIG, OSM, NASADEM, MenLHK, Facebook. Tools: ArcGIS Desktop 10.7\n\nDay 24 of #30DayMapChallenge ‚Äì Elevation\nLightweight relief shearing for enhanced terrain perception.\nData. NASA‚Äôs Shuttle Radar Topography Mission. Tools: http://elasticterrain.xyz/map/\n\nDay 23 of #30DayMapChallenge ‚Äì Boundaries\nAdministrative boundaries; red: admin1, orange: admin2, purple: admin3, blue: admin4; grey: admin5.\nData. Statistics Indonesia - https://sig.bps.go.id Tools: ArcGIS Desktop 10.7\n\nDay 22 of #30DayMapChallenge ‚Äì Movement\nPopulation movement in 24 April 2020, 16:00 - 24:00.\nData. Facebook Disaster Maps - https://dataforgood.fb.com/tools/disaster-maps/ Tools: Kepler.gl\n\nDay 21 of #30DayMapChallenge ‚Äì Water\nHistorical flood occurrence in November, 1984 - 2019.\nData. Global Monthly Water History - EC JRC. Tools: Google Earth Engine\n\nDay 20 of #30DayMapChallenge ‚Äì Population\nChildren under 5 population.\nData. Facebook Population Density - https://dataforgood.fb.com/tools/population-density-maps/ Tools: ArcGIS Desktop 10.7\n\nDay 19 of #30DayMapChallenge ‚Äì NULL\nCloud cover can cause NULL data on Land Surface Temperature information generated by MODIS satellite.\nData. MODIS - https://lpdaac.usgs.gov/products/mod11a2v006/ Tools: ArcGIS Desktop 10.7\n\nDay 18 of #30DayMapChallenge ‚Äì Landuse\nLanduse of Kalimantan in 2019\nData: Ministry of Environment and Forestry - http://geoportal.menlhk.go.id/arcgis/rest/services/KLHK/Penutupan_Lahan_Tahun_2019/MapServer Tools: QGIS 3.16\n\nDay 17 of #30DayMapChallenge ‚Äì Historical\nHistorical style map of Indonesia archipelago.\nData: OpenStreetMap. Tools: Mapbox Studio\n\nDay 16 of #30DayMapChallenge ‚Äì Island(s)\nLombok island\nData: OpenStreetMap. Tools: Mapbox Studio\n\nDay 15 of #30DayMapChallenge ‚Äì Connections\nPort accessibility, measured travel time (mixed transportation mode) to the nearest public port (operated by govt: Pelindo or Kemenhub). Cargo vessel frequency, recorded thousands of commercial ships that moved across the ocean in 2012\nData: Accessibility map using various data and modeled via ArcGIS Model Builder. Ship line http://ede.grid.unep.ch/download/shipping_tif.zip Tools: ArcGIS Desktop 10.7\n\nDay 14 of #30DayMapChallenge ‚Äì Climate changes\nGeneral sensitivity of rainfall in Indonesia to sea surface temperature (SST) changes in NINO-4 region (Strength of La Ni√±a signal)\nData: Precipitation (https://catalogue.ceda.ac.uk/uuid/89e1e34ec3554dc98594a5732622bce9) and SST (https://psl.noaa.gov/gcos_wgsp/Timeseries/). Tools: R Statistics\n\nDay 13 of #30DayMapChallenge ‚Äì Raster\nLandsat 8 RGB true-color composite. Path/Row 119/65.\nBy using ImageMagick (free and open-source cross-platform software suite for displaying, creating, converting, modifying, and editing raster images), to get something that looks like land looks, we need to increase both brightness and contrast. Most well-known method is the -sigmoidal-contrast flag for convert, which takes a two-part argument: a scale factor for the contrast, plus the brightness value in the input image that should end up at 50% (midtone) in the output image. We can take the haze into account by lowering the blue channel‚Äôs gamma (brightness) slightly, and raising the red channel‚Äôs even less, before increasing the contrast. This gives us something that‚Äôs green where it should be green.\nData: Landsat-8 downloaded from RemotePixel - https://search.remotepixel.ca/#8.67/-7.1864/111.5247 Tools: ImageMagick\n\nDay 12 of #30DayMapChallenge ‚Äì Map not made with GIS Software\nCOVID-19 cumulatives case as of 10 Nov 2020.\nData: https://covid19.who.int/WHO-COVID-19-global-data.csv Tools: Microsoft Excel\n\nDay 11 of #30DayMapChallenge ‚Äì 3D\nPopulation movement data in a day.\nData: Facebook Population Movement. Tools: kepler.glÔªø\n\nDay 10 of #30DayMapChallenge ‚Äì Grid\nRainfall grid with 0.05 deg spatial resolution.\nData: CHIRPS rainfall grid. Tools: ArcGIS Desktop\n\nDay 9 of #30DayMapChallenge ‚Äì Monochrome\nNyepi - Day of Silence, 25 Mar 2020.\nData: VNP46A1 - The VIIRS Nighttime Imagery (Day/Night Band, Enhanced Near Constant Contrast). Tools: ImageMagick, Inkscape\n\nDay 8 of #30DayMapChallenge ‚Äì Yellow\nS1 backscatter, R co-pol (dB), G cross-pol (dB), Blue ratio.\nData: Sentinel-1. Tools: GEE\n\nDay 7 of #30DayMapChallenge ‚Äì Green\nS1 backscatter, R co-pol (dB), G cross-pol (dB), Blue ratio.\nData: Sentinel-1. Tools: GEE\n\nDay 6 of #30DayMapChallenge ‚Äì Red\nS1 backscatter, R co-pol (dB), G cross-pol (dB), Blue ratio.\nData: Sentinel-1. Tools: GEE\n\nDay 5 of #30DayMapChallenge ‚Äì Blue\nS1 backscatter, R co-pol (dB), G cross-pol (dB), Blue ratio.\nData: Sentinel-1. Tools: GEE\n\nDay 4 of #30DayMapChallenge ‚Äì Hexagons\nRainfall exceeding the threshold for the period 25 Dec 2019 - 4 Jan 2020.\nData: GPM IMERG, Final Daily. Tools: kepler.gl\n\nDay 3 of #30DayMapChallenge ‚Äì Polygons\nAdapted from Philippe Rivi√®re works on the Martinez-Rueda-Feito polygon-clipping algorithm.\nData: Natural Earth 1:110m Physical Vectors, Land polygons including major islands. Tools: Observable\n\nDay 2 of #30DayMapChallenge ‚Äì Lines\nThe below lines is actually a ~21,237 km car route from Cape Town, South Africa to Singapore on a world scale.\nData: Generated by the GraphHopper Directions API based on OpenStreetMap data. Tools: QGIS, GraphHopper\n \nDay 1 of #30DayMapChallenge ‚Äì Points\nPopulation change and movement in Indonesia\nBased on Facebook population data, between 31 Mar 2020 and 24 Apr 2020, there were reductions in the population of Facebook users in major urban areas throughout the country. Concurrently, peri-urban and rural areas have observed increases in Facebook user population during the period.\nData: Facebook Population Movement. Tools: kepler.gl\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240523-xkcd-style-for-lseqm-illustration.html",
    "href": "blog/20240523-xkcd-style-for-lseqm-illustration.html",
    "title": "xkcd style for LSEQM illustration",
    "section": "",
    "text": "Source: https://gist.github.com/bennyistanto/b9e5d9c932dc6b034f559deaa26e2743\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230825-a-certified-gisp.html",
    "href": "blog/20230825-a-certified-gisp.html",
    "title": "A certified GISP",
    "section": "",
    "text": "Exciting news! I‚Äôm now a certified GIS Professional (GISP)\nBig thanks to GIS Certification Institute (GISCI) for this recognition. To learn more about the GISP certification process, visit https://www.gisci.org\nShoutout to my mentors at the WBG, Keith Garrett and Ben Stewart, for their guidance in the last 2 years.\nLooking forward to doing more meaningful work in climate analytics and geospatial technology for greater impact! üó∫Ô∏è\n\nUpdate: happy to received the certificate at the end of 2023\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20190422-surface-buffer.html",
    "href": "blog/20190422-surface-buffer.html",
    "title": "Surface buffer",
    "section": "",
    "text": "The current state of a volcano is extremely variable with time and it has no meaning to define fixed risk zones around volcanoes (e.g.¬†‚Äúfrom 5 to 10km from the crater‚Äù)\nCurrently, during the emergency situation in volcano eruption case, most of decision maker with help from the technical team define the safe zone from the crater without considering the surface (blue line in below picture).\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1 . Distance from a location [/caption]\nDespite there is no default distance for volcanic safe zone, but it is important to calculate the true distance (considering the surface) from the crater ‚Äî red line on above picture. For saving more lives and better preparedness.\nUsing GIS technology, we can calculate the true distance very easily. In the ESRI GeoNet forum, there has been discussions on how to calculate a surface buffer using ArcPy. What we need is a coordinate of selected location and the surface condition from the surrounding areas, we can use free elevation data from SRTM and can be downloaded from this site: 30-Meter SRTM Tile Downloader.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 2. 12 km from the peak of Gunung Agung, Bali - Indonesia [/caption]\nPicture above is the result, red line is the true distance of 12km from the crater considering the topographic condition. You can try to calculate with your own data using below script from Xander Bakker of ESRI Colombia below:\nYou need to adjust the distance in line 9 (in meters) and working directory in line 11‚Äì13. And then you can paste the code into python GUI inside ArcGIS, or run via command prompt.\nGood luck!\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20201109-a-reason-for-being.html",
    "href": "blog/20201109-a-reason-for-being.html",
    "title": "A Reason For Being",
    "section": "",
    "text": "According to Wikipedia, Ikigai (Áîü„ÅçÁî≤Êñê) (pronounced [iki…°ai]) is a Japanese concept that means ‚Äúa reason for being‚Äù. The word refers to having a meaningful direction or purpose in life, constituting the sense of one‚Äôs life being made worthwhile, with actions (spontaneous and willing) taken towards achieving one‚Äôs ikigai resulting in satisfaction and a sense of meaning to life.\nAnd here‚Äôs my version of Ikigai:\n\nI LOVE drawing and coloring, especially a map since I was a kid.\nI am GOOD AT turning data into maps for actionable, life-saving insights.\nI work as a crisis mapper and get PAID FOR it.\nI share the ideas of helping others and contributing to humanitarian NEEDS is as simple as I do.\n\nHow about you?\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1200‚Äù] Source: https://singularityhub.com/2018/06/15/the-more-people-with-purpose-the-better-the-world-will-be/ [/caption]\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20120304-banjir-tangse-di-akhir-februari-2012.html",
    "href": "blog/20120304-banjir-tangse-di-akhir-februari-2012.html",
    "title": "Banjir Tangse di akhir Februari 2012",
    "section": "",
    "text": "Tangse adalah sebuah kecamatan di Kabupaten Pidie, Aceh, Indonesia. Tangse berada di atas ketinggian 600-1200 mdpl. Iklim yang sejuk dengan curah hujan yang tinggi kualitas tanaman terbaik karena memiliki tanah yang subur. Memiliki hasil tambang seperti emas, biji besi dll. Tangse juga mempunyai beberapa tempat wisata yang sangat unik seperti Air Terjun tepi jalan, Pemandian Air Panas, dll\nAkhir-akhir ini nama Tangse juga terrenal arena seringa terdampak banjir dan tanah longsor, superti yang terjadi semalam 27 Februari 2012.\nBersama dengan teman-teman dari Dinas Bina Marga dan Cipta Karya - Kabupaten Pidie, mengunjungi lokasi banjir dan tanah longsof sekalian membuat laporan situasi.\nFoto-foto berikut ini diambil menggunakan GPS Garmin Montana. Foto dengan geotagging juga bisa dilihat di link berikut: https://photos.app.goo.gl/3QSZPeVgEoB6EX346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20061109-sistem-peringkat-bahaya-kebakaran.html",
    "href": "blog/20061109-sistem-peringkat-bahaya-kebakaran.html",
    "title": "Sistem Peringkat Bahaya Kebakaran",
    "section": "",
    "text": "LAPAN secara rutin melakukan pemantauan Sistem Peringkat Bahaya Kebakaran (SPBK/FDRS - Fire Danger Rating System) berbasis data satelit. SPBK tersebut mengacu pada sistem Fire Weather Index (FWI) yang dibuat oleh Canada.\nSistem FWI membutuhkan masukan beberapa variabel cuaca harian: curah hujan, suhu udara, kelembaban relatif dan kecepatan angin. Untuk mendapatkan semua data tersebut, LAPAN memanfaatkan luaran dari NOAA-16, QMORPH dan TXLAPS, dan menggunakan XLFWI addins (sistem yang sama digunakan oleh BMKG) yang berjalan di Microsoft Excel untuk menghitung FWI.\nSemua data yang dihasilkan disimpan dalam bentuk file teks XYZ (1 file, 1 variabel cuaca, 1 hari), untuk memudahkan proses analisis, query dan penyimpanan.\n\n\n\n\n\n\n\n\n\n\n\nTugas pertama yang saya kerjakan ketika pertama kali bergabung sebagai asisten peneliti di bidang Pemantauan Sumber Daya Alam dan Lingkungan (PSDAL) - LAPAN adalah melakukan modifikasi GISFORESTFIRE (sistem yang saya buat ketika melakukan tugas akhir di LAPAN dan SEAMEO BIOTROP) menjadi sebuah aplikasi yang simpel dan ringan yang dapat membaca, menganalisis dan visualisasi data teks yang dibuat LAPAN untuk menghitung FWI.\nAplikasi ini hanya menggunakan 1 form, 2 modules dan 1 class modules. Saya berencana untuk menulis laporan tentang proses pembuatannya, dan mengirimkannya ke Pertemuan Ilmiah Tahunan (PIT) - Masyarakat Ahli Penginderaan Jauh (MAPIN) yang akan dilaksanakan bulan depan di Bandung.\n\nCode untuk frmNumerik (Numerik.frm)\nCode untuk modFDRS (FDRSI.bas)\nCode untuk modSamples (Samples.bas)\nCode untuk MapTip (MapTip.cls)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "",
    "text": "In the realm of meteorological studies, the use of statistical models is pivotal for understanding and predicting various weather phenomena. Among these models, the second-order Markov chain model has emerged as a powerful tool, particularly in generating time series of rainfall occurrence (Wilks, 1998). This model provides a robust framework for simulating rainfall patterns, offering valuable insights that are crucial for weather forecasting, water resource management, and climate change studies.\nThe second-order Markov chain model distinguishes itself from its first-order counterpart through its ability to consider not just the state of the system at the previous time step, but also the state at the time step before that. This additional layer of historical context allows the model to capture more complex dependencies and transitions in the rainfall data (Bellone et al., 2000). This enhanced capability significantly improves the accuracy of the generated time series, making it a powerful tool in the study of rainfall patterns.\nRainfall, as a natural phenomenon, exhibits a high degree of variability and randomness. The second-order Markov chain model, with its ability to incorporate historical context, is well-equipped to handle this variability (Hughes et al., 1999). By considering the state of the system at two previous time steps, the model can capture the inherent randomness in rainfall occurrence, thereby generating a time series that closely mirrors real-world rainfall patterns.\nThe application of the second-order Markov chain model to rainfall data is not just a theoretical exercise. The generated time series of rainfall occurrence can have practical applications in various fields. For instance, in the field of agriculture, understanding rainfall patterns can help farmers plan their planting and harvesting schedules (Rosenzweig et al., 2000). In urban planning, accurate rainfall predictions can inform the design of drainage systems to prevent flooding (Ashley et al., 2005)."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#introduction",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#introduction",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "",
    "text": "In the realm of meteorological studies, the use of statistical models is pivotal for understanding and predicting various weather phenomena. Among these models, the second-order Markov chain model has emerged as a powerful tool, particularly in generating time series of rainfall occurrence (Wilks, 1998). This model provides a robust framework for simulating rainfall patterns, offering valuable insights that are crucial for weather forecasting, water resource management, and climate change studies.\nThe second-order Markov chain model distinguishes itself from its first-order counterpart through its ability to consider not just the state of the system at the previous time step, but also the state at the time step before that. This additional layer of historical context allows the model to capture more complex dependencies and transitions in the rainfall data (Bellone et al., 2000). This enhanced capability significantly improves the accuracy of the generated time series, making it a powerful tool in the study of rainfall patterns.\nRainfall, as a natural phenomenon, exhibits a high degree of variability and randomness. The second-order Markov chain model, with its ability to incorporate historical context, is well-equipped to handle this variability (Hughes et al., 1999). By considering the state of the system at two previous time steps, the model can capture the inherent randomness in rainfall occurrence, thereby generating a time series that closely mirrors real-world rainfall patterns.\nThe application of the second-order Markov chain model to rainfall data is not just a theoretical exercise. The generated time series of rainfall occurrence can have practical applications in various fields. For instance, in the field of agriculture, understanding rainfall patterns can help farmers plan their planting and harvesting schedules (Rosenzweig et al., 2000). In urban planning, accurate rainfall predictions can inform the design of drainage systems to prevent flooding (Ashley et al., 2005)."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#data",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#data",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "2 Data",
    "text": "2 Data\nOver the past three decades, Bogor‚Äôs climate has remained relatively consistent. The city experiences an average annual temperature of around 26 ¬∞Celsius. The temperature varies little throughout the year, with the warmest month averaging around 27 ¬∞Celsius and the coolest month averaging around 25 ¬∞Celsius.\nIn terms of rainfall, Bogor receives an average annual precipitation of over 3,000 millimeters. The city experiences the most rainfall from November to March, with each of these months receiving over 300 millimeters of rain on average. Even in the driest months, from June to September, Bogor still receives over 100 millimeters of rain per month on average.\nThis consistent and significant rainfall, combined with the city‚Äôs warm temperatures, contributes to its lush, tropical environment. The climatic conditions of Bogor provide a rich dataset for the application of a second-order Markov chain model to generate time series of occurrence and rainfall.\nDaily rainfall data of Bogor Climatological Station from 1984-2021 were used in this analysis, downloaded from BMKG Data Online in *.xlsx format. The file then manipulated by remove the logo and unnecessary text, leaving only two columns, namely date in column A and rainfall in column B for the header with the format extending downwards, and save as *.csv format.\nThe final input file is accessible via this link: https://drive.google.com/file/d/1molqggv9o71Z0VT50h5OvEqCxYq4Bp1Z/view?usp=sharing"
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#methods",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#methods",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "3 Methods",
    "text": "3 Methods\nThis exercise focuses on the second-order Markov chain model as a tool for generating rainfall occurrence probabilities and the gamma distribution for determining rainfall height (Boer, 1999).\nThe second-order Markov chain model is widely used to represent rainfall occurrence (Stern and Coe, 1984; Hann et al, 1976). In this model, rainfall occurrence on day ‚Äúi‚Äù is influenced by the presence or absence of rainfall in the previous days and the day after the previous day. If rainfall on day ‚Äúi‚Äù is only influenced by rainfall on the previous day, it is considered a first-order Markov chain, and if it is influenced by rainfall two days prior, it is considered a second-order Markov chain, and so on.\nThe second-order Markov chain model has been demonstrated to be effective in generating time series of rainfall occurrence (Nick and Harp, 1980; Richardson, 1981; Wilks, 1990). Furthermore, the gamma distribution is frequently utilized to determine rainfall height (Wilks, 1990). By employing both the second-order Markov chain model and gamma distribution, this study offers a comprehensive approach to generating rainfall data.\nThe combined use of the second-order Markov chain model for rainfall occurrence and gamma distribution for rainfall height provides a robust method for generating rainfall data. This approach has significant implications for various fields, such as agriculture and urban planning, where accurate rainfall data is crucial for informed decision-making.\nIn this exercise, the focus is limited to second-order Markov chains, as the analysis for lower/higher-order chains is fundamentally similar. The analysis employs the symbol 0 for non-rainy days and 1 for rainy days. The probability of rainfall on day i, given that it did not rain on the previous day and the day before previous day, is denoted as P001(i), while the probability of rain given that it rained the previous day and the day before previous day is represented as P111(i). The general form of the estimated probability of rainfall occurrence is as follows:\n\\[P_{jkl}(i) = \\frac{n_{jkl}(i)}{n_{jk0}(i) + n_{jk1}(i)} \\tag{1}\\]\nwhere njkl(i) represents the number of years in which the event l (0 or 1) occurred on day i, and the event jk (0 or 1) happened on the previous day and the day before the previous day.\n\n3.1 Rainfall occurrence model\nRainfall occurrence models commonly use Fourier regression equations to predict the probability of rainfall occurrence. However, these equations can sometimes produce a fitting line with values greater than 1 or smaller than 0. To address this issue, the probability values are first transformed into a logit function gjkl(i).\n\\[g_{jkl}(i) = \\ln\\left(\\frac{P_{jkl}(i)}{1 - P_{jkl}(i)}\\right) \\tag{2}\\]\nTo transform gjkl(i) back into probability values, the following equation is used:\n\\[P_{jkl}(i) = \\frac{1}{1 + \\exp(-g_{jkl}(i))} \\tag{3}\\]\nThe fitting line for gjkl(i) follows the form presented by Stern and Coe (1984):\n\\[g_{jkl}(i) = a_0 + a_1 \\sin(t'(i)) + b_1 \\cos(t'(i)) + a_2 \\sin(2t'(i)) + b_2 \\cos(2t'(i)) \\tag{4}\\]\nWhere \\(t'(i) = \\frac{2\\pi i}{365}\\) and \\(i = 1, 2, \\ldots, 365\\)\nThe number of harmonics, m, can be determined using multiple regression techniques, where independent variables are introduced sequentially, starting with harmonic 1, harmonic 2, and so on until no more variance is explained by the newly introduced variable.\n\n\n3.2 Rainfall generation model\nTo generate rainfall data, the probability information required is the probability of rainfall occurrence on day i, where the previous day‚Äôs occurrence is k (0 or 1), and the day before yesterday is j (0 or 1). The estimated value for gjkl(i) can be calculated if daily rainfall observation data is available.\nFor simulation purposes, probability data must be converted into occurrence data. This is done by generating random numbers from a uniform distribution U(0, 1; VanTassel et al., 1990). If the random value from the uniform distribution is smaller than the probability value, it indicates rainfall; otherwise, it indicates no rainfall. If the simulation result indicates rainfall, the next step is to generate rainfall height using theoretical distributions.\nThe next step in creating a rainfall data simulation model is to calculate the parameters of a theoretical distribution that approximates the rainfall data distribution. The Gamma distribution is widely used to describe rainfall intensity variability (Ison et al., 1971; Stern and Coe, 1984; Waggoner, 1989; Wilks, 1990). The probability density function is as follows:\n\\[f(x, \\alpha, \\beta) = \\frac{1}{\\beta\\Gamma(\\alpha)}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-x/\\beta} \\tag{5}\\]\nwith \\(\\beta\\) being the shape parameter and \\(\\alpha\\) being the scale parameter of the gamma function \\(\\Gamma\\).\nSeveral methods can be employed to estimate the values of the two parameters of the gamma distribution, one of which is the Maximum Likelihood Method. According to Shenton and Bowman (1970, as cited in Haan, 1979), the \\(\\alpha\\) value obtained from the Maximum Likelihood Method may still have a bias, and therefore needs to be corrected. The corrected \\(\\alpha\\) value, calculated using the Greenwood and Durand method, is:\n\\[FC_\\alpha = \\frac{(n - 3)\\alpha}{n} \\tag{6}\\]\nSubsequently, the \\(\\beta\\) parameter is calculated as follows:\n\\[\\beta = \\frac{\\bar{X}}{\\alpha} \\tag{7}\\]\nThe predicted rainfall is based on a predetermined set of patterns, P001, P010, P011 and P111, referred to as P-types. These P-types represent different combinations of previous and future rainy days and are used as event triggers in the model. The rainfall data is then separated into different seasons, DJF, MAM, JJA and SON,¬† based on the month of occurrence. The model takes into account the day-to-day variability within each season.\nThe gamma distribution, parameterized by \\(\\alpha\\) (shape) and \\(\\beta\\) (scale), is used to generate the predicted rainfall. \\(\\alpha\\) and \\(\\beta\\) parameters for each season are pre calculated. These parameters are fetched for each P-type and season combination.\nThe random gamma function, employed to simulate rainfall events, generates samples from a Gamma distribution. The number of samples drawn (pertaining to the size parameter in the gamma distribution) ideally aligns with the valid event days within a given season, conforming to the original precipitation data (Wilks, 2011). In essence, for each season and event type, the gamma distribution is simulated as frequently as the number of event days occurring within the season, according to the original data.\nAlthough the simulated events from a uniform distribution and the derived rainfall values from the gamma distribution are not intrinsically connected in the simulation process, they both represent the same event category. To maintain consistency in the temporal distribution of events, the generated rainfall values are matched with the valid event days in the original data. This coherence in the number of samples drawn from the gamma distribution is accomplished by aligning it with the structure of the initial precipitation data, rather than the simulated occurrences. Consequently, the gamma distribution is simulated for as many instances as the number of simulated event days within the season, thereby aligning with the frequency of simulated events in the synthetic weather data. The methodology of generating synthetic weather data using stochastic processes is a widely recognized approach in atmospheric sciences (Rodriguez-Iturbe, Cox, & Isham, 1987; Srikanthan & McMahon, 2001).\nFor every P-type, the model iterates through each season. During each iteration, it identifies the days in the season when an event (rainfall) is predicted to occur. These are the days that have a corresponding 1 in the event data for the current P-type.\nOnce these event days are identified, the model generates rainfall values for these days using the gamma distribution with the Œ± and Œ≤ parameters for the current season. This process is repeated for all the P-types and seasons.\nThe result is a predicted rainfall dataset that takes into account the specific patterns of rainfall events and the seasonal characteristics of rainfall intensity."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#implementation",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#implementation",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "4 Implementation",
    "text": "4 Implementation\nIn the implementation phase of this analysis, we utilized Python and the Pandas, Numpy and Matplotlib library to develop a rainfall occurrence generation model.\n\n4.1 How-to?\nThe step-by-step guide for the model is readily accessible in Google Colab or Jupyter Notebook, an ideal platform for data analysis and machine learning. This comprehensive how-to guide explains the entire process, starting with reshaping the data to ensure compatibility with the model, generate transition probabilities, essential for accurate predictions, calculate the number of probabilities, followed by translating these probabilities into meaningful rainfall event information. The final step involves chart generation, effectively visualizing the results for clear interpretation and analysis.\n\nConfiguration\nConfiguration is a crucial aspect of setting up any data analysis or processing workflow. Proper configuration ensures seamless access to data, efficient execution of tasks, and smooth integration of required tools and libraries. This article covers several essential subtopics related to configuration, such as connecting Google Drive to Colab, installing packages, importing libraries, and setting up working directories.\nGoogle Drive directory into Colab\nConnecting Google Drive to Colab is a vital step when working with data stored in Google Drive. It allows us to access and manipulate files directly from our Colab notebook. To connect our Google Drive, we can use the google.colab.drive module to mount our drive, enabling seamless access to our files and folders.\nNotes\nThis is only apply if we are working in Colab\nWorking Directories\nSetting up working directories involves defining the input and output directory paths for our project. This ensures that our code knows where to find the input data and where to store the results. Properly organizing our working directories makes it easier to manage our project, share it with others, and maintain a clean and structured codebase.\n\n\n4.1.1 Rainfall categorization\nIn the first stage of the analysis, we import the data and categorize whether the day is rainy (value = 1) or sunny (value = 0).\nAbove code will produce output previews like below.\n\n\n\n4.1.2 Function for transition probabilities order 2\nThis step explain the process to calculate the transition probabilities of weather states from one day to the next, considering the weather states of the previous two days, based on historical weather data. The weather states are represented as binary values: 0 for ‚ÄúSunny‚Äù and 1 for ‚ÄúRain‚Äù. The transition probabilities are calculated for eight different scenarios:\n\nP000: The probability that today is Sunny given that the day before yesterday was Sunny and yesterday was Sunny.\nP010: The probability that today is Sunny given that the day before yesterday was Sunny and yesterday was Rain.\nP100: The probability that today is Sunny given that the day before yesterday was Rain and yesterday was Sunny.\nP110: The probability that today is Sunny given that the day before yesterday was Rain and yesterday was Rain.\nP001: The probability that today is Rain given that the day before yesterday was Sunny and yesterday was Sunny.\nP101: The probability that today is Rain given that the day before yesterday was Sunny and yesterday was Rain.\nP011: The probability that today is Rain given that the day before yesterday was Rain and yesterday was Sunny.\nP111: The probability that today is Rain given that the day before yesterday was Rain and yesterday was Rain.\n\nThe given code defines a function calculate_transition_probabilities_orders_2_long that calculates transition probabilities based on weather conditions in a DataFrame (df). The function takes three conditions (condition1, condition2, and result) and checks if these conditions are met in consecutive rows of the DataFrame. It creates a new column with binary values indicating the occurrence of the specified conditions. NaN values are set for rows with missing data. The code then defines a list of conditions and results and iterates over them to calculate transition probabilities for each scenario. The resulting probabilities are stored in new columns in the DataFrame. The DataFrame is restructured and saved as a CSV file. Finally, the program prints ‚ÄòCompleted!‚Äô and displays a preview of the DataFrame.\nAbove code will produce output previews like below.\n\n\n\n4.1.3 Reshape the data\nThe provided code segment executes a series of steps to transform weather data from long to wide format, to make easy for further process.\nFirstly, it generates a list of unique scenarios represented by ‚ÄúP‚Äù values. Subsequently, a ‚Äòyear‚Äô column is added to the DataFrame bin_df based on the ‚Äòdate‚Äô information. The code then iterates through each unique ‚ÄúP‚Äù value. For each iteration, it selects the relevant columns (‚Äòyear‚Äô, ‚Äòday‚Äô, and the current ‚ÄúP‚Äù value) from bin_df while removing rows with missing values. The ‚ÄúP‚Äù column is renamed as ‚Äòvalue‚Äô. The DataFrame is then pivoted, organizing the data with ‚Äòday‚Äô as the index, ‚Äòyear‚Äô as the columns, and ‚Äòvalue‚Äô as the values. Each resulting pivoted DataFrame is saved as a CSV file, with the file name corresponding to the current ‚ÄúP‚Äù value.\nAbove code will produce output previews like below.\n\n\n\n4.1.4 Calculate number of event\nThis code below is responsible for calculating the total number of occurrences per day for each of the eight possible weather state transitions (P000, P001, P010, P100, P110, P101, P011, P111) over the entire period of the dataset.\nIn this context, each weather state transition represents a sequence of three consecutive days. For example, P010 represents a sequence where it was sunny two days ago, rained yesterday, and is sunny today. The weather states are represented as binary values: 0 for ‚ÄúSunny‚Äù and 1 for ‚ÄúRain‚Äù.\nThe code first calculates the total number of occurrences per day for each weather state transition by summing up the values in the respective columns of the binary DataFrame (bin_bin_reshape_dfxxx). It then creates a new DataFrame (num_df) that includes these totals along with the corresponding day. This DataFrame provides a daily summary of the weather state transitions for the entire period of the dataset.\nFinally, the code saves this DataFrame to a CSV file for further analysis and previews the data. This step is crucial as it allows for the inspection of the calculated totals and ensures the data is correctly processed and ready for the next steps of the analysis.\nAbove code will produce output previews like below.\n\n\n\n4.1.5 Calculate the probabilities\nThis specific code block calculates the transition probabilities for each of the four possible weather state transitions where the current day is rainy (P001, P011, P101, P111) and another four where the current day is sunny (P000, P010, P110, P100).\nThe transition probabilities are calculated by dividing the total number of occurrences of each rainy/sunny weather state transition by the total number of occurrences of both the rainy and sunny weather state transitions for the same previous two days. For example, the transition probability P011 is calculated by dividing the total number of P011 occurrences by the sum of the total number of P011 and P010 occurrences.\nThe calculated transition probabilities are then stored in a new DataFrame (prob_df_xxxx), which also includes the corresponding day. This DataFrame provides a daily summary of the transition probabilities for the entire period of the dataset.\nFinally, the code saves this DataFrame to a CSV file for further analysis and previews the data. This step is crucial as it allows for the inspection of the calculated probabilities and ensures the data is correctly processed and ready for the next steps of the analysis.\nAbove code will produce output previews like below.\n\n\n\n4.1.6 Converting to logit function and transform back to probability value\nThe code calculates Fourier coefficients and applies a logit transformation to the probability values in a pandas DataFrame prob_df. First, it modifies prob_df, replacing any instances of 0 or 1 probabilities with a small constant epsilon or 1 - epsilon respectively. This prevents errors when applying logarithms and exponentials later in the process. The script then calculates a set of variables based on the day of the year, including trigonometric functions sin_t_prime, cos_t_prime, sin_2t_prime, and cos_2t_prime based on the day of the year scaled by 2*pi/365 to reflect the cyclical nature of the calendar.\nAfter that, the script computes the logit of the probabilities, g_a_df, which is the log of the odds ratio (i.e., the ratio of the probability of an event occurring to the probability of it not occurring). Fourier coefficients are calculated for each original column in prob_df. The Fourier series is a way to represent a function as a sum of periodic components, and in this context, it‚Äôs used to capture the cyclical patterns of the probabilities throughout the year.\nFinally, the script constructs a new DataFrame result_df that includes the original probabilities, the calculated g_a_df values, fitted g_fit values (based on the Fourier series representation), and final probabilities (the inverse logit of g_a_df). This DataFrame is saved to a CSV file and then returned for review.\nAbove code will produce output previews like below.\n\n\n\n4.1.7 Visualize the calculated logit and their fitted\nThe script visualizes the calculated logit (g) values and their fitted counterparts (g_fit) from the result_df DataFrame for both rainy and sunny day scenarios.\nThis is accomplished by setting up a 2-row, 4-column grid of subplots. In the first row, it plots the rainy day scenarios (P_types_rainy) and in the second row, it plots the sunny day scenarios (P_types_sunny). For each scenario (rainy or sunny) and each type of day (defined by P_types), it creates a scatter plot of ‚Äòg‚Äô values and overlays a line plot of ‚Äòg_fit‚Äô values over the course of the year (represented by the ‚Äòday‚Äô variable).\nThe script then labels each subplot with its respective day type and scenario, sets the x and y labels, and includes a legend indicating which points represent ‚Äòg‚Äô and which line represents ‚Äòg_fit‚Äô.\nFinally, it adjusts the layout for better visualization and displays the plot. This way, it helps to analyze how well the fitted values (g_fit) are approximating the calculated logit (g) values.\n\n\n4.1.8 Generate random numbers from a uniform distribution to get the rainfall events\nThis code generates random numbers from a uniform distribution for each day and compares these to our probabilities to generate the events. Events are coded as 1 for rain and 0 for no rain. The new DataFrame event_df only contains the event data, with columns named event_Pxxx as specified. The data is saved in a CSV file called events.csv.\nAbove code will produce output previews like below.\n\n\n\n4.1.9 Visualize the probability of rainfall occurrence\nThe given script produces a set of heatmaps to visualize event data related to different scenarios of rainfall given that the current day is rainy. The data, divided by months and days, represents whether it‚Äôs a rainy day (indicated by a color) or a sunny day (represented by a white block).\nA heatmap is an apt choice of visualization here as it allows for an immediate visual assessment of patterns and trends in the data over a period of time (in this case, over the days of each month). Moreover, the color contrast between rainy and sunny days helps to easily distinguish between the two events. Heatmaps also excel at handling and displaying data over two dimensions (months and days, in this context), making them a clear choice for this kind of data presentation.\nFirstly, the code defines different types of events (represented as ‚ÄòP_types‚Äô), the layout for the subplots, and the number of days in each month (accounting for leap years).\nThen, it loops over each event type, creating a 2D array filled with NaNs to hold the event data for each day of each month. The event data is split by month and filled into this array, ensuring the correct day and month placement for each event.\nNext, a heatmap for each event type is generated using seaborn, with a color scheme denoting the presence or absence of rainfall, and an outline for each day block to enhance readability. The heatmap‚Äôs axes and title are customized for each scenario.\nA legend is also created to indicate the meanings of the colors in the heatmaps. The code finally adds a main title for the set of heatmaps, adjusts the layout for clear viewing, and displays the visualizations.\nBefore running below code, please make sure yopu already have ‚Äúseaborn‚Äù installed. If not, please install it using ‚Äúpip install seaborn‚Äù\n\n\n4.1.10 Gamma distribution\nThis code analyses a dataset of rainfall patterns. It first loads the data, and prepares it by converting the ‚Äòdate‚Äô column into a datetime format and adding a ‚Äòmonth‚Äô column. It then assigns each entry to a season (DJF, MAM, JJA, or SON) based on the month of the year. After isolating only the rainy days, the script applies a Gamma distribution model for each season‚Äôs rainfall data. The parameters (alpha and beta) of the Gamma distribution for each season are corrected for small sample sizes using the Greenwood and Durand method. These corrected parameters are then stored in a new DataFrame, which is exported as a CSV file for future use or analysis. The resulting DataFrame provides a seasonal breakdown of the rainfall data, and offers insights into how the rainfall pattern is distributed for each season.\nAbove code will produce output previews like below\n\n\n\n4.1.11 Generate rainfall value\nNow that we have estimated the parameters for the gamma distribution for each season, and have generated event data, we can generate rainfall values based on these parameters and events.\nThe gamma distribution is only used to generate rainfall values for rainy days (where event = 1), as it is typically used to model positive continuous data, and cannot generate the zero values corresponding to non-rainy days.\nIn this script, we create a new rainfall_PXXX column for each event_PXXX column. For each season, we select the days where event_PXXX = 1, and generate rainfall values for these days using the gamma distribution with the corresponding alpha and beta parameters. These generated values are then stored in the rainfall_PXXX column. At the end, the updated DataFrame is saved to a new CSV file.\nHere‚Äôs how we could do this for each P-type.\nAbove code will produce output previews like below.\n\n\n\n\n4.2 Evaluations\nEvaluating the quality of our predicted rainfall values depends on the specific goals of our analysis and the characteristics of our data. However, here are several common methods for evaluating prediction quality.\n\n4.2.1 Visualize the rainfall compared to predicted rainfall\nThe given script produces a set of maps to visualize rainfall data compared to predicted rainfall different scenarios of rainfall given that the current day is rainy.\nThis code is meant to load, process, and plot data on annual rainfall and rainfall predictions from the years 1984 to 2021.\nIt initializes a plot with 10 rows and 4 columns to make room for a line plot for each year from 1984 to 2021. Each plot will compare actual rainfall (in light blue) with the predicted rainfall (in orange) over the course of a year.\n\n\n4.2.2 Performance\nDistribution of Errors (Residuals): We can plot a histogram or a Kernel Density Estimate plot of the residuals, which are the differences between the actual and predicted values. If our model is a good fit, the residuals should be normally distributed around zero.\nTime Series of Residuals: Plotting residuals over time can show whether the errors are consistent throughout the time series, or if they vary significantly at certain time periods.\nBoxplot of Errors by Year: This can help us see if the model‚Äôs performance varies significantly from year to year."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#results",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#results",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "5 Results",
    "text": "5 Results\nWe delve into a comprehensive analysis of rainfall prediction and its various aspects. By examining the curve adjustment chart and transforming probabilities into rainfall events, we gain insights into the predicted outcomes. Furthermore, we assess the performance of these predictions using visual comparisons, distributed errors (residuals), time series of residuals, and boxplot of error by year. This chapter aims to elucidate the accuracy and reliability of our rainfall prediction model.\n\n5.1 Adjustment curve\nThe scatter plot visualizes the adjustment curve for generating daily rainfall data using Fourier regression analysis. The data spans from 1984 to 2021. Each subplot corresponds to different weather patterns, characterized by the variables ‚ÄòP001‚Äô, ‚ÄòP011‚Äô, ‚ÄòP101‚Äô, ‚ÄòP111‚Äô, ‚ÄòP000‚Äô, ‚ÄòP010‚Äô, ‚ÄòP100‚Äô, and ‚ÄòP110‚Äô.\nThe top row of plots shows the fitting model for rainy days (‚ÄòP001‚Äô, ‚ÄòP011‚Äô, ‚ÄòP101‚Äô, ‚ÄòP111‚Äô). Here, the patterns in the fitted models (g_fit) align with the data generated by g_a, indicating that the Fourier model accurately captures the distribution pattern of rainfall across different types of rainy day events.\nThe second row presents the fitting model for dry days (‚ÄòP000‚Äô, ‚ÄòP010‚Äô, ‚ÄòP100‚Äô, ‚ÄòP110‚Äô). In these plots, the peak of the dry season, occurring in the June-July-August (JJA) period, is prominently reflected in the peak of the g_fit line plot. Conversely, the rainfall is lowest during this period, which is depicted as a valley in the model.\n\nAbove visualization effectively demonstrates the application and accuracy of the Fourier regression analysis in modeling and simulating daily weather patterns, both for rainy and dry conditions, over a significant period. The g_fit line plots accurately reflect the distribution patterns of the original data (g), implying that the Fourier model is a suitable tool for simulating these weather patterns.\n\n\n5.2 Transforming the probability into rainfall event\nThe image is a set of four heatmaps, each representing a different scenario: ‚ÄòP001‚Äô, ‚ÄòP011‚Äô, ‚ÄòP101‚Äô, and ‚ÄòP111‚Äô. These scenarios are likely representative of different weather conditions or patterns. Each heatmap shows the pattern of rainfall across a year. The x-axis denotes the day of the month while the y-axis represents the month itself, ranging from 1 (January) to 12 (December).\nThe color intensity in each cell indicates the probability of rainfall. Darker shades symbolize a higher likelihood of rain, while lighter shades indicate a lower likelihood. This color gradient allows us to visually comprehend the variability and seasonality of rainfall across different periods of the year.\n\nFrom these heatmaps, one can observe the days and months when rainfall is more or less likely, given that the day is classified as ‚Äòrainy‚Äô. These visualizations provide an intuitive understanding of rainfall patterns and their variations throughout the year for each respective scenario.\n\n\n5.3 Predicted rainfall\nThe daily rainfall generated by the Fourier regression model and compared with the daily observation data from the Bogor Climatology Station from 1984-2021 in the image below (example using Year 2008-200- and 2012-2013) indicates that the predicted rainfall shows rainfall values produced by the model are higher than the observation data (overrated) with a pattern that tends to be somewhat dissimilar.\n\n\n\n5.4 Performance\nEvaluating the quality of our predicted rainfall values depends on the specific goals of our analysis and the characteristics of our data. However, here are several common methods for evaluating prediction quality:\nVisual comparison: It‚Äôs a plot of the predicted values against the observed values. This can give us a quick, intuitive sense of how closely our predictions match the actual values. While visually comparing the predicted and actual rainfall data is important and necessary, it is not sufficient on its own to evaluate the performance of the prediction model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Errors (Residuals): This plot shows the distribution of residuals (errors), which are the differences between the predicted and actual rainfall values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the context of rainfall prediction, if the residuals are normally distributed and centered around zero, it indicates that your model has made errors that are random and not biased, which is a good sign. If the distribution is not centered around zero or is highly skewed, it indicates that your model may be consistently overestimating or underestimating the rainfall.\nTime Series of Residuals: This plot shows how residuals change over time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should expect to see no clear pattern in the residuals over time. If we see patterns, such as the residuals increasing or decreasing over time, it suggests that our model is not capturing some trend in the data. This could indicate a problem with our model that needs to be addressed.\nBoxplot of Error by Year: This plot shows the distribution of residuals for each year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis can help you understand if your model‚Äôs performance is consistent over time. If some years have much higher or lower residuals, it may indicate that those years had unusual rainfall patterns that your model didn‚Äôt capture. You may want to investigate further to understand what‚Äôs causing these discrepancies."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#conclusion",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#conclusion",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nMarkov-chain models, when combined with Fourier regression equations and logit transformations, can be useful in estimating rainfall occurrence probabilities and generating synthetic rainfall data. This generated data can have practical applications in various fields, such as agriculture and urban planning, where accurate rainfall data is crucial for informed decision-making."
  },
  {
    "objectID": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#references",
    "href": "blog/20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.html#references",
    "title": "Second-order Markov chain model to generate time series of occurrence and rainfall",
    "section": "7 References",
    "text": "7 References\nAshley, R. M., Balmforth, D. J., Saul, A. J., & Blanskby, J. D. (2005). Flooding in the future‚Äìpredicting climate change, risks and responses in urban areas. Water Science and Technology, 52(5), 265-273. https://doi.org/10.2166/WST.2005.0142\nBellone, E., Hughes, J. P., & Guttorp, P. (2000). A hidden Markov model for downscaling synoptic atmospheric patterns to precipitation amounts. Climate Research, 15(1), 1-12. https://www.jstor.org/stable/e24867295\nBoer, R., Notodipuro, K.A., Las, I. 1999. Prediction of Daily Rainfall Characteristics from Monthly Climate Indices. RUT-IV report. National Research Council, Indonesia.\nCho, H., K. P. Bowman, and G. R. North. 2004. A Comparison of Gamma and Lognormal Distributions for Characterizing Satellite Rain Rates from the Tropical Rainfall Measuring Mission. J. Appl. Meteor. Climatol., 43, 1586‚Äì1597. https://doi.org/10.1175/JAM2165.1\nHughes, J. P., Guttorp, P., & Charles, S. P. (1999). A non-homogeneous hidden Markov model for precipitation occurrence. Journal of the Royal Statistical Society: Series C (Applied Statistics), 48(1), 15-30. https://doi.org/10.1111/1467-9876.00136\nRodriguez-Iturbe, I., Cox, D. R., & Isham, V. (1987). Some models for rainfall based on stochastic point processes. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 410(1839), 269-288. https://doi.org/10.1098/rspa.1987.0039\nRosenzweig, C., Tubiello, F. N., Goldberg, R., Mills, E., & Bloomfield, J. (2002). Increased crop damage in the US from excess precipitation under climate change. Global Environmental Change, 12(3), 197-202. https://doi.org/10.1016/S0959-3780(02)00008-0\nSrikanthan, R., & McMahon, T. A. (2001). Stochastic generation of annual, monthly and daily climate data: A review. Hydrology and Earth System Sciences Discussions, 5(4), 653-670. https://doi.org/10.5194/hess-5-653-2001\nStern, R. D., & Coe, R. (1984). A model fitting analysis of daily rainfall data. Journal of the Royal Statistical Society. Series A (General), 147(1), 1-34. https://doi.org/10.2307/2981736\nVanTassell, L.W., J.W. Richardson and J.R. Conner. 1990. Simulation of meteorological data for use in agricultural production studies. Agric. System 34:319-336. https://doi.org/10.1016/0308-521X(90)90011-E\nWaggoner, P.E. (1989). Anticipating the frequency distribution of precipitation if climate change alters its mean. Agric. For. Meteor. 47:321-337. https://doi.org/10.1016/0168-1923(89)90103-2\nWilks, D. S. (1990). Maximum likelihood estimation for the gamma distribution using data containing zeros. Journal of Climate, 3(12), 1495-1501. https://doi.org/10.1175/1520-0442(1990)003%3C1495:MLEFTG%3E2.0.CO;2\nWilks, D. S. (1998). Multisite generalization of a daily stochastic precipitation generation model. Journal of Hydrology, 210(1-4), 178-191. https://doi.org/10.1016/S0022-1694(98)00186-3\nWilks, D. S. (2011). Statistical methods in the atmospheric sciences (Vol. 100). Academic press. https://doi.org/10.1016/C2017-0-03921-6"
  },
  {
    "objectID": "blog/20201210-geotiff-to-netcdf-file-with-time-dimension-enabled-and-cf-compliant.html",
    "href": "blog/20201210-geotiff-to-netcdf-file-with-time-dimension-enabled-and-cf-compliant.html",
    "title": "GeoTIFF to NetCDF file with time dimension enabled and CF-Compliant",
    "section": "",
    "text": "UPDATE: as of 4 January 2024\n\n[15 July 2023] I made a revision to below script, to prevent the result shifted half pixel. Please use the notebook from here: https://gist.github.com/bennyistanto/a8aec5ce3130b13c48609a43daa8bc57\n[4 January 2024] I also make other version that could handle daily or time series data from multi year period, grouped by year, process each year. For this purpose, please use this notebook: https://gist.github.com/bennyistanto/0df25c2f8b453dc1f824e7628d51e605\n\n\nFew months ago, I wrote a step-by-step guideline on how to calculate SPI using CHIRPS data. New problem arises when I try to re-run SPI calculation using latest data, I didn‚Äôt imagine before that this process was so complicated. Several questions and idea came up in my mind:\n\nThe latest rainfall data is available for this month (3 files in total), in separate GeoTIFF file. They provide netCDF format only for the whole period and yearly data.\nShould I re-run for the whole periods, 1981 to date? That‚Äôs not practical as it required lot of storage and time processing if you do for bigger coverage (country or regional analysis).\nI have try to manually convert GeoTIFF file to netCDF using GDAL: gdal_translate -of netCDF input.tif output.nc, it‚Äôs worked. But the time dimension is not enabled. While SPI script will read the netCDF input file based on time dimension. It means SPI script will not work with this solution.\nUsing NCO, I can easily rename any variable with ncrename and add attributes to the content with ncatted. But I think this is too much work if every time I update the data, then I need to update the attribute.\nAfter discussed this problem with friends, finally I found the solution. I used Rich Signell‚Äôs script on StackExchange: https://gis.stackexchange.com/a/70487 and adjust few code so can works using CHIRPS dekad data\n\nThis script will open possibility to create netCDF data using various data, i.e.¬†MODIS Land Surface Temperature (MOD11C3), I can use this data to calculate Potential Evapotranspiration (PET) then the Standardized Precipitation Evapotranspiration Index (SPEI).\nSee below.\nAdjustment is needed if using other CHIRPS timesteps (daily, monthly, etc.) data. And NCO must be installed before using this script.\nPython script available in my Gist: https://gist.github.com/bennyistanto/1cf46ed00c2060a5c9d4182bb4da2e54\nAnd below is the notebook version.\nI tested using CHIRPS dekad data from Dekad 1, May 2020 to Dekad 3, Oct 2020. And below is the result checked using ncdump.\n\nAnd visualize the result using Panoply\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230525-impact-of-climate-change-in-cities.html",
    "href": "blog/20230525-impact-of-climate-change-in-cities.html",
    "title": "Impact of climate change in cities",
    "section": "",
    "text": "A new World Bank report is launched, in which I had the opportunity to contribute the analysis.\nThe report examining the two-way relationship between cities and climate change, including valuable insights to help cities boost resilience and thrive, both now and in the future.\nThe team are utilizing temperature and precipitation based index: SPEI, SPI, CDD, CWD, number of annual hotdays and annual mean temperature, distance and magnitude of tropical cyclone to the city center, to support the analysis which organized into four inter-related workstreams:\n\nWho is affected?\nStressor that make urban development less green.\nStressors that make urban development less resilient.\nStressors that make urban development less inclusive.\n\nIf you are interested to read the story and report, feel free to visit below:\n\nStory: https://www.worldbank.org/en/publication/thriving\nRecorded of Live event:https://live.worldbank.org/events/thriving-making-cities-climate-ready\nPublication: https://openknowledge.worldbank.org/entities/publication/7d290fa9-da18-53b6-a1a4-be6f7421d937\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200102-jakarta-flood-2020.html",
    "href": "blog/20200102-jakarta-flood-2020.html",
    "title": "Jakarta flood 2020",
    "section": "",
    "text": "[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. 1-day rainfall [/caption]\nKicking off 2020 with massive flood in Greater Jakarta. NASA‚Äòs GPM satellite captured heavy rain of at least 300 mm pouring over East Jakarta and Bekasi nonstop from new years eve up until the morning.\nOther surrounding areas experienced 100-160mm/day of rainfall. What‚Äòs most to blame? Upstream Ciliwung river of course üò¨. Yet, local rainfall in the area was only 60 mm.\nLet‚Äôs check rainfall intensity per 30-minutes from the same source, NASA‚Äôs GPM.\nIf you are curious every line in the chart referring to which areas, then you should check below map index.\nThe highest rainfall around 370 mm was fall in East Jakarta and Kota Bekasi, compared to official BMKG‚Äôs released from weather station around 340 mm.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230210-install-the-wrf-model-in-wsl2.html",
    "href": "blog/20230210-install-the-wrf-model-in-wsl2.html",
    "title": "Install the WRF Model in WSL2",
    "section": "",
    "text": "UPDATE: 25 Feb 2024\nOld version: https://gist.github.com/bennyistanto/fb5dcfb1f9d9b3b8745e199523368a62/d03e0e0a9517bebe19d716843f0f704bdfb6821e\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230215-sbr-market-day-2023-when-creativity-and-demon-slayer-colors-led-to-success.html",
    "href": "blog/20230215-sbr-market-day-2023-when-creativity-and-demon-slayer-colors-led-to-success.html",
    "title": "SBR Market Day 2023: When Creativity and Demon Slayer Colors Led to Success",
    "section": "",
    "text": "My fourth-grade daughter recently participated in her school‚Äôs market day as part of the IB Primary Years Programme at Sekolah Bogor Raya, and it turned into an inspiring journey of creativity, entrepreneurship, and passion. She transformed her love for anime ‚Äì particularly Demon Slayer ‚Äì into a wonderful entrepreneurial adventure.\nWhile many of her classmates chose to sell food items, my daughter envisioned something unique. As an enthusiastic Demon Slayer fan with the complete English manga series from VIZ (volumes 1-23) and a love for the Netflix series, she decided to blend her artistic talents with her favorite anime. Her creative spark led to the idea of hand-painted Demon Slayer-themed artwork on small canvases.\nSupporting her vision, we visited KKV a month before the market day to gather supplies: 20 small 10x10 cm canvases, acrylic paints, and various art materials. Together, we explored different color patterns that would bring her beloved characters to life through art.\nThe Production Journey\nOur creative process was both fun and educational. We began with the fundamentals ‚Äì applying white (or other color, depending on the character) basecoats to all canvases before letting them dry in the warm sunlight. I guided her in drawing pencil guidelines to create clean color boundaries.\nThe color mixing process became an exciting learning experience as she applied her mathematical knowledge of proportions and ratios, combining digital inspiration from colordesigner.io with hands-on acrylic paint mixing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe set a comfortable pace of completing five artworks weekly, aiming for one piece per day. Though the initial week moved slowly as we focused on base layers and pencil sketches, her enthusiasm and joy made every moment worthwhile.\nAdding a Personal Touch\nA week before the event, we celebrated completing all the artworks ‚Äì a proud moment for our family.\nDrawing inspiration from Starbucks Reserve‚Äôs storytelling approach, we enhanced each piece with complimentary cards explaining the color patterns‚Äô significance. She embraced this idea warmly, diving into research using the Demon Slayer Wiki Fandom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe final collection showcased 15 distinctive color patterns, each reflecting a different Demon Slayer character‚Äôs essence. From Tanjiro‚Äôs signature green and black checkered pattern to Zenitsu‚Äôs vibrant yellow and orange lightning motif, and Inosuke‚Äôs dynamic blue beast-like design ‚Äì every piece told its own story. The collection featured other beautiful interpretations including Shinobu‚Äôs elegant purple wisteria pattern, Rengoku‚Äôs energetic red and orange, and Giyu‚Äôs peaceful water-ripple blues. Each pattern thoughtfully represented the character‚Äôs unique breathing style and personality, creating an instant connection with fellow fans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt school, she collaborated wonderfully with her classmates, creating engaging posters and refining plans under Pak Chandra and Ibu Lucia‚Äôs supportive guidance.\n\nHer pricing strategy reflected careful thought ‚Äì IDR25,000 per piece, offering great value with each purchase including a magnetic canvas (perfect for refrigerator display), a canvas stand, story card, and paper bag. She mindfully calculated production costs at around IDR18,000 per piece.\n \nMarket Day Success\nWhat happened on market day exceeded everyone‚Äôs expectations. Her entire collection sold out in just 20 minutes ‚Äì the fastest-selling items at the event! She was genuinely shocked by the response, with students continuing to visit her stall asking for more. We even heard that some students successfully resold the pieces at double the price ‚Äì a testament to their popularity!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond Sales: A Learning Experience\nThis market day blossomed into something far more meaningful than an art sale. It became a beautiful example of practical learning across various subjects and skills:\n\nMathematical skills through pricing and profit calculations\nResearch abilities through market surveys and price analysis\nTime management in production planning\nFinancial understanding through proposal writing\nMarketing and communication skills\nProduct development and critical thinking\nReport writing and thoughtful reflection\n\nAs parents, watching our daughter apply her classroom knowledge to a real-world project filled us with joy. She didn‚Äôt just create and sell artwork ‚Äì she experienced the full journey of entrepreneurship while staying true to her interests. The success of her Demon Slayer artwork proved that when passion meets preparation, amazing things can happen.\nAwesome El!\n\n\n\n\n\n\n\n\n\n\n\nCredits: Some pictures are courtesy of Pak Chandra, Ibu Lucia, and SBR.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180911-dssat-training-in-chiang-mai-2018.html",
    "href": "blog/20180911-dssat-training-in-chiang-mai-2018.html",
    "title": "DSSAT training in Chiang Mai, 2018",
    "section": "",
    "text": "Last month, I attended training on DSSAT and MWCropDSS: Efficient and precision agricultural resource utilization under changes with simulation models and GIS in Chiang Mai, Thailand from 27 Aug to 1 Sep.¬†The goal of the trainings is to gain better understanding of simulation models in predicting relationships of agricultural resources, i.e., water and nitrogen under various weather and climate conditions, on crop phenology and growth.\n\nThere are 24 participants from Bhutan, China, Indonesia, Japan, Nepal and Thailand. And the trainer are Prof.¬†Gerrit Hoogenboom from University of Florida, USA and Prof.¬†Attachai Jintrawet from Chiang Mai University.\nHere‚Äôs some topic I learned during the training.\n1st day\n\nHistory and overview of DSSSAT\nExercises: Running Crop Models\nSimulating phenological development\nSensitivity analysis tool\nExercises: Sensitivity Analysis Tools\nCreating FileX: Potential crop production\nExercises: Potential Crop Production\n\n\n\n\n\n\n\n\n\n\n\n\n2nd day\n\nSimulating basic growth processes\nWeather data input and utilities\nExercises: Weather Data Files\nMinimum dataset concept\nLearning the DSSAT file system\nConcept of genetic coefficients species vs ecotype vs cultivar coefficients\nGenetic coefficients - CROPGRO and CERES\nExercises: Cultivar Sensitivity Analyses\nEstimatic genetic coefficient, concepts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3rd day\n\nEstimating Genetic Coefficients, Concepts Tools for Estimating Cultivar Coefficients\nCultivar Coefficient Calibration using the Glue Tool\nSimulating Water Limited Production Soil and Flood Water Balance in Rice\nSoil Data Inputs and Utilities\nExercises: Soil Data Files\nCreating FileX: Water Balance On\nExercises: Water Limited Production\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4th day\n\nExperimental Data Collection - Model Evaluation\nExperimental Data Files and Utilities\nExercises: Experimental Data Files\nSimulating Nitrogen Limited Production Processes in the Soil\nSimulating Nitrogen Limited Production Processes in the Plant\nCreating FileX: Water and N Balance On\nExercises: Nitrogen Limited Production\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5th day\n\nSpatial Modeling Applications Demonstration: MWCropDSS\nUncertainty, Risk, BMPs, and Sustainability\nCreating FileX: Seasonal Analysis\nExercises: Seasonal Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6th day\n\nCropping Systems ‚Äì Simulating Crop Rotations\nCreating FileX: Rotation/Sequence Analysis\nGroup Discussion of Applications and Needs\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20050610-magang-di-tisda-bppt.html",
    "href": "blog/20050610-magang-di-tisda-bppt.html",
    "title": "Magang di TISDA, BPPT",
    "section": "",
    "text": "Pada tanggal 27 Juni 2004, saya memulai magang di Laboratorium Teknologi Geosystem, Pusat Pengkajian dan Penerapan Teknologi Inventarisasi Sumber Daya Alam (P3-TISDA), BPPT dibimbing oleh Pak Agus Wibowo yang saat itu menjabat sebagai Ketua Kompetensi Inti - Sistem Informasi Geografis di Laboratorium TISDA.\nIdealnya magang dilakukan selama 2 bulan penuh dimasa liburan semester 6 ke 7. Jadwal magang cukup teratur dengan progress cukup lumayan. Saya mendapatkan banyak ilmu selama magang di TISDA. Tapi proses penulisan laporan yang memakan waktu cukup lama, baru selesai setahun kemudian, tepatnya 3 Juni 2005. Berikut adalah ringkasan mengenai apa yang saya kerjakan selama magang di TISDA.\n\nDesain software pengolahan dan penyajian peta tematik\nDunia sistem informasi banyak terdapat model sistem informasi yang bertujuan akhir memberi berbagai macam informasi. Model sistem informasi juga diharapkan dapat digunakan sebagai alat prediksi kejadian di masa depan dengan berdasar pada data yang ada pada masa lalu dan masa sekarang.\nDengan menggunakan data iklim kemudian mengolah dan menyajikan dalam bentuk peta tematik dapat diketahui informasi yang berguna mengenai berbagai proses penyebab iklim untuk menjawab banyak masalah praktis yang berkaitan dengan kondisi di waktu yang akan datang.\nPemetaan tematik murni otomatik nampaknya sulit dilakukan, karena faktor manusia merupakan hal yang esensial dan berubah-ubah dalam interpretasi visual terhadap variasi bentang alam setempat.\nSarana yang diperlukan untuk menunjang kegiatan tersebut salah satunya dengan menggunakan teknologi Sistem Informasi Geografis [SIG]. Data spasial dari penginderaan jauh dan survei terestrial tersimpan dalam basis data yang memanfaatkan teknologi komputer digital untuk pengelolaan dan pengambilan keputusannya.\nUntuk mempermudah dalam penyampaian informasi yang berguna mengenai berbagai proses penyebab iklim maka disusunlah sebuah program yang berbasis SIG dengan nama Spatial Information System of Rainfall . Program ini didesain untuk menyajikan suatu peta tematik berbasis sistem informasi geografis dari hasil pengolahan data iklim. Konsep yang digunakan dalam Spatial Information System of Rainfall [SISR] adalah mengubah data iklim suatu daerah (dalam kasus ini hanya menggunakan data curah hujan bulanan) menjadi sebuah informasi spasial maupun non spasial.\nAnalisis spasial dalam SIG dapat dilakukan secara multi temporal dengan menggunakan data multi waktu. Perkembangan antar waktu dari beberapa data tersebut menjadi dasar analisis kemungkinan yang akan terjadi pada masa depan. Analisis ini akan memberi penjelasan tentang sesuatu yang mungkin akan terjadi di masa mendatang dengan penggambaran lokasi di mana fenomena tersebut akan terjadi.\nPola sebuah fenomena dapat dilihat dari sebarannya secara spasial. Sebuah kawasan dapat dilihat bentuk pola curah hujannya dengan melihat bagaimana sebaran curah hujan yang ada di kawasan tersebut. Dengan mengetahui pola-pola suatu fenomena secara spasial, dapat dicari korelasinya dengan fenomena lain seperti pola suhu udara, sebaran angin, dan lain-lain.\nHasil interpretasi dari sebuah fenomena dapat melalui peta tematik yang berbentuk statis maupun dinamis. SISR mampu memberikan informasi keadaan iklim suatu daerah berdasarkan data curah hujan yang dimiliki, sedangkan secara spasial mampu memberikan informasi (Gambar 1) yang berguna mengenai berbagai proses penyebab iklim untuk menjawab banyak masalah praktis yang berkaitan dengan kondisi di waktu yang akan datang.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Gambar 1. Form utama dari SISR [/caption]\nSISR memerlukan input berupa data curah hujan dengan resolusi bulanan. Setelah semua data curah hujan diolah menjadi peta Isohyet menggunakan extension Spatial Analyst yang ada pada ArcView, maka untuk memudahkan pemakai mengetahui distribusi curah hujan yang terjadi di semua stasiun, diperlukan suatu map display yang disusun menggunakan MapObject yang dikombinasikan dengan bahasa pemrograman Visual Basic. Desain SISR ini dibangun dengan bantuan CorelDRAW Graphics Suite 12. Hal ini dimaksudkan agar pengguna dapat melakukan pemilihan menu dalam aplikasi dengan mudah karena media penyaji ini dibangun sebagai aplikasi yang memiliki sifat kemudahan dan interaktif.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1632‚Äù] Gambar 2. Info peta yang ada didalam SISR [/caption]\nHasil pengolahan dari SISR ditampilkan dalam tiga bentuk yaitu grafik, tabel dan peta. Dalam kasus ini dipilih Propinsi Sumatera Barat sebagai objek kajian. Karena letak Sumatera Barat yang membentuk sudut 45¬∫ terhadap garis lintang 0¬∫ menyebabkan wilayah ini dilintasi DKAT (Daerah Konvergensi Antar Tropis) sebanyak dua kali (bulan April dan Nopember) dan mempengaruhi peredaran angin setempat. Keberadaan Samudera Hindia disebelah barat yang berinteraksi dengan topografi wilayah yang bervariasi menjadikan iklim Sumatera Barat bersifat spesifik. Kondisi cuaca dan iklim di suatu wilayah dipengaruhi oleh faktor internal (lokal) dan faktor eksternal (regional dan global). Faktor lokal terbentuk karena posisi geografi dan topografi wilayah.\nInformasi yang dapat diperoleh dari SISR ini pengguna dapat mengetahui distribusi curah hujan tiap stasiun dan perubahannya di seluruh Sumatera Barat. Secara spasial, output dalam bentuk peta memudahkan pengguna melihat suatu wilayah yang dianalisis. Hasil pengolahan untuk peta tematik statis ini berupa peta tematik iklim dalam bentuk hard copy dan soft copy. Data historis digunakan untuk pembuatan peta tematik statis dan desain peta tematik. Data historis adalah data yang telah terkumpul sampai dengan tahun 2000 dan telah dilakukan cek [quality control].\nSedangkan pengolahan untuk peta tematik dinamis berupa peta tematik iklim dinamis yang terupdate sesuai dengan data yang telah diperbaharui. Data near real time merupakan data yang digunakan untuk update peta tematik dinamis. Frekuensi updating pada tahap ini masih pada skala waktu bulanan. Entry data dilakukan secara manual dengan format standard sesuai dengan jenis datanya. Untuk keperluan quality control, pada saat entry manual dilengkapi dengan prosedur quality control untuk setiap jenis data. Contoh: suhu udara berkisar antara 10-40 ¬∫C.\nDari gambar di bawah dapat diketahui distribusi curah hujan tiap stasiun dan perubahannya tiap bulan selama 30 tahun di seluruh Sumatera Barat. Untuk masa mendatang SISR dapat dikembangkan lagi tidak sebatas sebagai sebuah informasi curah hujan saja, tetapi dapat juga sebagai informasi iklim. Atau bahkan mungkin juga sebagai sistem pengambilan keputusan (Decission Support System) untuk pembangunan suatu wilayah. Karena untuk mengembangkan suatu wilayah harus didahului dengan analisa iklim, bencana dan lainnya. Sehingga dari hasil tersebut dapat digunakan sebagai alat prediksi kejadian di masa depan dengan berdasar pada data yang ada pada masa lalu dan masa sekarang.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Gambar 3. Curah Hujan bulan Desember 1960 ‚Äì 2000 Sumatera Barat [/caption]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20191225-dry-and-start-of-rainy-season-in-timor-leste-20192020.html",
    "href": "blog/20191225-dry-and-start-of-rainy-season-in-timor-leste-20192020.html",
    "title": "Dry and Start of Rainy Season in Timor-Leste 2019/2020",
    "section": "",
    "text": "For the last 6 weeks, I am working with my colleagues in Bangkok and Dili to produces climate monitoring bulletin related to dry season in the last 2019 and start of rainy season for 2019-2020. The bulletin has already been published and is accessible online via https://www.wfp.org/publications/wfp-timor-leste-rainy-season-2019-2020.\nOr you can directly read both bulletin below.\nAll data to produces the analysis and maps are publicly available on internet, and I GDAL/OGR, python and ArcGIS to process the data and create the maps.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20160927-satellite-based-monitoring-of-growing-season.html",
    "href": "blog/20160927-satellite-based-monitoring-of-growing-season.html",
    "title": "Satellite-based monitoring of growing season",
    "section": "",
    "text": "Climate and vegetation data sourced from remote sensing satellites are widely used for agricultural monitoring. Most of the usage is focused on medium-low resolution data because it is associated with long-term data availability.\nThese data provide useful information for:\n\nProviding early warning when climate shocks occur against food security.\nContextual analysis needed for planning interventions, for example: market operations when rice scarcity occurs.\nClimate change adaptation program.\n\nPaddy growth stages and satellite data\nIn paddy cultivation, there are three growth stages, namely the vegetative (0-60 days), reproductive (60-90 days), and ripening stages (90-120 days). Water requirements in the three stages vary, namely in the stage of active tillers, maximum tillers, panicle formation, heading and flowering stages.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1024‚Äù][](https://www.mdpi.com/1424-8220/15/1/769/htm) Figure 1. Paddy growing stages (Source: https://www.mdpi.com/1424-8220/15/1/769/htm ) [/caption]\nThe above stages can be easily observed using remote sensing data, such as NOAA, MODIS, Landsat and Sentinel. To observe the changes in the crop growing stages, the easiest way is to use a vegetation index. The 4 data mentioned have similar characteristics from the temporal aspect, which allows an area to be crossed by satellite every 1-16 days period and has a vegetation index product.\nData from several satellites has many advantages, one of which is a very long collection of historical data. Some instruments (AVHRR and Landsat) provide information for more than 30 years, and MODIS is approaching 17+ years of observation, while some newer sensors (Sentinel) have become available since 2013 and it is estimated that this satellite mission will reach 40 years into the future. And of course all the data is freely available to the public.\nIdeal remote sensing data characteristics for vegetation monitoring\nOperational data production ‚Äì routinely produce the same data products at a set time interval.\n\nExample: production a vegetation condition map each month.\n\nAnomaly, Percent of Normal, Change, or Ranking Maps\n\nProvide historical context of how current conditions compare to the historical conditions for a specific location and time during the year.\nEasier to differentiate moderate, severe, and extreme drought events.\n\nData easily accessible and in multiple formats\n\nDigital data in analysis using GIS and computers\nGraphical maps that can be downloaded and printed for visual analysis\n\nVegetation Index\nThe Enhanced Vegetation Index (EVI) is an ‚Äòoptimized‚Äô index designed to enhance the vegetation signal with improved sensitivity in high biomass regions and improved vegetation monitoring through a de-coupling of the canopy background signal and a reduction in atmosphere influences.\nWhereas the Normalized Difference Vegetation Index (NDVI) is chlorophyll sensitive, the EVI is more responsive to canopy structural variations, including leaf area index (LAI), canopy type, plant physiognomy, and canopy architecture.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1495‚Äù] Figure 2. EVI and NDVI, July 2015 [/caption]\nThe data used for monitoring of paddy planting and harvesting area is MODIS EVI - MOD13Q1, which has a spatial resolution of 250m, temporal resolution 16 days and available from 24 February 2000. The MOD13Q1 data characteristics can be read at the following link: https://lpdaac.usgs.gov/products/mod13q1v006/\nAgriculture monitoring in global and national level\nInternational\n\nUSDA\n\nCrop Explorer - https://ipad.fas.usda.gov/cropexplorer/imageview.aspx?regionid=seasia#\n\nPROBA-V and NOAA AVHRR\n\nGlobal Agricultural Monitoring - https://ipad.fas.usda.gov/glam.htm\n\nMODIS and NOAA AVHRR\n\n\nFAO\n\nGlobal Information and Early Warning System - www.fao.org/giews/earthobservation/country/index.jsp?lang=en&code=IDN\n\nNOAA AVHRR and SPOT Vegetation\n\n\nGEO Group on Earth Observation\n\n\nGeo-GLAM Crop Monitor - https://cropmonitor.org\n\nMODIS MOD09CMG\n\n\nIndonesia\n\nBPS - BPPT\n\nArea Sampling Frame - https://ksa.bps.go.id/index.php\n\nField check using mobile data collection\n\n\nBPPT\n\niSky - http://pii.or.id/wp-content/uploads/Junal_PII_2016_Sidik_Mulyono_Edited.pdf\n\nMODIS and Landsat 8\n\n\nLAPAN - Litbang Kementerian Pertanian\n\nStanding Crop dan Kalender Tanam (KATAM) ‚Äì http://katam.litbang.pertanian.go.id/main.aspx\n\nMODIS MOD09 and Sentinel 2\n\n\nPusdatin - Kementerian Pertanian\n\nSIMOTANDI ‚Äì http://sig.pertanian.go.id\n\nLandsat 8\n\n\n\nMethodology\nState of planting and harvesting estimates were determined by importing MODIS Vegetation (MOD13Q1 - 16 days and 250m resolution) data into TIMESAT (http://web.nateko.lu.se/timesat/timesat.asp) ‚Äì a program for analyzing time-series satellite sensor data. TIMESAT conducts pixel-by-pixel classification of satellite images to determine whether or not planting has yet begun. This process was followed for all of Indonesia over multiple years in order to evaluate current planting vis-√†-vis historical years from 2001 - 2016.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú512‚Äù] Figure 3. TIMESAT parameters [/caption]\nSome of the seasonality parameters generated in TIMESAT: (a) beginning of season, (b) end of season, (c) length of season, (d) base value, (e) time of middle of season, (f) maximum value, (g) amplitude, (h) small integrated value, (h+i) large integrated value.\nThe blue line is the real EVI value and red is EVI value after smoothing. The following figure shows the differences in MODIS EVI that have not and have been smoothing.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1264‚Äù] Figure 4. Smoothed and un-smoothed difference [/caption]\nThe process of monitoring the status of paddy planting and harvesting using a vegetation index can be seen in the following figure.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1280‚Äù] Figure 5. Monitoring of growth stages based on EVI [/caption]\nData analysis\nThe following are the steps to get information on rice planting and harvesting area.\n\nData preparation\n\nDownload EVI data.\nMosaic, Resample, Extract, Clip.\nClip with paddy field area.\nConvert data to Band Interleaved by Line (BIL) format\n\nTIMESAT phenology process\n\nPrepare data list and settings for each year.\nRunning TIMESAT and configure class specific setting, see Figure 6.\n\nExtract seasonality parameters\n\nStart of Season\nMid of Season\nEnd of Season, etc.\n\nPost-processing\n\nCreate header (HDR) file to read TIMESAT output.\nConvert BIL to GeoTIF.\n\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1680‚Äù] Figure 6. TIMESAT user interface [/caption]\nExample of TIMESAT output\nStart of Season (SoS), Mid of Season (MoS), Length of Season (LoS) and End of Season (EoS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation\nIdeally validation is conducted via ground checking. But sometimes time and cost constraints make data validation in the field is rarely done. How to deal with this?\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1280‚Äù] Figure 9. CCTV location [/caption]\nThe Ministry of Agriculture through the Integrated Planting Calendar (KATAM) activity - http://katam.litbang.pertanian.go.id/main.aspx - has installed 55 CCTVs scattered in several paddy fields in Java - Bali and Lampung which can be accessed by the public free via the website above on the Monitoring menu.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCCTV shows the same information as released by TIMESAT\nNotes\nData: MODIS EVI and Paddy cultivation area\nSoftware: in addition to ArcGIS, some analyses also use open-source software such as GDAL, python, TIMESAT to process data, HDF libraries and NetCDF to read MODIS data.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180606-food-accessibility-for-rohingya-refugees.html",
    "href": "blog/20180606-food-accessibility-for-rohingya-refugees.html",
    "title": "Food accessibility for Rohingya refugees",
    "section": "",
    "text": "Following my visit to Rohingya camps in Cox‚Äôs Bazaar - Bangladesh two-months ago, I try to apply accessibility modeling to help understand on how well covered is Kutupalong megacamp in terms of WFPs services and how efficiently their food distribution points can reach refugees.\nAs a first draft, below maps provides information on accessibility (travel time and catchment area) to WFP facilities: Food Distribution Point, LogBase, eVoucher shop, BSFP/TSFP. ¬†It estimates (at each pixel) the time it takes to travel to the nearest location of WFP facilities with key assumption: mixed transportation mode - by foot until road then vehicle.\n \nAfter having few discussion with team in the field, seems the mix transportation premise is not realistic for Kutupalong megacamp, and they would like to have walking-distance only maps, even on main roads. This will allow WFP to plan for the porter system for the Logbase, clinics, food and evouchers distribution points, as well as the areas that are less well covered by the current points.\nI revised the maps based on their feedback, see below.\n \n\nBelow is an example on how GIS can support WFP to determine the best location for food distribution point, improved supply chain and better serve refuges.\n\nInitial location of WFP food distribution point (FDP) at the western part of Kutupalong megacamp\n\n\n\nTravel time (on foot) is analysed based on accessibility model to identify how long Rohingya refugees are able to reach the nearest FDP.\n\n\n\nFDP catchment, calculated using cost allocation tools, indicates that about 138,000 refugees at Camp 8W, 10, 17, 18, 19 and 20 currently rely on FDP Modhur Chara 1 - 3, FDP Balukhali 1 - 2 and Mainnergona 1 and 2. And around 25,000 refugees need approximately an hour to access the nearest FDP\n\n\n\nFurther, WFP established FDP Balukhali 3 to provide closer access for 19,000 refugees in Camp 8W, 10, 17, 18, 19 and 20 with travel time approximately is less than an hour on foot.\n\n\n\nThey plan to establish new FDP at Camp 20 to serve and provide better access to refugees in Camp 19 and 20. Leaving around 1,700 refugees still have access more than an hour on foot.\n\n\nData:\n\nFood Distribution Point (WFP)\nCamps boundary (REACH and ISCG)\nRoad and river network (OpenStreetMap)\nShelter polygon (OpenStreetMap)\nLand cover (Global Land Cover 30)\nSurface water (JRC)\nElevation (DSM from Drone provided by IOM and ISCG)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20140915-kelas-inspirasi-bogor-2.html",
    "href": "blog/20140915-kelas-inspirasi-bogor-2.html",
    "title": "Kelas Inspirasi Bogor 2",
    "section": "",
    "text": "Last week, on 9 Sep 2014 I participated in Kelas Inspirasi Bogor - part of Indonesia Mengajar activities. I and some other professional and assisted by Anneke Puspa Calliandra, give inspiration on how to dream big and prepare for the future to students in grade 1 to 6 in SDN Ciomas 8, Bogor.\nI introduced myself as a crisis mapper of the United Nations agency. I love drawing and colouring since I was a kid, this inspire me to do my job now as a crisis mapper who mainly is working with drawing and colouring maps.\nI shared the idea that helping others and contributing to humanitarian is as simple as I do.\nFirst picture from left to right: Darma, Nunuz, Ayu, Rusydan, Anneke, Herli, Yuna, Alfa, Hendri, Benny, Fahmi\nPictures taken by Nunuz Rohmatullayaly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031028-pengukuran-suhu.html",
    "href": "blog/20031028-pengukuran-suhu.html",
    "title": "Pengukuran Suhu",
    "section": "",
    "text": "Makalah berikut merupakan tugas dari mata kuliah Metode Observasi dan Instrumentasi Cuaca di semester 3. Tugas ini saya kerjakan bersama 3 orang teman lainnya: Risyanto (G24101014), Dini Oktavia Ambarwati (G24101025) dan Sri Lestariningsih (G24101037).\n\n\nLATAR BELAKANG\nSuhu merupakan salah satu unsur cuaca terpenting dalam Meteorologi. Ada banyak hal dalam kehidupan sehari-hari yang berkaitan erat dengan suhu. Alat yang paling lazim digunakan untuk pengukuran suhu adalah termometer. Sejak pertama kali ditemukan oleh Galileo pada tahun 1612, termometer terus berkembang dan bervariasi menurut penggunaanya.\nPengamatan suhu dengan menggunakan termometer memang tidak sulit, namun pembacaannya harus dilakukan secara cepat, tepat dan kontinu. Selain itu perlu diperhatikan pula cara pengkalibrasiannya sehingga data-data yang diperoleh dapat digunakan. Persyaratan-persyaratan umum lain, seperti letak (lokasi) instrumen juga harus dipenuhi. Untuk mendapatkan hasil yang lebih baik, metode pengukuran dan mekanisme kerja alat perlu diketahui.\n\n\nTUJUAN\nData suhu udara sangat diperlukan dalam banyak bidang, khususnya Meteorologi dan Klimatologi. Untuk itu diperlukan pengetahuan dalam hal pengukuran, metode dan mekanisme kerja alatnya. Pengetahuan ini sangat bermanfaat agar pada saat melakukan observasi, pengamat dapat mamperoleh hasil yang tepat dan akurat.\n\n\nPENGUKURAN SUHU\n4.1 Aspek-aspek umum\n4.1.1 Definisi\nSuhu adalah kondisi yang menentukan arah aliran panas diantara dua benda. Benda yang kehilangan panas, dikatakan memiliki suhu yang lebih tinggi. Untuk mengukur suhu suatu termometer dapat dikondisikan ke suhu yag sama seperti objek (yaitu kedalam keseimbangan termodinamika) dan kemudian suhu termometer tersebut dapat diukur. Suhu juga dapat ditentukan oleh sebuah radiometer.\n4.1.2 Satuan dan skala Suhu\nSuhu termodinamika (q), dengan satuan kelvin (K) adalah suhu dasar. Suhu yang banyak digunakan dalam meteorologi yaitu suhu celcius (t).\nPada skala suhu termodinamika, pengukuran diungkapkan sebagai perbedaan dari nol mutlak (0 K), yaitu suhu ketika molekul-molekul suatu zat tidak memiliki energi kinetik. Skala yang umum digunakan yaitu¬†Internasional Practical Temperature Scale¬†(IPTS) atau skala suhu praktis internasional yang berdasarkan pada nilai yang ditentukan untuk sejumlah besaran yang dapat diturunkan dan pada alat-alat standar khusus yang dikalibrasi pada suhu-suhu tersebut.\n4.1.3 Termometer\nSifat fisik apapun dari suatu zat yang merupakan sebuah fungsi dari suhu dapat digunakan sebagai dasar dari terrmometer. Sifat ‚Äìsifat yag banyak digunakan pada termometer meteorologi adalah perluasan panas dan perubahan dalam hambatan listrik dengan suhu.\n4.1.4 Persyaratan Meteorologi\nMeteorologi memerlukan pengukuran dan pencatatan yang kontinu dari suhu:\n(a)¬†¬†¬†Udara dekat permukaan\n(b)¬†¬†¬†Tanah pada kedalaman berbeda\n(c)¬†¬†¬†Permukaan laut dan danau\n(d)¬†¬†¬†Udara atas\nBab ini membahas tentang (a) dan (b)\n4.2 Pelindung termometer\n4.2.1 Mengukur suhu udara\n4.2.1.1 Pengaruh Radiasi\nRadiasi dari matahari, awan, tanah, dan objek sekitar lainnya lewat melalui udara tanpa melalui perubahan suhu yang berarti, tetapi termometer di udara terbuka dapat menyerap radiasi yang cukup banyak. Akibatnya, suhu dapat berbeda dari suhu udara yang sesungguhnya, perbedaannya berdasarkan pada intensitas radiasi, dan pada perbandingan radiasi yang diserap dengan panas yang dihamburkan. Agar tidak terdapat nilai kesalahan yang besar, perlu untuk melindungi termometer dengan sangkar atau penyangga yang juga melindunginya dari presipitasi, serta membiarkan sirkulasi udara bebas dan mencegah kerusakan.\n4.2.1.2 Sangkar Cuaca\nSebuah sangkar sebaiknya dirancang untuk menyediakan pagar yang mengelilingi termometer yang dapat mengeluarkan pancaran panas dan presipitasi. Dindingnya berjelusi lapis dua. Atapnya berlapis dua dengan persediaan untuk ventilasi dari ruangan diantara dua lapisan.pada iklim dingin, karena reflektivitas salju yang tinggi (diatas 88 %) sangkar juga sebaiknya memiliki dua lapis lantai yang terbuat dari papan yang dapat diturunkan atau dimiringkan dengan mudah sehingga salju yang memasuki sangkar selama badai dapat dipindahkan.\nUkuran dan bentuk sangkar dapat menjaga kapasitas panas serendah mungkin dan membiarkan cukup ruang diantara alat-alat dan dinding sehingga tidak terjadi kontak langsung unit penginderaan termometer dengan dinding bagian dalam dan luar sangkar di cat putih.\nKetika dinding lapis dua disediakan, lapisan udara diantaranya membantu mengurangi jumlah panas yang diterima dari luar dinding kebagian dalam pagar. Sirkulasi udara melalui sangkar dapat mengakibatkan suhu berubah. Pada hari yang sinar mataharinya kuat dan angin¬†calm, suhu udaranya lebih tinggi daripada ketika malam cerah dan tenang. Kesalahan lain mungkin diketahui dari pendinginan yang mengacu pada evaporasi dari sangkar yang basah setelah hujan. Semua kesalahan ini juga memiliki pengaruh langsung pada pembacaan alat-alat lain di dalam sangkar seperti higrometer, evaporimeter, dll.\nUntuk kerja meteorologi, suhu yang diamati harus mewakili kondisi sirkulasi udara bebas dengan area seluas mungkin mengelilingi stasiun, pada ketinggian diantara 1.25 m dan 2.00 m diatas permukaan tanah. Tempat terbaik untuk sangkar yaitu diarea terbuka dan tidak dilindungi atau dekat dengan pohon, gedung dan halangan lainnya. Sebuah tempat diatas lereng curam atau di lembah merupakan tempat dengan kondisi ekstrim yang harus dihindari.\nDitempat bersalju, dimungkinkan untuk menggunakan sebuah penyangga yang membuat sangkar dapat dinaikkan atau diturunkan untuk menjaga ketinggian yang tepat diatas permukaan salju.\nUmumnya hanya satu pintu dibutuhkan, sangkar ditempatkan sehingga matahari tidak menyinari termometer ketika pintu dibuka pada waktu pengamatan. Didaerah tropik dua pintu dibutuhkan untuk penggunaan selama periode tahun yang berbeda. Demikian juga didaerah kutub (dimana matahari berada pada sudut rendah).\nSangkar juga dapat terbuat dari bahan plastik yang menawarkan perlindungan yang lebih baik terhadap pengaruh radiasi karena rancangan jelusinya yang lebih baik. Pada sebuah kasus, sangkar dan penyangga harus dibangun dari bahan yang kokoh dan dipasang dengan baik sehingga kesalahan dalam pembacaan termometer maksimum dan minimum yang disebabkan oleh getaran angin dijaga tetap kecil. Penutup tanah dibawah sangkar harus rumput atau, pada tempat-tempat dimana rumput tidak tumbuh, permukaan alami didaerah tersebut.\nSangkar harus dijaga bersih dan dicat secara teratur; dibanyak tempat mengecat dua tahun sekali cukup, tetapi didaerah yang terkena polusi atmosfer setidaknya sekali setahun.\nSebagai alat untuk mengurangi kesalahan karena ventilasi yang tidak cukup dan untuk membantu psikrometri yang akurat telah disarankan ventilasi buatan dan sangkar radiasi tambahan yang digunakan pada stasiun sinoptik, khususnya untuk termometer listrik yang tidak memerlukan samgkar yang luas.\n4.2.1.3 Ventilasi buatan\nAlternatif utama untuk perlindungan pada sangkar berventilasi alami terdiri dari pelindung bola termometer dari radiasi langsung. Pelindungnya biasanya terbuat dari baja yang dipelitur untuk mengurangi penyerapannya dari radiasi panas. Bagian dalam pelindung dijaga tetap bersentuhan dengan aliran udara pada kedua sisi.¬†¬†Apabila ventilasi buatan disediakan oleh kipas angin listrik, harus diperhatikan untuk mencegah panas dari motor dan kipas angin mencapai termometer.\n4.2.2 Mengukur suhu tanah dan suhu minimum rumput\nSuhu minimum rumput merupakan suhu terendah yang dicapai dalam semalam oleh termometer yang terbuka bebas terhadap langut, tepat diatas rumput pendek. Suhunya diukur dengan termometer minimum. Termometer dinaikkan pada penyangga pada sudut kira-kira 2¬∫ dari horizontal dengan bola lebih rendah daripada tangkai, 25 mm ‚Äì 50 mm diatas tanah dan bersentuhan dengan ujung rumput.\nUmumnya termometer dibiarkan terkena udara pada jam pengamatan terakhir sebelum matahari terbenam dan pembacaannya diambil keesokan harinya.\nKedalaman standar untuk pengukuran suhu tanah yaitu, 5, 10, 20, 50, dan 100 cm dibawah permukaan. Tempat pengukuran tersebut harus sebidang permukaan tanah gundul, kira-kira tb cm2. Jika permukaan tidak mewakili keadaan umum sekeliling, luasnya tidak bisa kurang dari 100 m2. Bila tanah ditutupi salju, dapat diukur salju yang menutupi.\nKetika menerangkan tempat untuk pengukuran tanah, tipe tanah, penutup tanah dan derajat serta arah kemiringan tanah harus dicatat. Konstanta tanah fisik seperti kerapatan besar, konduktivitas panas dan kandungan kelembaban pada kapasitas lahan harus ditunjukkan. Struktur tanah juga harus dimasukkan.\nPada stasiun meteorologi pertanian diharapkan adanya pencatatan kontinu suhu tanah dan suhu udara pada permukaan yang berbeda pada lapisan yang berbatasan dengan tanah (dari permukaan tanah naik sampai kira-kira 10 m diatas batas atas tumbuh-tumbuhan yang berpengaruh).\n4.3 Waktu Tanggap Termometer\nTidak ada keuntungan penggunaan termometer dengan konstanta waktu sangat kecil, karena suhu udara terus berubah naik sampai ¬Ω derajat dalam beberapa detik. Sedangkan termometer dengan konstanta waktu yang lebih luas cenderung meratakan perubahan yang cepat. Konstanta waktu yang terlalu panjang, dapat menghasilkan kesalahan ketika perubahan suhu dalam waktu lama terjadi. Konstanta waktu, diartikan sebagai waktu yang dibutuhkan oleh termometer untuk mendata 63,2 % perubahan dalam suhu udara, diantara 30 dan 60 detik pada kecepatan angin 5 m/s. Secara kasarnya, proporsi kebalikan dari akar persegi kecepatan angin.\n4.4 Termometer Cairan dalam Kaca\n4.4.1 Syarat-syarat penyusunnya\nUntuk pengamatan rutin suhu udara termasuk suhu bola basah, maksimum dan minimum, termometer air raksa masih umum digunakan. Untuk menunjukkan suhunya termometer ini menggunakan pemuaian yang berbeda dari cairan yang terdapat pada gelasnya. Batangnya memiliki tabung yang mempuyai kaliber yang disematkan ke benda utama.\nCairan yang dipakai bergantung pada nilai suhu yang digunakan. Air raksa digunakan untuk suhu diatas titik bekunya (-32¬∫ C). Sementara etil alkohol atau cairan organik murni lainnya digunakan untuk suhu yang lebih rendah. Salah satu gelas kacanya terbuat dari gelas kaca normal atau gelas kaca ‚Äòborosilikat‚Äô yang disetujui dalam penggunaan termometer. Bola kaca dibuat setipis dan sekuat mungkin untuk memfasilitasi konduksi panas ke bola dan dari bola kemuatannya.\nAda 4 tipe utama konstruksi untuk termometer meteorologi, yaitu :\n\nTipe berlapis skala dicatat pada batang termometer.\nTipe berlapis skala dicatat pada jalur kaca opal yang disematkan pada batang di dalam lapisan termometer.\nTipe tidak berlapis dengan tanda kenaikan pada batang, baja, porselen atau kayu yang mempunyai nomor skala.\nTipe tidak berlapis dengan skala tidak diukur pada batang.\n\nTipe 1 dan 2 memiliki keuntungan daripada tipe 3 dan 4 yaitu tanda skalanya dilindungi dari kerusakan. Untuk tipe 3 dan 4 penandaan harus diisi ulang dari waktu ke waktu. Keuntungan lain dari tipe 1 dan 2 yaitu kurang rentan terhadap kesalahan paralaks.\nTipe manapun yang dipakai lapisan tidak boleh rusak sehingga kapasitas panas tetap rendah.\nUntuk termometer air raksa dan termometer maksimum, ruang hampa udara diatas kolom mercury adalah penting.\n4.4.2 Syarat ketelitian\nAngka 0 pada termometer air raksa memiliki kecenderungan naik secara lambat terhadap waktu, karena itu lebih disukai yang lebih tahan. Untuk termometer maksimum dan biasa disusun untuk memungkinkan kesalahan negatif yang lebih tinggi dari pada yang positif.\nTermometer yang digunakan untuk psikometri harus disediakan dengan lembar koreksi yang kelebihan¬†¬±¬†0.1¬∫ C. Pada stasiun sinoptik harus dicek terhadap referensi instrumen standar setiap satu atau dua tahun. Pengecekan termometer yang dipasang pada sepasang psikometri harus diseleksi sehingga mengurangi perbedaan kesalahan diantara dua termometer.\n4.4.3 Termometer maksimum\nTipe yang dianjurkan oleh termometer air raksa dengan penyempitan kaliber yang mencegah kolom air raksa surut dengan suhu turun. Pengamat memegangnya dengan benar dasar bola menurun dan menggoyangkan tangannya sampai kolm air raksa bersatu kembali. Termometer maksimum dinaikkan kira-kira 2¬∫ dari horizontal pada dasar bola yang lebih rendah untuk memastikan bahwa kolam air raksa tenang terhadap penyempitan tanpa gaya gravitasi menekannya ke angka. Pelebaran kaliber bagian-bagian kolam yang telah terpisah dapat mudah bersatu.\n4.4.4 Termometer minimum\nInstrumen yang paling umum adalah termometer alkohol dengan indeks kaca gelap kira-kira panjangnya 2 cm yang dicelupkan ke dalam air keras.\nKekurangan dari termometer minimum yaitu dengan pecahnya kolom selama pengangkutan dan adhesi air keras ke kaca.\nKolom cairan yang pecah dapat disatukan lagi dengan memegang dasar bola, menurun dan menggerakkan termometer secara ringan dan cepat dengan jari atau kesesuatu yang elastis dan tidak terlalu keras. Penggerakan harus berkelanjutan beberapa waktu, setelah itu termometer didirikan tegak lurus, setidaknya satu jam untuk membiarkan air keras menempelke kaca untuk mendinginkan bola adlah dengan campuran es dan garam yang membeku.\nSementara supaya bagian batang tetap panas, cairan akan menyaring kembali secara penahan ke kolom utama. Termometer dipegang tegak lurus dengan bolanya dalam bejana air panas, sementara batangnya ditekuk dan digoyangkan dari waktu ke waktu. Termometer harus segera dipindahkan dari air setelah bagian atas kolom mercury mencapai ruangan aman pada bagian atas batang.\nVariasi cairan yang dapat digunakan pada termometer minimum seperti etil alkohol, pentana dan toluol. Hal ini agar cairan oleh kehadiran kotoran tertentu meningkatkan kecenderungan cairan untuk berpolimer dengan pencahayaan dan setelah beberapa waktu polimerisasi seperti itu menyebabkan perubahan di dalam kalibrasi. Pada kasus etil alkohol contohnya alkohol harus bebas dari aseton.\nTermometer minimum juga dibiarkan didaerah luas untuk mendapatkan suhu minimum rumput.\n4.4.5 Termometer Tanah\nUntuk mengukur suhu tanah pada kedalaman 20 cm atau kurang umumnya menggunakan termometer air raksa dengan batangnya yang bengkok pada sudut yang tepat. Bola termometer dimasukkan ke dalam tanah pada kedalaman yang diperlukan dan skala dibaca dengan termometer insitu. Termometer ini diukur untuk pencelupan diatas sampai mengukur kedalaman.\nUntuk mengukur suhu kedalaman lebih besar dari 20 m, dianjurkan termometer air raksa yang ditempelkan di tabung kayu, gelas kaca, plastik dan bolanya dilekatkan dicat lilin atau logam. Pemasangan tabung termometer dipasang pada logam berdinding tipis atau tabung plastik dimasukkan kedalam tanah. Di iklim dingin bagian atas tabung luar dipasang diatas tanah ke ketinggian lebih besar dari kedalaman penutup salju. Pengamatan suhu tanah dapat lebih baik dengan menggunakan remote perekam sensor suhu listrik.\n4.5 Pembacaan Termometer\nTermometer harus dibaca cepat, konsisten dan akurat untuk menghindari perubahan suhu selama kehadiran pengamat. Koreksi untuk nilai kesalahan, jika ada, harus digunakan dalam pembacaan. Termometer maksimum dan minimum harus dibaca dan dipasang sekurangnya dua kali dalam sehari. Pembacaannya harus dibandingkan secara frekuentif dengan termometer biasa untuk menjamin bahwa tidak ada kesalahan serius pada termometer tersebut.\n4.6 Termograf Mekanik\n4.6.1 Syarat-syarat Umum\nTipe yang masih sering digunakan, disediakan sensor bimetallic atau tabung¬†Bourdon¬†karena relatif murah, akurat dan mudah dibawa. Pada umumnya termograf harus beroperasi diatas nilai 60¬∞C atau bahkan 80¬∞C, jika ingin digunakan dalam iklim benua (kontinental). Nilai skala dibutuhkan, sehingga suhu bisa dibaca sampai 0.2¬∞C tanpa kesulitan pada grafik dengan ukuran yang bisa diterima. Untuk melakukannya, ketetapan harus dibuat untuk perubahan ‚Äòsetting‚Äô angka nol dari alat menurut musim. Kesalahan maksimum termograf tidak boleh lebih dari 1¬∞C.\n4.6.2 Termograf Bimetallic\nPada alat ini, pergerakan dari pena perekam dikendalikan oleh perubahan pada lekukan dari kepingan bimetallic atau ‚Äòhelix‚Äô. Instrumen ini harus dilengkapi dengan alat peubah nilai skala dengan mengatur panjang dari pengungkit yang mentransfer pergerakan dari bimetal ke pena. Penyesuaian ini lebih baik dikerjakan oleh pegawai yang diberikan kuasa. Elemen bimetallic harus terlindungi dari korosi, hal ini dapat dilakukan dengan menggunakan tembaga berat, nikel atau plat kronium. Konstanta waktu yang khas instrumen ini adalah kira-kira 25 detik dihasilkan pada kecepatan udara 5 m/s.\n4.6.3 Termograf Tabung Bourden\nPenyusunannya secara umum menyerupai dengan tipe bimetallic, tetapi elemem suhu sensitifnya berbentuk tabung logam cekung yang diisi dengan alkohol. Tabung tidak lebih sensitif dari elemen bimetallic dan biasanya membutuhkan mekanisme pengungkit yang berulang-ulang untuk memberikan nilai skala yang cukup. Konstanta waktu yang khas untuk instrumen ini adalah 60 detik yang dihasilkan pada kecepatan udara 5 m/s.\n4.6.4 Pengecekan Termograf\nPembacaan termograf harus diperiksa secara teratur dan membandingkannya dengan termometer kontrol. Metode sederhana untuk melakukan pengecekan adalah dengan menggunakan referensi dari termometer maksimum dan minimum. Metode yang lebih akurat dari pengecekan termograf adalah dengan mengetesnya dalam ruangan yabg didesain secara tepat, memiliki keseimbangan panas di laboratorium. Dilakukan sekurangnya satu kali dalam tiap dua tahun.¬†¬†Untuk mengecek nilai skala instrumen adalah dengan mengelompokan (memplot) antara pembacaan termometer bola kering yang diambl pada jam-jam utama observasi, dengan pembacaan pada observasi termograf. Akan ada beberapa tersebar, tetapi garis cocok yang terbaik harus pada sudur 45¬∞¬†ke salah satu sumbu.\n4.7 Termometer Listrik\nKelebihan utama termometer listrik terletak pada kemampuan menyediakan sinyal output yang sesuai untuk penggunaan dalam penunjuk jarak jauh, perekaman, penyimpanan atau transmisi data temperatur. Sensor yang sering digunakan adalah elemen hambatan listrik, transmitter dan thermocouples.\n4.7.1 Termometer Hambatan Listrik\n4.7.1.1 Menggunakan logam dan campuran\nUntuk perubahan temperatur yang kecil, kenaikan nilai hambatan dari logam murni sebanding dengan perubahan suhu, seperti yang ditunjukan pada persamaan (4.2).\n\\[R_T = R_o[1+ a (T-T_o)] \\tag{4.2}\\]\ndimana \\((t-T_o)\\) kecil\n\n\\(R_T\\) adalah hambatan logam dengan jumlah tetap pada suhu \\(T\\) dalam Kelvin\n\\(R_o\\) adalah koefisien suhu referensi \\(T_o\\), dan\n\\(a\\) adalah koefisien suhu dari hambatan disekitar \\(T_o\\).\n\nUntuk perubahan temperatur yang lebih besar dan untuk campuran logam tertentu, persamaan (4.3) menunjukan hubungan yang lebih akurat :\n\\[R_T = R_o [1 + a (T-T_o) + b (T-T_o)^2] \\tag{4.3}\\]\ndimana:\n\n\\(a\\) dan \\(b\\) adalah koefisien yang dapat dicari dengan mengkalibrasi termometer yang dimaksud.\n\nHambatan logam yang baik mengikuti persyaratan sebagai berikut :\n\nSifat fisik dan kimia akan tetap sama dengan nilai pengukuran suhu\nHambatan akan meningkat secara tetap dengan kenaikan suhu adanya ketidakberlanjutan dari nilai pengukuran\nPengaruh luar seperti kelembaban, korosi atau perubahan fisika tidak akan mengubah nilai hambatan cukup besar\nResistivitasnya akan tetap konstan selama periode dua tahun atau lebih\nResistivitasnya dan koefisien termal resistivitas akan cukup besar bermanfaat dalam pengukuran sirkuit.\n\nPlatinum asli merupakam bahan terbaik untuk termometer standar primer yang dibutuhkan dalam mentransfer IPTS 1968. Tembaga merupakan bahan yang baik untuk termometer standar sekunder.\nTermometer praktis biasanya terbuat dari campuran platinum, nikel atau tembaga (dan adakalanya tungsten¬†¬†untuk keperluan meteorologi).\n4.7.1.2 Menggunakan Thermistor\nThermistor adalah sebuah semikonduktor dengan koefisien termal yang cukup besar yang nilainya bisa positif atau negatif tergantung bahan yang digunakan. Persamaan umum untuk ketergantungan suhu dari hambatan R thermistor diberikan pada persamaan (4.4)\n\\[R = a \\exp(b/T) \\tag{4.4}\\]\ndimana \\(a\\) dan \\(b\\) adalah konstan dan \\(T\\) adalah suhu thermistor dalam Kelvin. Keuntungan thermistor adalah :\n\nKoefisien suhu hambatan yang besar memungkinkan tegangan yang melewati hambatan berkurang ketika mencapai senistivitas yang sama, pengurangan atau peniadaan tersebut dibutuhkan¬†¬†untuk menghitung hambatan yang mendahului dan perubahannya.\nElemen dapat dibuat sangat kecil, sehingga kapasitas termalnya yang kecil dapat menghasilkan konstanta waktu yang kecil pula.\n\nJika¬†¬†suhu referensi, To digunakan dalam persamaan (4.4), maka persamaan (4.5) dapat diperoleh :\n\\[R = R_o \\exp\\left[\\frac{b}{T}-\\frac{b}{T_o}\\right] \\tag{4.5}\\]\n4.7.1.3 Metode pengukuran\nTermometer hambatan bisa dihubungkan dengan sejenis sirkuit pengukuran listrik yang diantaranya merupakan variasi dari sirkuit penghubung hambatan dalam seimbang (balance) dan tidak seimbang (unbalance).\nDigital voltmeter (DVMs) dapat digunakan dalam hubungannya dengan sumber arus konstan untuk mengukur ketegangan suhu tertentu yang turun melewati elemen termometer; hasilnya dapat diskalakan secara langsung dalam suhu. Output digital juga dapat disimpan atau dipindahkan tanpa mengurangi akurasinya dan tersedia bagi penggunaannya selanjutnya. Digital output dari DVM sesudah itu diubah ke bentuk tegangan analog.\nUntuk tujuan perekaman, instrumen sirkuit pnghubung tidak seimbang (unbalance bridge circuit) dapat digunakan, tetapi lebih akurat hasilnya,¬†potentiometric recorder¬†harus disiapkan.\n4.7.2¬†¬†¬†¬†Thermocouples\n4.7.2.1 Sifat-sifat\nTermocouples banyak digunakan ketika termometer dengan konstanta waktu yang sangat kecil dan kemampuan pembacaan jarak jauh dan perekaman diperlukan. Satu kerugian jika suhu mutlak diperlukan, peralatan penyokong untuk pengukuran gaya elektromotif perlu dipasang. Termocouples lebih cocok untuk pengukuran suhu yang berbeda.\n4.7.2.2 Metode prngukuran\nSedikit teknik yang diperlukan dengan menggunakan pembelokan dari pergerakan meteran.\nBeberapa sumber kesalahan :\n\nHambatan termoelemen dan ketergantungan suhunya\nPersediaan garis dan penggantian petunjuk\nPergerakan meteran dan hambatan serinya\n\nJika akurasi pengukuran lebih baik dari 0.05% dibutuhkan, maka tegangan termoelektrik harus ditentukan dengan prosedur pengganti, yaitu menggunakan penunjuk angka alkohol nol sensitif¬†(sensitive zero indicator).\n4.7.3¬†¬†¬†¬†Teknik Digital\nProsedur pengukuran digital memiliki akurasi yang tinggi dan kemudahan dalam tranmisi, indikasi, penyimpangan dan pemrosesan. Ketika komputer digital digunakan untuk perolehan dan pemrosesan¬†¬†data dari sejumlah tempat pengamatan, maka teknik pengukuran digital menjadi penting. Dengan teknik tersebut, nilai dapat diwakilkan hanya sebagai perkalian integral dari kenaikan nilai minimum, sedangkan pengukuran analog, bagian apapun secara teoritis mungkin (dapat dilakukan) pada prakteknya, resolusi pengukuran analog dibatasi dengan penggunaan teknik dan tak sering lebih baik dari yang dihasilkan dengan metode digital. Dalam metode digital, nilai pengukuran dikuantitaskan (ke dalam) perkalian integral dari kenaikan minimum (resolusi). Nilai dari kenaikan minimum harus ditentukan sehingga tidak banyak mempengaruhi akurasi pengukurannya. Kenaikan 1-3 akurasi final (ketepatan akhir) instrumen, umumnya diterima sebagai solusi. Dengan pengukuran temperatur secara elektrik, sinyal output dari sensor merupakan arus langsung, tegangan langsung atau hambatan lain yang besar jaraknya yang bervariasi secara terus menerus. Ketika peraaltan tersebut dioperasikan dalam sistem pengukuran digital, quantization dan konversi ke dalam bentuk kode digital diperlukan. Perkembangan yang tepat dalam bidang elektronik menyebabkan berkembangnya penggunaan prinsip konversi. Mengacu pada kekebalannnya dari bunyi listrik, menggabungkan pengubah analog ke digital umum digunakan.\nKESIMPULAN\nSuhu adalah kondisi yang menentukan arah aliran panas diantara dua benda. Satuan suhu yang banyak digunakan dalam meteorologi yaitu Kelvin dan Celcius. Suhu mencerminkan energi kinetik rata-rata dari gerakan molekul-molekul.\nSecara umum, termometer (alat pengukur suhu) terbagi dalam 3 macam, yaitu :\n\nTermometer cairan dalam kaca\nTermometer mekanik\nTermometer listrik\n\nBeberapa contoh termometer kaca misalnya : termometer maksimum dan minimum, termometer tanah. Termograf mekanik terdiri dari termograf bimetallic dan termograf tabung Bourdon. Sedangkan termometer listrik terdiri dari termometer hambatan dan thermo couples.\nPengukuran suhu udara harus dilakukan diareal terbuka, tanpa adanya halangan seperti gedung, pohon, dan lain-lain. Pembacaan termometer harus dilakukan secara cepat, tepat dan kontinu sehingga didapatkan data yang baik.\nLampiran :\nDefinisi titik tetap\nTitik referensi sekunder dan suhunya pada IPTS\nNilai hambatan khusus, Ro, dan koefisien suhu, a, beberapa logam umum dan campuran\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210525-hp-clj-pro-mfp-m181fw-supply-error-message.html",
    "href": "blog/20210525-hp-clj-pro-mfp-m181fw-supply-error-message.html",
    "title": "HP CLJ Pro MFP M181fw supply error message",
    "section": "",
    "text": "I bought printer HP Color Laserjet Pro MFP M181fw and a set spare original HP 204A catridges not long after I started to work from home due to COVID-19 pandemic situation. The printer really fit for me and family, as my daughter frequently needed to print some school assignment and scanned the paper works, and to support my wife‚Äôs works too.\n\n9 months passed, time to replace the cartridges after one month I was forced to keep using it even though the notifications ‚Äútoner low‚Äù kept appearing.\nI was surprised when I replace the empty toner and I got a warning ‚ÄúSupply Problem‚Äù message. After googling, I found it caused by update a firmware and this update excludes cartridges from old production or third party cartridges from use and causes this error message. I remembered previously getting a firmware update few weeks ago.\nSeems the solution related this problem is only one: downgrade the firmware\nHere‚Äôs how-to guide to downgrade the firmware:\n(You need a computer/laptop with Windows 10 installed)\nStep 1. Confirm Cartridge Policy is off.\nNewer HP mono/color printers have what they call a ‚Äúcartridge policy‚Äù. Without turning this off, the printer will not accept aftermarket cartridges and may even write to the chip, rendering the cartridge useless. To turn of the Cartridge Policy setting, follow one of the paths described below according to the options available on your particular machine:\n\nHome Screen &gt; Setup Menu &gt; System Setup &gt; Supply Setting &gt; Cartridge Policy &gt; Off\n\nStep 2. Find the Firmware Datecode\n\nHome Screen &gt; Setup Menu &gt; Service &gt; Firmware Datecode\n\nIf the firmware version is NOT 20201021, immediately disable the ‚ÄúCheck Automatically‚Äù and ‚ÄúAllow Updates‚Äù options to avoid an upgrade in the future.\n\nHome Screen &gt; Setup Menu &gt; LaserJet update &gt; Manage Updates &gt; Check Automatically. Choose NO\nHome Screen &gt; Setup Menu &gt; LaserJet update &gt; Manage Updates &gt; Allow Update. Choose NO\n\nStep 3. ONLY Complete this Step if Your Firmware Date is 20201021\n\nDownload Firmware Version 20200612 for HP Color Laserjet Pro M180 ‚Äì M181 Series. Link: https://on.istan.to/3TUk8Vg\nRun the downloaded application to roll back to the previous firmware, then click Send Firmware. The process should only take a few minutes.\n\n\nStep 4. Re-check the Firmware Datecode\n\nHome Screen &gt; Setup Menu &gt; Service &gt; Firmware Datecode\n\nIf everything success, you will get response for the code 20200612. It means your problem is solved and the printer is ready to use.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20220217-modis-lst-explorer.html",
    "href": "blog/20220217-modis-lst-explorer.html",
    "title": "MODIS LST Explorer",
    "section": "",
    "text": "Recently I worked on utilising daily MODIS LST (MOD11A1) to calculate number of day in a year with LST exceeding 35degC. At some point curious about the daily coverage of MODIS has pixel with good data quality.\nSo, I wrote a Google Earth Engine script to see visual differences between applying QC Bitmask and the original based on daily data.\nCheck this out!\napp: https://bennyistanto.users.earthengine.app/view/modis-lst-explorer\ncode: https://code.earthengine.google.com/6f6d1750bc67f384dbf9060c17c710ef\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20071215-gis-for-immunization-activities.html",
    "href": "blog/20071215-gis-for-immunization-activities.html",
    "title": "GIS for immunization activities",
    "section": "",
    "text": "In the last 4-months I have been working for USAID funded project as a GIS Consultant on Immunization for 64 districts in 7 provinces in Indonesia. My main task is develop map products in user friendly format in CDs and print and provide technical support to project‚Äôs provincial office, and counterparts on use of maps in planning for immunization activity location.\nBy the end of the project, I submitted 1412 map sheet in soft-copy and hard-copy. Below is one of example.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20160402-so-fun-aerial-bold.html",
    "href": "blog/20160402-so-fun-aerial-bold.html",
    "title": "So fun Aerial Bold!",
    "section": "",
    "text": "Have you heard about Aerial Bold? - a project from Benedikt Gro√ü & Joey Lee, devoted to finding all the human-built letters of the world. The idea was to pore over satellite imagery to locate architectural ABCs, then turn those images into a font. Wired also discussed this.\n\nHere‚Äôs the example of my daughter‚Äôs name using Aerial Bold. If you are curious, yes it‚Äôs real, W letter in the picture comes from Rosenhof Berlin-Zehlendorf Seniorenwohnanlage Betriebsgesellschaft mbH, Schlettstadter Stra√üe 22, 14169 Berlin, Germany. https://maps.app.goo.gl/TYBs856cSeDM9imz9\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031016-menduga-distribusi-ukuran-butir-hujan.html",
    "href": "blog/20031016-menduga-distribusi-ukuran-butir-hujan.html",
    "title": "Menduga distribusi ukuran butir hujan",
    "section": "",
    "text": "Minggu kemarin kita melakukan praktikum mata kuliah Hidrometeorologi tentang pendugaan distribusi ukuran butir hujan pada berbagai kondisi atmosfer, awal atau akhir hujan. Pada sesi menjelaskan prosedur pengamatan butir hujan, Prof.¬†Daniel Murdiyarso menunjukkan contoh ukuran butir hujan yang dihasilkan oleh suntikan mikro-milimeter pada kertas khusus warna biru yang mempunyai daya serap tinggi dan dipasang pada ‚ÄúPemidangan kayu bundar‚Äù, yang biasa dipakai untuk menjahit.\nSebelum kita membahas tentang analisa datanya, sebaiknya kita baca dulu beberapa paragraf berikut ini.\nHujan adalah salah satu bentuk presipitasi air atmosfer ke permukaan bumi. Hujan akan terjadi apabila uap air/air yang dapat menimbulkan hujan (precitible water) mengalami kondensasi sehingga membentuk butir-butir yang memiliki ukuran dan kecepatan jatuh yang cukup.\nTetesan hujan menyebabkan pecahnya bongkahan tanah yang besar, menghancurkannya dan menyebabkan pengangkutan partikel-partikel tanah dengan percikan dan pencucian (Seyhan, 1976d dan Riezebos, 1975)\nButir hujan bermacam-macam ukurannya mulai dari butir yang sedikit besar dari kabut samapai diameter maksimum sedikit lebih besar dari 7 mililiter. Rata- rata butir hujan hujan yang terjadi berukuran antara 1 sampai 4 mililiter. Rata- rata hujan didaerah tropika mempunyai butir hujan sekitar tiga mililter dan diameter 4,4 mm. Diameter didaerah tropika lebih besar dibanding daerah iklim sedang. (Kowal dan Kassam dalam Sitanala, 2000)\nPada setiap kejadian hujan berbagai ukuran hujan terdapat. Namun demikian terdapat korelasi yang nyata antara intensitas hujan dengan ukuran median butir-butir hujan. Ukuran median butir hujan yang membagi butir-butir besar menjadi butir-butir yang kecil dalam kelompok yang sama volumenya bervariasi mulai dari mulai i 1mm untuk hujan berintensitas 1,25 mililiter per jam sampai tiga mm untuk hujan dengan intensitas 100 mililiter per jam (Laws dan Parson dalam Sitanala, 2000). Ini menunjukkan bahwa suatu peningkatan butir hujan sebesar tiga kali maka terjadi peningkatan intensitas hujan sebersar 80 kali.\nKecepatan jatuh butir hujan ditentukan oleh grafitasi, tahanan udara, dan angin. Gravitasi bekerja secara seragam terhadap semua butir hujan yang ada. Tetapi tahanan udara persatuan massa akan semakin besar dengan semakin kecilnya ukuran butir hujan. oleh kerena itu semakin kecil butir hujan, maka besar permukaan jenisnya.\nAngin adalah faktor lain yang menyebabkan kecepatan jatuh butir hujan. Sementara itu kecepatan jatuh butir hujan maksimum adalah berkisar antara 33 km/jam. Angin kencang dapat memperbesar kecepatan jatuh butir hujan.\nDengan diketahui kecepatan butir hujan maka kita akan dapat menentukan besarnya energi kinetik hujan tersebut. Energi kinetik hujan adalah sifat hujan yang sangat penting dan mempengaruhi erosi.\n\nData\nProses mengukur butir hujan ini sangat melelahkan, membuat mata jereng. Saya harus mengukur diamater setiap butir hujan yang tertangkap kertas di pemidangan. Grafik di bawah menunjukkan bahwa Kecepatan adalah fungsi dari Diameter butir hujan.\n\n\n\nGambar 1. Data butir hujan\n\n\nGambar 1. Data butir hujan\nSelain itu bekerja dalam kelompok, kita harus melakukan kalibrasi butir hujan dalam berbagai ukuran milimeter dan dengan 2 kali ulangan kemudian dihitung diameter rataannya. Cukup menyenangkan tapi juga membosankan ketika harus mengukur butir satu-per-satu.\n\n\n\nGambar 2. Kalibrasi data butir hujan\n\n\nGambar 2. Kalibrasi data butir hujan\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230329-monitoring-dry-and-wet-season-in-south-sudan.html",
    "href": "blog/20230329-monitoring-dry-and-wet-season-in-south-sudan.html",
    "title": "Monitoring dry and wet season in South Sudan",
    "section": "",
    "text": "I am excited to share some news with you all! The World Bank has recently released a new publication, in which I had the opportunity to contribute the analysis. The publication is titled ‚ÄúRising from the Depths: Water Security and Fragility in South Sudan‚Äù, and it delves into the crucial subject of dry and wet periods in the region.\nMy contribution to the analysis seeks to unveil the complex connection between water security and fragility in South Sudan, while pinpointing opportunities for the World Bank Group to employ water security measures as catalyst for stability, peace and resilience.\nThe team employs the Standardized pPrecipitation Index (SPI) based on CHIRPS global precipitation estimates data to track the evolution of dry and wet conditions on a monthly and yearly basis.\nThe images below serve as examples, showcasing the severe flooding experience in South Sudan over the past 60 years - events accurately capture through the data.\n\nIf you are interested to read the story and report, feel free to visit below:\n\nPress release: https://www.worldbank.org/en/news/press-release/2023/03/27/new-report-with-inclusion-and-accountability-water-security-is-key-for-development-and-stability-in-afe-south-sudan\nPublication: https://www.worldbank.org/en/country/southsudan/publication/rising-from-the-depths-water-security-and-fragility-in-afe-south-sudan\nPublication: https://openknowledge.worldbank.org/entities/publication/91048a50-eacb-5a24-9fa4-30cf8a9a9c9b\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210416-how-to-get-daily-rainfall-forecast-data-from-gfs-part-2.html",
    "href": "blog/20210416-how-to-get-daily-rainfall-forecast-data-from-gfs-part-2.html",
    "title": "How to get daily rainfall forecast data from GFS? (Part 2)",
    "section": "",
    "text": "Few days ago I wrote a post on how to get GFS data using GRIB filter and NCAR/UCAR Research Data Archive. Now I will explain on how to get GFS rainfall forecast data using Google Earth Engine (GEE).\nGFS data description in GEE available in this link: https://developers.google.com/earth-engine/datasets/catalog/NOAA_GFS0P25\nThe Global Forecast System (GFS) is a weather forecast model produced by the National Centers for Environmental Prediction (NCEP). The GFS dataset consists of selected model outputs (described below) as gridded forecast variables. The 384-hour forecasts, with 3-hour forecast interval, are made at 6-hour temporal resolution (i.e.¬†updated four times daily). Use the ‚Äòcreation_time‚Äô and ‚Äòforecast_time‚Äô properties to select data of interest.\nI found good explanation about GFS data in GEE in StackExchange, written by Kel Markert.\nYou have to be a little careful with GFS data because it can get pretty confusing with the timing (as you have already seen). First, the GFS forecast is initialized 4 times a day (at 00, 06, 12, 18 hours GMT) so you get 4 forecasts per day. Next, GFS forecast interval for an individual forecast is hourly for the first 5 days then goes to 3-hour intervals up to day 10 then goes to 12-hour interval for the rest up to 16 days as shown in the returned data from your example. So, theoretically you will get 60 (24+18+12+6) images per day with different forecast times¬†if you keep track of all the timing.\nHowever, Earth Engine does not keep track of all of the timing for you. If you look at the metadata, you will see that the¬†system:time_start¬†is equal to the¬†creation_time¬†(i.e.¬†initialization time). So, when you use¬†.filterDate()¬†in EE (which uses the¬†system:time_start¬†property internally for filtering) you only filter by the day that the forecast was created (all the forecast initialization and forecast hours) and you get (5*24+5*8+6*2)*4 which equals the 688 images from your example.\nHopefully that helps make a little more sense on how the data is structured in terms of time and now we can use that understanding to start filtering. Here is an example getting a single day from different forecasts initializations:\nAs you can see from the example, the .filterDate() is done on 6-hour intervals to get a specific forecast initialization, then the collection is filtered by the forecast time (within the date of interest).\n\n\nHourly and daily rainfall forecast from GFS\nBefore we start to use precipitation data from GFS, it‚Äôs better to understand the unit of precipitation. Let‚Äôs check in bands information below:\n\nPrecipitation unit in GFS data: ‚Äúkg m-2 s-1‚Äù, is the International System of Units (SI) used for precipitation. While 1kg = 1Liter = 10e6 mm3 and 1m2 = 10e6 mm2. Therefore 1kg of rain over 1 m2 is equivalent to 1mm. As the unit is already in millimeters (mm), so this data is ready to use, no conversion needed.\nLet‚Äôs start to write the code. As our area of interest is Indonesia, lets set the map center to Indonesia, symbology and set the date. In below example, I set the date to 4 April 2021, the day when Tropical Cyclone Seroja hit the eastern part of Nusa Tenggara, Indonesia\nNow as the main input, Import GFS data from here https://developers.google.com/earth-engine/datasets/catalog/NOAA_GFS0P25 and select Bands for ‚Äútotal_precipitation_surface‚Äù and do filtering based on date and ‚Äúforecast_hours‚Äù.\nBelow script is example to grab forecast for hour-1 after the release hour. See details below:\n\nRelease date and time: 4 April 2021, 18:00 UTC ~ 5 April 2021 01:00 WIB\nForecast hours = 1, it means rainfall forecast for 01:00 to 02:00 WIB\n\nWhat if you want to have forecast for other hour periods (hour-2)? You can easily change value 1 in above script. See below.\nImportant notes related to GFS forecast data:\n\nforecast_hours equal to 1, means total in millimeters of rainfall accumulation forecast from beginning of forecast date as a start, until the next 1 hours.\nWhile the value is equal to 2, means total in millimeters until the next 2 hours, on and on up to 6 hours.\nBack to 1 starting from the 7th forecast_hours, on and on up to 12 hours.\nContinues repeatedly by 6 up to 120 hours (6,12,18,24,30,36,42,48,54,60,66,72,78,84,90,96,102,108,114,120).\nforecast_hours equal to 6, means total in millimeters of rainfall accumulation forecast from beginning of forecast date as a start, until the next 6 hours.\nWhile the value is equal to 12, means total in millimeters from hour-7 to 12.\nAs stated early in above paragraph, GFS forecast interval for an individual forecast is hourly for the first 5 days (forecast_hours equal to 120) then goes to 3-hour intervals up to day 10 (forecast_hours equal to 240) then goes to 12-hour interval for the rest up to 16 days.\n\nNow I will write process to calculate rainfall accumulation for 24-hours (1-day). The output will visualised using same standard symbology.\nHow about rainfall accumulation for 48-hours or 2-days. You can add below script.\nIs there any alternative to calculate rainfall accumulation? You got it.\nExample below, I try to calculate rainfall accumulation for 1-day which coming from hour-6, 12, 18 and 24. I need to extract individual hours before added into single image and do calculation via image expression.\nLets add the legend information into UI panel, to make user easy to understand how much rainfall will fall in certain areas.\nNow this is the important task, downloading the data. Before write the download script, It‚Äôs better to clip it based on area of interest.\nI don‚Äôt need global data, I am only interest to get data from 60E to 180E. Then I can continue to export it to Google Drive for each number of days of forecast.\nNote: Please adjust folder name in your Google Drive.\nBelow is the result!\n\nLink for the full code\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200206-paddy-harvesting-and-planting-area-2019.html",
    "href": "blog/20200206-paddy-harvesting-and-planting-area-2019.html",
    "title": "Paddy harvesting and planting area 2019",
    "section": "",
    "text": "BPS (Statistics of Indonesia) release: paddy harvested area of 2019 was estimated around 10,68 million Ha.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Figure 1. BPS release on harvest area in 2019 [/caption]\nOutputs of my analysis indicates higher number, with less than 10% difference. I suppose this is still acceptable due to many factors i.e.¬†accuracy of interpretation, uncertainty estimate using satellite data, does not count pixels which are interpreted as crop failure, and no ground check or validation.\nBy analyzing MODIS satellite images over the growing season of 2019, total planted area for paddy was 11,3 million Ha and total harvested area was 11,4 million Ha. Please notice that planting area was calculated only for the period of 1 Jan - 31 Dec (excluding main planting season Oct - Dec from the earlier year).\n\nThe Start of Season (SoS - Planting) and End of Season (EoS - Harvesting) estimates the day of the year identified at the beginning/end of a consistent upward/downward trend in timeseries EVI and is interpreted as the beginning/end of measureble of photosynthesis in the vegetation canopy.\nAll the figures came from TIMESAT analysis, please check on the following page for detailed methodology.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20191109-visit-to-dlr-and-wfp-innovation-accelerator.html",
    "href": "blog/20191109-visit-to-dlr-and-wfp-innovation-accelerator.html",
    "title": "Visit to DLR and WFP Innovation Accelerator",
    "section": "",
    "text": "Shortly after finishing workshop in Rome, I flew to Munich to attend a workshop in 29 October 2019 with Earth Observation Centre at Deutsches Zentrum f√ºr Luft- und Raumfahrt (DLR) Oberpfaffenhofen.\nThe 1-day workshop discussed PRISM major technical highlights and areas of operation, Global climate monitoring tools for WFP, DLR major highlights and areas of operation. Following with a presentation about the project that currently runs within DLR Emergency Operation Center (EOC): Emergency mapping at DLR EOC, An automatic system for near-real time flood extent and duration mapping based on multi-sensor satellite data, Land cover mapping - drought monitoring and agriculture productivity monitoring.\nThe next day, I worked at WFP Innovation Accelerator office and have meeting on corporate solution for VAM data product with VAM HQ team, this activities will cover some thematic areas: climate trends and variability, seasonal and natural hazard monitoring and early warning, land cover dynamic and livelihood resources, settlements, infrastructure and populations. And continued discussion with SKAI team, project SKAI is a collaboration between Google AI and WFP Innovation Accelerator on Artificial intelligence and aerial imagery for emergencies. SKAI uses Artificial Intelligence (AI) and satellites to help WFP reduce the amount of time needed to understand the impact of disasters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230401-visualising-the-wrf-output.html",
    "href": "blog/20230401-visualising-the-wrf-output.html",
    "title": "Visualising the WRF output",
    "section": "",
    "text": "1 Introduction\nThe Weather Research and Forecasting (WRF) model is a powerful numerical weather prediction system used to simulate atmospheric phenomena at various scales. WRF produces a significant amount of output data that can provide valuable information for meteorological and climatological research, weather forecasting, and environmental management.\nWRF output data is typically stored in netCDF files, which contain multiple variables with different units and dimensions. However, the netCDF files generated by WRF are not always following the Climate and Forecast (CF) metadata convention, which can make it difficult to interpret the data.\nThe CF metadata convention provides a standardized way of describing the metadata and units of variables in netCDF files. Adhering to this convention makes it easier to interpret the data and compare it with other datasets. However, the WRF output files are often not fully CF-compliant.\nVisualizing the output from the WRF model can help researchers and practitioners to gain insights into various meteorological and climatological phenomena, including temperature, wind, precipitation, cloud cover, and atmospheric pressure. Visualization is an important step in understanding the data and extracting meaningful insights. Visualization can help identify patterns, trends, and anomalies in the data that might not be apparent from raw numerical output.\n\nTime series plots are a common way to visualize the temporal evolution of variables such as temperature, precipitation, and wind speed. These plots can reveal patterns and trends in the data and help to identify anomalies or outliers.\nContour plots can be used to visualize the spatial distribution of variables such as temperature, pressure, and precipitation. These plots can show the magnitude and direction of the variables and help to identify patterns and features such as fronts, ridges, and troughs.\nMaps are a common way to visualize the spatial distribution of variables over a region of interest. Maps can be used to display variables such as temperature, precipitation, wind speed, and cloud cover, and can help to identify patterns and features such as mountains, coastlines, and rivers.\nAnimations can be created from the WRF output data to visualize the temporal and spatial evolution of variables over a specific period. Animations can be useful for identifying trends, patterns, and anomalies in the data and for communicating the results to a wider audience.\n3D visualizations can be used to represent the three-dimensional structure of atmospheric phenomena such as clouds and fronts. These visualizations can provide a more detailed and realistic representation of the data and help to identify features such as updrafts, downdrafts, and vortices.\n\nThere are several tools available for visualizing WRF output, ranging from simple plotting libraries to more advanced graphical user interfaces.\n\nOne of the most popular tools for visualizing WRF output is the NCAR Command Language (NCL). NCL is a programming language designed specifically for scientific data analysis and visualization. NCL provides a powerful set of tools for working with NetCDF files, including the ability to subset and manipulate data, generate contour plots and maps, and create animations.\nPython is a popular programming language for data processing, analysis, and visualization. Python libraries such as Matplotlib, Cartopy, and Basemap can be used to create a wide range of visualizations from the WRF output data.\nXarray is another popular Python library that can be used to handle and visualize multi-dimensional datasets. Xarray provides a powerful set of functions for data manipulation, analysis, and visualization and can be used to create a variety of plots and maps.\nR is a statistical programming language that can also be used for data processing, analysis, and visualization. R provides a wide range of packages for creating static and interactive visualizations, including ggplot2, lattice, and Shiny.\nCommercial software packages like ArcGIS and open-source Geographic Information System (GIS) software such as QGIS can be used to visualize WRF output data on maps and perform spatial analysis. QGIS provides a user-friendly interface for creating maps, visualizing data, and conducting geospatial analysis.\n\nRegardless of the tool used, there are some key considerations to keep in mind when visualizing WRF output. These include choosing appropriate color maps, selecting appropriate contour intervals, and ensuring that the data is presented in a clear and understandable way.\nIt is also important to consider the spatial and temporal scales of the data when visualizing WRF output. For example, high-resolution data may require different visualization techniques than coarser resolution data, and data spanning multiple time scales may require animations or time series plots.\nVisualization of WRF output is not only important for understanding the data but also for communicating results to stakeholders and decision-makers. Effective visualization can help convey complex scientific information in a way that is easily understood by non-experts.\nOverall, visualization is a crucial step in the WRF modeling process, enabling scientists and researchers to extract meaningful insights from the vast amounts of data generated by the model. While there are many tools and techniques available for visualizing WRF output, it is important to choose the most appropriate tool for the specific task at hand and to consider best practices for data visualization to ensure clear and accurate representation of the data.\n2 New CF based NetCDF files from native WRFOUT NetCDF files\nThe WRFOUT files created by the WRF model are not the most straightforward to interpret, and pose several challenges. These files include a series of three-dimensional fields that cover a specific region over a specific period of time, which can be used to analyze different meteorological phenomena, such as temperature, pressure, wind speed and direction, clouds, and precipitation.\nHowever, WRF output data does not always follow the CF convention, especially when dealing with large datasets. This makes it difficult to interpret and analyze the data in its raw format, without the ability to visualize it. Additionally, the files can be very large and contain many variables related to the WRF simulation that may not be necessary for a research project.\nModifying the WRF registry can help to address some of these issues by allowing the user to change the variables included in the WRFOUT files. This can be a tedious and potentially messy process though, as some variables may be included in WRFOUT for reasons that the user may never know. As such, it is important to consider the potential unintended consequences of removing certain variables. Furthermore, the variables will still be on the staggered grid and on the model vertical levels, so visualization can still be challenging.\nTo address this issue, users can use the NCL to translate the netCDF output into a format that follows the CF convention. This can help to standardize the metadata and units of the variables, making it easier to interpret and compare the data.\nwrfout_to_cf is an NCL based script designed to create CF compliant NetCDF files with user selectable variables, time reference, vertical levels, and spatial and temporal subsetting. This script is designed to be a simple, user-flexible post-processing utility and is particularly useful for research projects because it produces output files that are more convenient to work with.\nHowever, wrfout_to_cf can be a relatively inefficient post-processing utility and may not be suitable for all applications. It is important to consider the pros and cons of each post-processing utility to determine which one is the best fit for any given research project\n\nAfter running the simulation, WRF will produce three output files, you can check it by typing command in your simulation folder:\n\nIt will return\n\nDownload wrfout_to_cf.ncl from https://sundowner.colorado.edu/wrfout_to_cf/wrfout_to_cf.ncl and put it in the same folder with WRFOUT file. If you check one of file output using below:\nYou will get responses and see the netCDF structure, but it is not easy to understand.\n\nStart to convert the native WRFOUT netCDF file to new CF based netCDF file using below command:\n\nYou will find a new file wrfpost.nc in the folder. Let check it using ncdump command below:\n\nAfter you get the wrfpost.nc in place, you are ready to visualize it using various tools.\n3 Visualizing the Wind\nWith 3 days of hourly information, there are several ways to visualize wind speed and direction to best illustrate the information.\nLet‚Äôs start writing the code using Python.\nAfter importing the library and defining the data, we can start visualizing the wind data.\nHere are some options:\n3.1 Time series plot\nWe can create a time series plot showing the variation of wind speed and direction over the three-day period. This type of plot is useful for identifying patterns or trends in the data over time. We can use Python libraries such as Matplotlib or Seaborn to create time series plots.\nAbove code will produce a map below.\n\nAs an alternative, we can have other visualizations for each grid as a line over time.\nAbove code will produce a chart below.\n\n3.2 Wind rose plot\nA wind rose plot can be used to show the distribution of wind direction and speed over the three-day period. This type of plot is useful for identifying the prevailing wind direction and speed. You can use Python libraries such as Windrose or Matplotlib to create wind rose plots.\nAbove code will produce a chart below.\n\n3.3 Contour plot\nA contour plot can be used to show the spatial distribution of wind speed and direction at a particular time during the three-day period. This type of plot is useful for identifying areas with high or low wind speed and direction. You can use Python libraries such as Matplotlib, Cartopy, or Basemap to create contour plots.\nAbove code will produce a map below.\n\n3.4 Streamline plot\n¬†A streamline plot can be used to show the flow of wind direction and speed at a particular time during the three-day period. This type of plot is useful for identifying the flow of wind over a geographic area. You can use Python libraries such as Matplotlib, Cartopy, or Basemap to create streamline plots.\nAbove code will produce a map below.\n\nOverall, the choice of visualization method will depend on the specific research question or application, and the intended audience. It may be useful to try different visualization methods and compare the results to determine which method is most effective for the task at hand.\n4 Notebook\nThe compilation for all the above code available as a notebook and hosted here: https://gist.github.com/bennyistanto/3f7877b44eebaf0db5e37fa8e7b8603a\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230307-experimental-climatological-rainfall-zone.html",
    "href": "blog/20230307-experimental-climatological-rainfall-zone.html",
    "title": "Experimental climatological rainfall zone",
    "section": "",
    "text": "Climatological rainfall zones represent distinct areas with different rainfall patterns. They are characterized by specific precipitation behaviors, including factors such as the amount, frequency, and timing of rainfall. Understanding these zones is crucial for a variety of reasons:\n\nAgriculture: Agriculture heavily relies on rainfall patterns. Knowing the climatological rainfall zones can help farmers and agricultural planners understand when to plant crops and which types of crops would be most suitable for a given area.\nWater Resource Management: Rainfall contributes significantly to freshwater resources. Understanding the rainfall patterns can aid in planning and managing these resources effectively.\nClimate Change Studies: Changes in rainfall patterns can be an indicator of larger climatic changes. Studying these zones over time can provide insights into climate change.\nDisaster Planning: Areas prone to heavy rainfall could be at a higher risk of flooding. Knowledge of these zones can inform disaster planning and mitigation strategies.\n\nThe Python code provided below aids in the process of identifying climatological rainfall zones. The code does this through the following steps:\n\nPreprocessing: The code first preprocesses the rainfall data to make it suitable for clustering. This includes standardizing the data and reducing its dimensionality using Principal Component Analysis (PCA).\nClustering: The code then applies a clustering algorithm (KMeans or Agglomerative Clustering) to the preprocessed data to identify distinct rainfall patterns, representing different climate zones. The optimal number of clusters is determined using the Calinski-Harabasz and Silhouette methods.\nAssignment: Each location is then assigned to the climate zone of the nearest cluster centroid.\nVisualization: Finally, the code visualizes the identified climate zones on a map, allowing for easy interpretation and application of the results.\n\nIn essence, the code facilitates the data-driven identification and visualization of climatological rainfall zones, providing valuable insights for various applications.\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nThis Python code performs an analysis of climatological rainfall zones, with study case for Uganda, applying KMeans or Agglomerative Clustering to precipitation data sourced from CHIRPS (Climate Hazards Group InfraRed Precipitation with Station) for two time periods: 1981-2010 and 1991-2020. This data is contained in a CSV file that includes unique identifiers, longitude, latitude, and dates.\nHere‚Äôs a summary of what the code does:\n\nImport necessary libraries: The code begins by importing necessary libraries for data manipulation, clustering, standardization, calculating metrics, and visualization.\nChoose the clustering method: The variable cluster_method is set to ‚ÄòKMeans‚Äô by default, but can be changed to ‚ÄòAgglomerativeClustering‚Äô.\nLoad the precipitation data: The CSV data file is loaded into a pandas dataframe. By default, the code reads data for the period 1991-2020, but it can be switched to load the 1981-2010 data.\nData Transformation: The date columns in the dataframe are renamed and reformatted to a datetime object. The dataframe is then ‚Äòmelted‚Äô to convert it into long format, which makes it easier to manage and analyze. The ‚Äòdate‚Äô column is again converted into a datetime object.\nCalculate monthly mean precipitation: The code then calculates the monthly mean precipitation for each location (defined by unique id, longitude, and latitude) by extracting the month from the ‚Äòdate‚Äô column and using it to group the data. This monthly mean precipitation data is then rearranged into a pivot table format for further processing.\nData Visualization: Finally, the transformed dataframe is displayed for visual inspection.\n\nThis initial portion of the code is focused on loading and preparing the data for clustering analysis, which is performed in subsequent steps (not shown in the provided code). Depending on the chosen method, KMeans or Agglomerative Clustering is applied to this monthly mean precipitation data to classify the different climatological rainfall zones. The number of clusters can be a specific integer or determined using an optimal result generated by the Calinski-Harabasz or Silhouette method.\nThis section of the code is about data standardization and dimensionality reduction using Principal Component Analysis (PCA).\nHere‚Äôs the step-by-step explanation:\n\nRemove unnecessary columns: The code first creates a new dataframe X that drops the ‚Äúid‚Äù, ‚Äúlon‚Äù, and ‚Äúlat‚Äù columns from the monthly_precip_df dataframe. This is done because clustering should be based on the rainfall data, not identifiers or coordinates.\nStandardize the data: The data is then standardized using StandardScaler(), which scales the data to have a mean of 0 and a standard deviation of 1. This is a common requirement for many machine learning estimators, as they might behave badly if the individual features do not more or less look like standard normally distributed data.\nApply PCA: Next, PCA is applied to the scaled data to reduce its dimensionality. PCA(n_components=0.90) means that PCA will keep enough components to explain 90% of the variance in the data. This is a way to reduce the complexity of the model and avoid overfitting.\nFit and transform the data: The fit_transform() function fits the PCA model with the scaled data X_scaled and applies the dimensionality reduction on X_scaled.\n\nThe print statement ‚ÄòDone!‚Äô indicates the successful completion of these steps. Now, the data is ready for the clustering step. The transformed data, X_pca, can be used as the input for the clustering algorithms. The PCA transformation is beneficial especially for visualization purposes, as it allows us to plot high-dimensional data in 2D or 3D space, and it can also improve the computational efficiency and performance of the clustering algorithm.\nThis function get_optimal_plot_calinski() calculates the optimal number of clusters for a given dataset X and a specified clustering method (KMeans or AgglomerativeClustering), and then visualizes the results. It does this using the Calinski-Harabasz criterion, which is a method for determining the optimal number of clusters. It operates on the principle that clusters should be compact and well separated.\nHere‚Äôs what the code does, step-by-step:\n\nDefine the model: Depending on the cluster_method parameter, it sets the model to either KMeans or AgglomerativeClustering. If another value is passed, it raises a ValueError.\nDefine the compute_score function: This inner function creates a model with k clusters, fits the model to the data X, and returns the Calinski-Harabasz score. The Calinski-Harabasz score is a measure of cluster validity; higher scores indicate better clustering configurations.\nCalculate scores for range of clusters: It then calculates the Calinski-Harabasz score for each number of clusters in the range from 2 to 20. These computations are performed in parallel to speed up the process, especially beneficial for large datasets.\nFind the optimal number of clusters: The number of clusters (k) that yields the maximum Calinski-Harabasz score is identified as the optimal number of clusters.\nPlot the scores: Finally, it visualizes these scores in a plot, where the x-axis represents the number of clusters and the y-axis represents the corresponding Calinski-Harabasz scores. The optimal number of clusters is marked with a vertical red dashed line, and its value is also displayed on the plot.\nReturn the optimal number of clusters: The function returns the optimal number of clusters as determined by the Calinski-Harabasz criterion.\n\nThis function is used to explore and determine the optimal number of clusters for the dataset, which can then be used in the actual clustering process. It‚Äôs an essential step in unsupervised machine learning tasks like clustering, as deciding on the number of clusters can often be non-trivial.\nThis function get_optimal_plot_silhouette() is designed to compute the optimal number of clusters for a given dataset X using either the KMeans or Agglomerative Clustering method, using the silhouette score as a measure of cluster quality. It also produces a plot of the silhouette scores as a function of the number of clusters.\nHere‚Äôs a step-by-step breakdown of what the code does:\n\nDefine the compute_score function: The function compute_score is defined to calculate the silhouette score for a given number of clusters. The silhouette score measures the quality of a clustering. A higher silhouette score indicates that the instances in the same cluster are similar to each other and different from the instances in other clusters.\nCalculate scores for range of clusters: It then calculates the silhouette score for each number of clusters in the range from 2 to 20. These computations are performed in parallel to speed up the process, which can be especially beneficial for large datasets.\nFind the optimal number of clusters: The number of clusters (k) that yields the maximum silhouette score is identified as the optimal number of clusters.\nPlot the scores: The silhouette scores are then plotted against the number of clusters. The optimal number of clusters is marked with a vertical red dashed line, and its value is also displayed on the plot.\nReturn the optimal number of clusters: Finally, the function returns the optimal number of clusters as determined by the silhouette score.\n\nThe silhouette score is an alternative to the Calinski-Harabasz score for finding the optimal number of clusters in a dataset. It considers both the compactness of the clusters (how close the instances in the same cluster are) and the separation between the clusters (how far apart the clusters are). The optimal number of clusters is the one that maximizes the average silhouette score over all instances.\nThis part of the code calculates the optimal number of clusters for the PCA-transformed data X_pca using two different methods: the Calinski-Harabasz method and the Silhouette method. The clustering method is defined by the variable cluster_method.\n\nCalinski-Harabasz: The function get_optimal_plot_calinski(X_pca, cluster_method) is called to calculate the optimal number of clusters using the Calinski-Harabasz method. This function computes the Calinski-Harabasz scores for different numbers of clusters, plots the scores as a function of the number of clusters, and returns the optimal number of clusters that yields the highest Calinski-Harabasz score. The optimal number of clusters is stored in the variable optimal_c.\nSilhouette: Similarly, the function get_optimal_plot_silhouette(X_pca, cluster_method) is called to calculate the optimal number of clusters using the Silhouette method. This function computes the Silhouette scores for different numbers of clusters, plots the scores as a function of the number of clusters, and returns the optimal number of clusters that yields the highest Silhouette score. The optimal number of clusters is stored in the variable optimal_s.\n\nThen, it prints out the optimal number of clusters as determined by both the Calinski-Harabasz and Silhouette methods. The ‚ÄòDone!‚Äô print statement indicates the successful completion of these steps.\nThis section of the code is crucial as it determines the most suitable number of clusters for the data, which is a key parameter for clustering algorithms. The Calinski-Harabasz and Silhouette methods are two popular methods for determining this optimal number, and comparing their results can provide additional validation for the chosen number of clusters.\n \nThe cluster_data() function takes in the input data array X_pca, the chosen clustering method cluster_method, and an optional number of clusters n_clusters. It performs clustering on the data and returns the cluster labels.\nHere‚Äôs a step-by-step breakdown of what the code does:\n\nCheck for optimal cluster number: If n_clusters is set to 'optimal_c' or 'optimal_s', the function calls the previously defined functions get_optimal_plot_calinski() or get_optimal_plot_silhouette(), respectively, to compute the optimal number of clusters. It does this for the specified clustering method, either 'KMeans' or 'AgglomerativeClustering'.\nRaise an error for invalid input: If n_clusters is not one of the allowed strings and it is not an integer, the function raises a ValueError.\nCreate the cluster model: Depending on the value of cluster_method, the function creates a KMeans or AgglomerativeClustering model with the specified number of clusters.\nFit the model to the data: The function then fits the clustering model to the input data. It also provides a progress bar to track the process.\nRetrieve the cluster labels: After the model has been fitted, the function retrieves the cluster labels, which indicate the cluster to which each data point has been assigned.\nReturn the labels and number of clusters: Finally, the function returns the cluster labels and the number of clusters used in the clustering model.\n\nBy encapsulating the clustering process into a function, the code allows for easy and repeatable clustering of the data using different methods and numbers of clusters. The function also handles the computation of the optimal number of clusters, making it easy to compare the results of different clustering approaches.\nThis section of the code conducts clustering of the data with the specified clustering method and the defined number of clusters (14 in this case). Once the clusters are determined, it assigns a climatic zone to each row based on the nearest centroid. Here‚Äôs a step-by-step breakdown:\n\nPerform Clustering: The function cluster_data(X_pca, cluster_method, n_clusters=14) is invoked to perform clustering on the data. The function returns the labels of the clusters and the number of clusters used, which are stored in labels and n_clusters respectively.\nCalculate Centroids: A centroid is a point at the center of each cluster. It‚Äôs the mean position of all the points in a cluster. The code calculates these centroids for each cluster and stores them in the centroids array.\nCalculate Euclidean Distances: The code then calculates the Euclidean distance between each data point (each row) and each of the cluster centroids. The Euclidean distance is a measure of the straight line distance between two points in a space. These distances are stored in the distances array.\nAssign Closest Cluster: For each row in the dataset, the code assigns the cluster that is closest (has the smallest Euclidean distance) to it. This is achieved using the np.argmin() function, which returns the index of the smallest value along an axis. The assigned clusters are added as a new column climate_zone in the monthly_precip_df dataframe.\nGrouping Data: The dataset is then grouped by id, lon (longitude), and lat (latitude), and for each group, the mode (the most frequently appearing value) of the climate_zone is calculated. This essentially assigns a single climate zone to each unique location based on the most frequent climate zone assigned to it over the timeseries. This grouped data is stored in monthly_precip_df_centroid.\nCompletion Message: Upon successful completion of these steps, a ‚ÄòDone!‚Äô message is printed.\n\nThis section of the code allows each location (represented by a unique combination of id, lon, and lat) in the dataset to be assigned to a specific climatological rainfall zone based on the monthly timeseries precipitation data. This information can be useful for various climatological and environmental studies.\nThe function plot_climate_zone_map(climate_zone_csv, shapefile_path) is designed to generate a scatter plot of climate zones over a given geographical region, which could be a country or a continent, for instance. The plot utilizes longitude and latitude coordinates from a CSV file and a polygon shapefile to depict the geographical boundaries of the region of interest. Here‚Äôs a breakdown of the steps:\n\nLoad the Data: The function starts by loading a CSV file containing longitude (lon), latitude (lat), and climate zone (climate_zone) data.\nCreate a Scatter Plot: A scatter plot is generated using the longitude and latitude values as x and y coordinates, respectively. The climate zone data is used to color-code the points on the scatter plot.\nLoad the Polygon Shapefile: A polygon shapefile, which represents the geographical boundaries of the region of interest, is loaded using the geopandas library.\nPlot the Polygon Shapefile: The loaded polygon shapefile is overlaid on the scatter plot to provide geographical context. The boundaries are shown as black lines.\nAdd a Colorbar: A colorbar is added to the plot to provide a reference for the color-coding of the climate zones.\nSet the Title and Axis Labels: The plot is given a title, and the x and y axes are labeled as longitude and latitude, respectively. A footnote reference to the study providing the climatological rainfall zone is also included.\nDisplay the Plot: Finally, the plot is displayed using plt.show().\n\nThis function provides a visual representation of the climate zones within a specific geographical region, which can help researchers and policymakers understand the spatial distribution of different climate characteristics based on rainfall patterns.\nThe code snippet provided is performing the following steps:\n\nSave DataFrame to CSV: It saves the monthly_precip_df_centroid DataFrame, which contains the ‚Äòid‚Äô, ‚Äòlon‚Äô, ‚Äòlat‚Äô, and ‚Äòclimate_zone‚Äô columns, into a CSV file. This file will be saved in the location specified by the path string. The filename is constructed using the method of clustering and the number of clusters.\nPrint Confirmation Message: After saving the file, a confirmation message is printed stating ‚ÄúSave the output to csv completed‚Äù. A separator line is then printed for clarity.\nPlot Climate Zone Map: The plot_climate_zone_map function is then called, which generates a scatter plot of climate zones over a given geographical region based on the CSV file saved in the previous step and a shapefile which represents the geographical boundaries.\nPrint Separator Line: Another separator line is printed for clarity.\nPrint Completion Message: Finally, a message is printed stating ‚ÄúDone!‚Äù to signify the end of the code execution.\n\nRemember to ensure that the directory paths used in the code (‚Äú../csv/‚Äù and ‚Äú../shapefiles/‚Äù) exist in your current working directory and contain the necessary files. If not, you will need to change these paths to the appropriate ones that are relevant to your working environment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240103-firmware-upgrade-on-thuraya-satsleeve-for-iphone.html",
    "href": "blog/20240103-firmware-upgrade-on-thuraya-satsleeve-for-iphone.html",
    "title": "Firmware upgrade on Thuraya SatSleeve for iPhone",
    "section": "",
    "text": "I have old version of Thuraya SatSleeve for iPhone, now categorized as a legacy product on Thuraya website.\nSince year ago the SatSleeve device experiencing intermittent connection, between bluetooth on iPhone SE and SatSleeve, also the GPS connection. Luckily, Thuraya providing a firmware upgrade with release v3.0.1 on their website https://www.thuraya.com/en/support/upgrades/legacy/thuraya-satsleeve-for-iphone\nBefore upgrading a SatSleeve, please check which firmware is installed (SatSleeve &gt; Settings &gt; Device Info &gt; Firmware version). Perform the upgrade only if Thuraya releases a firmware version newer than your existing one (mine is v2.94)\nTo upgrade the firmware, follow steps on the website:\nStep 1\n\nDownload the below SatSleeve Upgrader program.\nSatSleeve upgrader\nUnzip and Run the setup file - the Upgrader program including the USB driver will be installed.\n\nStep 2\n\nDownload the latest Thuraya SatSleeve firmware release to your hard disk.\nSatSleeve iPhone firmware release v3.0.1\n(works only on the SatSleeve for iPhone Data model) - Unzip it.\nRelease notes of v3.0.1: Fixed GPS rollover issues\n\nStep 3\n\nConnect your SatSleeve with the PC/laptop via USB data cable.\nYou can now start the SatSleeve Upgrader program (please make sure you run this software as Administrator.\n\nThe requirement is using PC with Windows 8/8.1, Windows 7 or Windows Vista, and I am using Windows 11 is working fine\nRight click Thuraya SatSlevee Upgrader &gt; More &gt; Run as administrator\n\nand locate the firmware on your hard disk. The Upgrader program will help you through the upgrade process.\n\nNow, my SatSleeve is working fine on iPhone SE\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240416-utilizing-cuda.html",
    "href": "blog/20240416-utilizing-cuda.html",
    "title": "Utilizing CUDA",
    "section": "",
    "text": "This week I try to utilize CUDA on my desktop, to support the upcoming activities on heavy geospatial and climate analytics. Bit tricky but I managed to install it in both Windows 11 and WSL2 Debian 12. See below.\n\nInstall CUDA and cuDNN using Conda\nTested on:\nWindows 11 Pro for Workstations and WSL2 Debian 12\nProcessor: Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz 2.00 GHz (2 processors)\nInstalled RAM: 384 GB\nVGA: NVIDIA Quadro P2000 5GB\n\n\n1. Install the GPU driver\nThis step only apply to Windows\nDownload and install the NVIDIA Driver for GPU Support to use with your existing CUDA ML workflows. For my case, I choses:\n\nProduct type: NVIDIA RTX/Quadro\nProduct series: Quadro Series\nProduct: Quadro P2000\nOperating System: Windows 11\nDownload Type: Production Branch/Studio\nLanguage: English (US)\n\nClick Search, then you will Click Download, follow with Click on Agree & Download. It will grab a file from this link https://us.download.nvidia.com/Windows/Quadro_Certified/551.86/551.86-quadro-rtx-desktop-notebook-win10-win11-64bit-international-dch-whql.exe with size 483 MB.\nNext, install and follow to step until completed.\n\nNote\nThis is the only driver we need to install. Do not install any Linux display driver in WSL.\nReference: https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl-2\n\n\nStep 2-7 below, apply for both Windows and WSL\n\n\n2. Create new Conda environment\nOpen Anaconda Prompt on Windows or Terminal on WSL (I am sure both are in the same Windows Terminal with different Tab). Please make sure we are outside the Conda environment, by typing:\nconda deactivate\nLet‚Äôs create new Conda environment, called cudawith Python version 3.11\nconda create -n cuda python==3.11\n\n\n3. Install essential Python package for geospatial analysis and data visualization\nI would like to use this cuda env to do heavy geospatial and climate data process, so I will install Python geospatial package\nconda install -c conda-forge geospatial\nIf needed, we can install other package too. Example: cdo, nco, gdal, awscli\n\ncdo package only available in Linux (WSL) environment.\n\n\n\n4. Install CUDA toolkit\nInstall cudatoolkit v11.8.0 - https://anaconda.org/conda-forge/cudatoolkit\nconda install -c conda-forge cudatoolkit\n\n\n5. Install cuDNN\nInstall cudnn v8.9.7 - https://anaconda.org/conda-forge/cudnn\nconda install -c conda-forge cudnn\n\n\n6. Install Pytorch\nInstall Pytorch - https://pytorch.org/\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\n\n7. Install Tensorflow\nInstall Tensorflow 2.14.0, as this is the last Tensorflow compatible version with CUDA 11.8. Reference: https://www.tensorflow.org/install/source#gpu\nconda install -c conda-forge tensorflow=2.14.0=cuda118py311heb1bdc4_0\n\n\n8. Setting the Library\nThis step only apply to WSL\nIf we installed CUDA and cuDNN via Conda, then typically we should not need to manually set LD_LIBRARY_PATH or PATH for these libraries, as describe by many tutorial when we install the CUDA and cuDNN system-wide, because Conda handles the environment setup for us.\nHowever, sometimes we are encountering issues like - errors related to cuDNN not being registered correctly - there might still be a need to ensure that TensorFlow is able to find and use the correct libraries provided by the Conda environment.\nWhy We Might Still Need to Set LD_LIBRARY_PATH?\nEven though Conda generally manages library paths internally, in some cases, especially when integrating complex software stacks like TensorFlow with GPU support, the automatic configuration might not work perfectly out of the box.\nFind the library paths: We can look for CUDA and cuDNN libraries within the Conda environment‚Äôs library directory:\nls $CONDA_PREFIX/lib | grep libcudnn\nls $CONDA_PREFIX/lib | grep libcublas\nls $CONDA_PREFIX/lib | grep libcudart\nManually Set LD_LIBRARY_PATH (If Needed)\nIf we find that TensorFlow still fails to recognize these libraries despite them being present in the Conda environment, we might try setting LD_LIBRARY_PATH manually:\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\nIn my case, I have set the PATH in .zshrc, so above approach is already done\n# Anaconda \n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/home/bennyistanto/anaconda3/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/bennyistanto/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/bennyistanto/anaconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/home/bennyistanto/anaconda3/bin:$PATH\"\n        export LD_LIBRARY_PATH=\"/home/bennyistanto/anaconda3/lib:$LD_LIBRARY_PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nBased on my .zshrc settings and the Conda environment settings, my LD_LIBRARY_PATH is already set to include the Conda libraries at /home/bennyistanto/anaconda3/lib. This should generally be sufficient for TensorFlow to locate and use the CUDA and cuDNN libraries installed via Conda, given that Conda typically manages its own library paths very well.\nEvaluation of Current Setup\nSince I‚Äôve already set LD_LIBRARY_PATH in my .zshrc, TensorFlow should correctly recognize and utilize the CUDA and cuDNN libraries installed in my Conda environment, assuming there are no other conflicting settings or installations. The LD_LIBRARY_PATH in my .zshrc appears correctly configured to point to the general Conda library directory, but there are a few additional things we might consider:\nMake sure we are stil working inside cuda environment.\nIf TensorFlow continues to have issues finding or correctly using the cuDNN libraries, we might consider adding a direct link to the specific CUDA and cuDNN library paths in LD_LIBRARY_PATH within our Conda activation scripts. We can modify the environment‚Äôs activation and deactivation scripts as follows:\n\nActivate Script ($CONDA_PREFIX/etc/conda/activate.d/env_vars.sh):\n#! /bin/sh\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\nDeactivate Script ($CONDA_PREFIX/etc/conda/deactivate.d/env_vars.sh):\n#! /bin/sh\nexport LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | sed -e \"s|$CONDA_PREFIX/lib:||g\")\n\nThis explicitly ensures that our specific Conda environment‚Äôs library path is prioritized while the environment is active.\nIn my case (as I am working inside cuda environment, $CONDA_PREFIX = /home/bennyistanto/anaconda3/envs/cuda\nIf the env_vars.sh file does not exist in both the activate.d and deactivate.d directories within our Conda environment, we should create them. These scripts are useful for setting up and tearing down environment variables each time we activate or deactivate our Conda environment. This ensures that any customizations to our environment variables are applied only within the context of that specific environment and are cleaned up afterwards.\nHere‚Äôs how to create and use these scripts:\nStep 1: Create the Directories\nIf the activate.d and deactivate.d directories don‚Äôt exist, we‚Äôll need to create them first. Here‚Äôs how we can do it:\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\nmkdir -p $CONDA_PREFIX/etc/conda/deactivate.d\nStep 2: Create the Activation Script\nCreate the env_vars.sh script in the activate.d directory. This script will run every time we activate the environment.\n\nNavigate to the directory:\ncd $CONDA_PREFIX/etc/conda/activate.d\nCreate and edit the env_vars.sh file:\nnano env_vars.sh\nAdd the following content to set up the LD_LIBRARY_PATH:\n#!/bin/sh\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\nSave and exit the editor (in nano, press Ctrl+O, Enter, and then Ctrl+X).\n\nStep 3: Create the Deactivation Script\nSimilarly, create the env_vars.sh script in the deactivate.d directory. This script will clear the environment variables when we deactivate the environment.\n\nNavigate to the directory:\ncd $CONDA_PREFIX/etc/conda/deactivate.d\nCreate and edit the env_vars.sh file:\nnano env_vars.sh\nAdd the following content to unset the LD_LIBRARY_PATH:\n#!/bin/sh\nexport LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | sed -e \"s|$CONDA_PREFIX/lib:||g\")\nSave and exit the editor.\n\nStep 4: Make Scripts Executable\nEnsure that both scripts are executable:\nchmod +x $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nchmod +x $CONDA_PREFIX/etc/conda/deactivate.d/env_vars.sh\nStep 5: Testing\nActivate our environment again to test the changes:\nconda deactivate\nconda activate cuda\nCheck that the LD_LIBRARY_PATH is correctly set:\necho $LD_LIBRARY_PATH\nThis should reflect the changes we‚Äôve made, showing that the library path of our Conda environment is included.\nIn my case, the output from echo $LD_LIBRARY_PATH shows /home/bennyistanto/anaconda3/envs/cuda/lib: indicates that my LD_LIBRARY_PATH is correctly set to include the library directory of our Conda environment named ‚Äúcuda‚Äù. This setup is what we want because it directs the system to look in our Conda environment‚Äôs lib directory for shared libraries, such as those provided by CUDA and cuDNN, which are crucial for TensorFlow to correctly utilize GPU resources.\n\n\n9. Configure Jupyter Notebook\nTo configure Jupyter Notebook to use GPUs, we need to create a new kernel that uses the Conda environment we created earlier cuda and specifies the GPU device. We can do this by running the following command:\npython -m ipykernel install --user --name cuda --display-name \"Python 3 (GPU)\"\nThis command installs a new kernel called ‚ÄúPython (GPU)‚Äù that uses the cuda Conda environment and specifies the GPU device.\n\nVoila, the installation process is completed. Next we can test using test_GPU.ipynb\nGithub Gist file: https://gist.github.com/bennyistanto/46d8cfaf88aaa881ec69a2b5ce60cb58\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210311-today-a-year-ago.html",
    "href": "blog/20210311-today-a-year-ago.html",
    "title": "Today - a year ago",
    "section": "",
    "text": "Today marks the first anniversary of my office‚Äôs decision to asks all staffs to work from home (WFH) due to COVID-19.\nThis means a lot for me and changed the way on how everybody works. For me, WFH means an escape from wasting my lives commuting from Bogor - Jakarta - Bogor. I have done the maths, on average, Jakarta/Bogor commuters spend 30 days a year traveling to and from work.\nSome fun stuff during WFH too, I can:\n\nTeach my daughter how to code using Scratch, and now we are starting to use Lua\nReplace my wife‚Äôs duty, frequently buy vegetables and recipes at the vegetable vendors or local market\nNetflix movie marathon\n\nThe pandemic still not over, please stay at home, stay healthy and wear your mask. I know wearing a mask is not a political right, but it respect other lives.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20110810-first-try-using-tilemill-and-tilestream.html",
    "href": "blog/20110810-first-try-using-tilemill-and-tilestream.html",
    "title": "First try using TileMill and TileStream",
    "section": "",
    "text": "Since it was announced through Development Seed‚Äôs blog, I am really excited to try TileMill and TileStream. Currently I am living in remote area where internet connection bit a problem. But luckily I have a friend who is happy to send me a CD containing the pre-installed Virtual Box Image (VDI) with RedHatLinux, including TileMill.\nThen finally Development Seed team released 0.4.0 installer for Mac OS X, makes me more even excited to try both TileMill and TileStream.\nBelow is my project to visualize Aceh terrain.\n\nAnd next picture is TileStream running in my local Mac\n\nCurrently I am trying to visualize 2010 Indonesia Land Cover data from Ministry of Forestry. Below is the MSS code:\nlandcover.mss\nlabel.mss\nstyle.mss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am very satisfied with the result from TileMill and how the mbtiles loaded in TileStream: FAST.\nNew cartographic technique using new technology.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20160510-2016-gis-for-sustainable-world-conference.html",
    "href": "blog/20160510-2016-gis-for-sustainable-world-conference.html",
    "title": "2016 GIS for Sustainable World Conference",
    "section": "",
    "text": "Early this month I had the opportunity to attend the¬†2016 GIS for a Sustainable World Conference held on May 2‚Äì4, 2016 in Campus Biotech, Geneva, Switzerland. View slides from the event via this link: https://proceedings.esri.com/library/userconf/unic16/index.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20061221-jacub-rais-award.html",
    "href": "blog/20061221-jacub-rais-award.html",
    "title": "Jacub Rais Award",
    "section": "",
    "text": "Minggu lalu saya mengikuti Pertemuan Ilmiah Tahunan XV dan Kongres IV Masyarakat Ahli Penginderaan Jauh Indonesia (MAPIN), yang diadakan di Bandung, 13 - 14 Desember 2006.\nSaya mengirimkan makalah yang saya tulis bersama Pak Idung Risdiyanto dan Pak Rokhis Khomarudin dengan judul ‚ÄúPenyusunan perangkat lunak untuk sistem peringatan dini kebakaran hutan di Indonesia‚Äù. Saya mendapatkan jadwal presentasi pada hari kedua, ini merupakan pengalaman pertama yang sangat berkesan dan mendebarkan. Karena dari daftar pemakalah yang ada, sepertinya hanya saya yang masuk kategori ‚Äúanak kemarin sore‚Äù karena baru 5 bulan menjalani wisuda S1, sedangkan yang lain namanya sudah cukup terkenal di dunia SIG dan Penginderaan Jauh di Indonesia.\nSetelah selesai sesi presentasi dan tanya jawab yang berjalan cukup lancar, panitia memngumumkan bahwa saya mendapatkan Jacub Rais Award untuk kategori Penulis Terbaik.\nKaget dan senang tentunya.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Gambar 1. Sertifikat dari MAPIN [/caption] \nJadwal PIT MAPIN 2016 bisa dilihat di link ini dan makalah lengkapnya ada di link ini.\nGambar di bawah ini merupakan salah satu output yang saya tampilkan dalam presentasi, user interface pengolahan FDRS menggunakan input data wilayah.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2500‚Äù] Gambar 3. Indonesia FDRS [/caption]\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20171213-openstreetmap-mapathon-in-atambua.html",
    "href": "blog/20171213-openstreetmap-mapathon-in-atambua.html",
    "title": "OpenStreetMap Mapathon in Atambua",
    "section": "",
    "text": "My office frequently conduct an OpenStreetMap (OSM) Mapathon series, to improve the quality and speed of disaster preparedness, response and recovery in Indonesia through the development of an up to date and detailed base map for decision-makers and planners.\nLast week, I went to Atambua, Belu District, East Nusa Tenggara to conduct the OSM Mapathon with the OSM Indonesia Association (POI) folks from Jakarta. This activity is designed to complement attribute data collection of the building footprints and road network already mapped on OpenStreetMap. The results of this mapping will be further analysed by the World Food Programme (WFP) for food security and other humanitarian issues. The survey will particularly collect information on public facilities and infrastructures, such as traditional markets, road networks, health facilities (clinics, maternal and child health services, hospitals), educational facilities, houses of worship, and government offices. The POI worked with WFP and Indonesian Red Cross (PMI) - Belu office, to map public infrastructures in Belu District. Previously, PMI has organized their own mapping programme using GPS and drones in four priority villages in Belu, including Sarabau, Ranrua, Fatuketi, and Rafae Villages.\nBelu District consists of 12 subdistricts directly adjacent to East Timor. It is a mountainous area with the Atambua City takes place at its city centre. These places were among the main priorities during the survey so it can reach out to all areas in Belu. Before starting the survey, participants were equipped with basic OSM trainings for two days, and it continued with QGIS training on the last day.\nThe first day was started with training courses on the introduction to OSM, creating account, operating JOSM, operating Tasking Manager, and an introduction to surveying tools, such as GPS, OSMTraker, OMK, and ODK Collect. Some of the participants already have experience using GPS for field surveys in their respective villages for Red Cross activities, making them familiar with field survey training materials. The session was concluded with assigning participants into smaller groups for exercises to survey the areas nearby the training venue using the newly introduced tools.\nParticipants were divided into four smaller groups, each consisting of one trainer, one participant from Belu Red Cross, and 1 from Red Cross from outside of Belu. The group assignment remains the same for the infrastructure surveys in Day 3 and 4.\nIn the second day, the training was started with a survey exercise surrounding the training venue within 2 km. In the exercise, each group brings smartphones with pre-downloaded satellite imagery for the survey area, complete with its administration boundaries.\nThe field survey exercise lasted for about 40-50 minutes and was continued with data entry exercise using JOSM. After inputting all survey data to OSM, participants inputted GPX data from their respective area that are not yet available on OSM.\nParticipants had previously digitized their villages using Tasking Manager created by PMI, this makes the process of adding attribute data run smoothly. In the second day‚Äôs sessions, the training was continued with conflict resolution and quality assurance on OSM. Courses on resolving conflict is an additional courses as some participants experienced data conflict from their survey outcomes. The sessions were concluded with a briefing for field survey Day 3 and 4. The infrastructure data that were collected, include:\n\nTraditional markets\nRoad networks\nHealth facilities, especially small maternal and child health services\nEducation facilities\nHouses of worship\nGovernment offices\n\nParticipants collect infrastructure data for the whole Belu Regency on the Day 3 and 4 using rented cars. There are 12 subdistricts, divided among the four groups.\nThe field survey run efficiently, as each group traveled with one group member who are from the local PMI community members that can help navigate and who already recognize the objects that become the field survey priority objects. The following is the outcome of the survey:\nParticipants will continue with uploading their *.gpx data from their respective area as the training venue‚Äôs internet connection could not support large files upload, there are still several objects data already collected but are yet to be uploaded to the OSM. Once participants thoroughly understand how to map with little difficulties, we continued with training how to download OSM data using HOT Export so they can use the data to make their own maps. Participants then learned how to process the data using QGIS. These data will then be customised with symbology and labels. The session ended with map making with Map Composer QGIS. Participants went through this course fairly smoothly as they already understood Geographic Information System and are comfortable using laptops.\nThe data collection training was concluded by announcing the winners, evaluated with the following criteria:\n\nParticipants with the highest points at the post test and the highest margin between post and pre test points.\nParticipants with the best map composing skills and was most active during the training.\nParticipants who could answer impromptu questions in the forum\n\nBelow are some pictures from the Mapathon.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20220501-maximizing-thinkpad-t480.html",
    "href": "blog/20220501-maximizing-thinkpad-t480.html",
    "title": "Maximizing Thinkpad T480",
    "section": "",
    "text": "I never owned a Thinkpad before, but I used Thinkpad since 2008, T400 version from my office, and every 2-3 years got upgrade to the latest.\nFor my personal daily use, I am happy with my Macbook Pro 13 2011 and Mac mini Server 2012.\n\nRecently, I have my first own Thinkpad T480 (released on April 2019) with standard spec (it‚Äôs secondhand btw, with very good condition, and still under warranty up to this month).\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3024‚Äù] Unboxing the T480 [/caption]\nI decided to choose the T series (T14, L14, T480 and T490 on my list) and finally with T480 after few months learning the best type to support my works. It‚Äôs came with i7 8650U 1.9ghz vPro (8 CPUs), Intel UHD620 4GB, DDR4 8GB, 512GB SSD Sata3, HD display 1366x768, non backlight keyboard, 6-cell external and without wwan and internal batteries.\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2913‚Äù] Let see inside [/caption]\nThen I start to upgrade some parts:\n\n2 x 32GB DDR4 Corsair Vengeance 2666MHz. https://www.corsair.com/ww/en/Categories/Products/Memory/Vengeance-SODIMM/p/CMSX32GX4M1A2666C18\nI am aware the original RAM for T480 is 2400MHz, but it‚Äôs difficult to find it. I also surprised the 2666 is cheaper than 2400, and easy to find from the marketplace. Is it dangerous to use 2666MHz in a 2400MHz laptop? No, the ram will work fine, and the 2666 frequency will under-clocked to 2400.\n870 QVO SATA III 2.5‚Äù SSD 4TB. https://www.samsung.com/us/computing/memory-storage/solid-state-drives/870-qvo-sata-iii-2-5--ssd-4tb-mz-77q4t0b-am/\nInternal Battery, 24 Wh. https://www.amazon.com/01AV421-01AV419-SB10K97577-SB10K97576-Replacement/dp/B09N1HYTTX\nInstall the new Windows 11 - https://www.microsoft.com/software-download/windows11, previously was Windows 10\n\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú4032‚Äù] Installing new SSD, RAM and internal battery [/caption]\nI also change the US non-backlight to backlight keyboard too. https://www.tokopedia.com/laptopstorebatam/keyboard-thinkpad-t470-t480-keyboard-thinkpad-t480-t470-backlight\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú4032‚Äù] Woohooo, it has backlight now [/caption]\nAnd few days ago I just completed another upgrades:\n\nReplace the HD screen into WQHD 2560x1440 along with the 40pins cable, as the original cable came with 30pins.\nParts:\n\nGenuine WQHD Screen for Lenovo ThinkPad T480 T480s X1 Carbon,¬†with an FRU of 00NY664 and LP140QH2-SPB1. https://www.amazon.com/dp/B09FZ9TJBT?psc=1&ref=ppx_yo2ov_dt_b_product_details\nOriginal LCD Cable for Lenovo ThinkPad T480 01YR503 Windu-2 (20L5/20L6) FRU WQHD LCD Cable, WN-2. https://www.amazon.com/dp/B093GKPPN8?psc=1&ref=ppx_yo2ov_dt_b_product_details\n\nInstall wwan card + antenna kit.\nParts:\n\nSenFend L850-GL CAT9 WWAN 4G LTE Module for Lenovo and Thinkpad T480 T480S T490 T495S T590, FRU: 5W10V25803 OR 01AX792. https://www.amazon.com/dp/B09Q2T7CBV?psc=1&ref=ppx_yo2ov_dt_b_product_details\nGinTai WLAN WWAN Antenna Kit Replacement for Lenovo ThinkPad T480 T470 3G 4G 01YR494 01YR495. https://www.amazon.com/dp/B083S5JJ8J?psc=1&ref=ppx_yo2ov_dt_b_product_details\n\n\n\n\n\n\n\n\n\n\n\n\n\nHave a look, cellular data option is available and my screen is better now!\n \nI did all the installation myself by following the awesome guideline https://www.ifixit.com/Device/Lenovo_ThinkPad_T480 and use Pro Tech Toolkit https://www.ifixit.com/Store/Tools/Pro-Tech-Toolkit/IF145-307 from IFIXIT.\nI am quite satisfied and excited, hope it lasts until next 5 years.\nBack to work, to support my daily activities on climate analytics and geospatial technology, I installed various application:\n\nArcGIS Desktop 10.8.2\nArcGIS Pro 2.9\nR Statistics 4.1.3 and RStudio\nTotal Commander\nWindows Subsystem for Linux 2, and GDAL/OGR, CDO, NCO, geopandas, xarray, rasterio, jupyter notebook, etc inside Anaconda.\n\nUPDATE: 4 Jun 2022\nI replaced existing Wi-Fi card Intel Model 8265NGW with Intel Killer 6E AX1675x Tri Band AX210 M.2 2230 Bluetooth 5.2 Wi-Fi Card.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20190531-summer-in-mongolia.html",
    "href": "blog/20190531-summer-in-mongolia.html",
    "title": "Summer in Mongolia",
    "section": "",
    "text": "I visited Mongolia again last week during Summer season, it was nice. Summer is the warmest season and the best season to travel to Mongolia. The summer can be unpredictable, sometimes it gets really hot but then it can be rainy for several days.\nThis time, I have follow-up from previous meeting early this year with NEMA, NAMEM, NSO, ALAGAC and MercyCorps.\nHere‚Äôs some pictures in UB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20060623-multi-person-decision-making.html",
    "href": "blog/20060623-multi-person-decision-making.html",
    "title": "Multi Person Decision Making",
    "section": "",
    "text": "Problem?\nPrediction on weather phenomena could be done using data from weather station. For small certain location may be could use data from one station. But for global area would be need more than one weather station. Formulated parameter to predict weather conditions for all weather station in the large area may different between each of stations. For example if we have a watershed for study area; this area maybe has more than one weather station, then we must concern about parameters that give big influences for weather conditions. Temperature 25¬∞C on the low land maybe would give the same impact if on the up land has same temperature but different air humidity.\nI would like to determine weather information from 7 stations in certain watershed and each station has the different rank of weather stations data that give impact on weather conditions for this area. Information on that would needed if we want to create generalisation which weather data will given big influence of watershed area.\nFormulated of the problem\nThe problem will be solved with multi person decision-making. Derived from 5 weather stations data, we get 6 kind of weather data, they are :\n\nAir Temperature (a)\nAir Relative Humidity (b)\nAir Pressure ¬©\nWind (d)\nSolar radiance (e)\nSunshine hours (Measured by Campbell Stokes Instrument) (f)\n\nWeather data priority to predict weather conditions each of stations:\n\nW1 = (d, a, c, b, e, f)\nW2 = (a, d, f, b, e, c)\nW3 = (e, a, b, d, c, f)\nW4 = (c, f, d, b, a, e)\nW5 = (a, f, e, d, c, b)\n\nDetermine the degree of weather data of priority by below equation:\n\nS(xi,xj) = N(xi,xj) / n\nN(xi,xj) is number of stations preferring xi over xj\nn is number of stations\n\nCreating fuzzy preference relations in matric (would be created in computer program)\nSelected maximum degree of group preference of alternative xi over xj (would be created in computer program)\nDetermine data prefering xi over xj which recommendation by computer program\nDetermine possible combination\nProblem Solving\n\nBased on result above program we can determine the possible combination with the maximum degree of group preference is 0.8 and data preferring xi over xj has been recommended are :\n\na &gt; b\na &gt; c\na &gt; e\nd &gt; b, and\nd &gt; c\n\nThe possible data ranking and combination are :\n\na, d, c, b, e, f\na, d, b, c, e, f\na, d, b, c, f, e\na, d, e, f, b, c\na, d, e, f, c, b\na, d, f, e, b, c\na, d, f, e, c, b\nd, a, c, b, e, f\nd, a, b, c, e, f\nd, a, b, c, f, e\nd, a, e, f, b, c\nd, a, e, f, c, b\nd, a, f, e, b, c\nd, a, f, e, c, b\netc.\n\nNote : Possible combination must be put a or d in the first or second series.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20220803-heat-wave-duration-index.html",
    "href": "blog/20220803-heat-wave-duration-index.html",
    "title": "Heat wave duration index",
    "section": "",
    "text": "The World Meteorological Organization (WMO), defines a heat wave as five or more consecutive days of prolonged heat in which the daily maximum temperature is higher than the average maximum temperature by 5¬∞C (9¬∞F) or more.\nTo measure the heat wave index, one of recommendation of the Intergovernmental Panel on Climate Change (IPCC) was to use the heat wave duration index (HWDI). This index has been defined as as the maximum period in each year of at least 5 consecutive days where the maximum temperature is at least 5¬∞C warmer than the daily climatology.\n\nHow-to?\nTo calculate the HWDI, you need to have timeseries daily maximum temperature data and tools to calculate the index. Please follow below procedure to get started.\n\nTools\n\nWindows Subsystem for Linux (WSL) - https://docs.microsoft.com/en-us/windows/wsl/install. Since Climate Data Operator (CDO) is not available for Windows, the easiest way to use it is via the WSL. If you are on Linux/macOS, then you are good to go.\nAnaconda or Miniconda\nClimate Data Operator (CDO) - https://code.mpimet.mpg.de/projects/cdo, installed using conda install -c conda-forge cdo. Please create a new environment to install CDO, just to make sure the CDO will not breaking your current python environment.\n\n\n\nData\n\nGlobal high resolution (30 arc sec ~ 1km) daily maximum temperature data (tasmax), 1979 - 2016, available as 1 month data per 1 file, from CHELSA - https://chelsa-climate.org/, downloaded from https://data.isimip.org/datasets/92b05291-fbe5-4ed2-b3df-29ff0cded9f2/.\n\n\n\nCase\nCalifornia, US\n\n\nStep-by-step\nLets start!\nNOTE\n\nTo follow below step, you need to download CHELSA tasmax data from above link. Total size for tasmax data is 1.2TB, please make sure you have bandwidth and unlimited data package\nOr you can proceed directly to step 2, using pre-computed result for step 1 which is available in folder 01_tasmax, with size 9.75GB.\nAlso make sure you have plenty of storage. Step 2 and 3 will produce intermediate output each around 40GB, step 4 and 5 each around 80GB, step 6 around 2GB and step 7 around 430MB.\n\n\nClip global data using bounding box\nfor fl in ./00_chelsa_global_tasmax/*.nc; do cdo sellonlatbox,-8.3,-7.6,12.3,12.9 $fl ./01_tasmax_/usa_california_`basename $fl`; done\nMerge monthly data into annual\nfor year in {1979..2016}; do cdo mergetime ./01_tasmax/usa_california_chelsa-w5e5v1.0_obsclim_tasmax_30arcsec_global_daily_${year}??.nc ./02_tasmax_annual/usa_california_chelsa_daily_tasmax_${year}.nc; done\nDelete data who has 29th February.\nAccording to https://code.mpimet.mpg.de/boards/2/topics/9522, dealing with leap years in climate model output has caused many scientists to stumble, since there are just about 25% February 29ths in the time series so that the sample size for February 29th is just not big enough. Therefore, remove it from the time series before analysing it.\nfor year in {1979..2016}; do cdo -delete,month=2,day=29 ./02_tasmax_annual/usa_california_chelsa_daily_tasmax_${year}.nc ./03_tasmax_del29feb/usa_california_chelsa_daily_tasmax_${year}.nc; done\nbelow script also works, using del29dfeb\nfor year in {1979..2016}; do cdo -del29feb ./02_tasmax_annual/usa_california_chelsa_daily_tasmax_${year}.nc ./03_tasmax_del29feb/usa_california_chelsa_daily_tasmax_${year}.nc; done\nConvert Kelvin to degree Celsius and don‚Äôt forget to change the variable (here tasmax) units, too. Combining operators:\nfor year in {1979..2016}; do cdo -b 32 -setattribute,tasmax@units=\"degC\" -addc,-273.15 ./03_tasmax_del29feb/usa_california_chelsa_daily_tasmax_${year}.nc ./04_tasmax_celsius/usa_california_chelsa_daily_tasmax_${year}.nc; done\nMerge all nc files result from point 4 into single nc.\ncdo mergetime ./04_tasmax_celsius/usa_*.nc ./05_tasmax_all/usa_california_chelsa_daily_tasmax_1979_2016.nc\nCalculate the mean TXnorm of daily maximum temperatures for any period used as reference\ncdo ydrunmean,5,rm=c ./05_tasmax_all/usa_california_chelsa_daily_tasmax_1979_2016.nc ./06_tasmax_meanofreference/usa_california_chelsa_daily_tasmaxnorm_ref_1979_2016.nc\nCalculate annual heat wave duration index w.r.t mean of reference period\nfor year in {1979..2016}; do cdo eca_hwdi ./04_tasmax_celsius/usa_california_chelsa_daily_tasmax_${year}.nc ./06_tasmax_meanofreference/usa_california_chelsa_daily_tasmaxnorm_ref_1979_2016.nc ./07_hwdi/usa_california_chelsa_daily_hwdi_${year}.nc; done\n\n\n\nResult\nYear 2008\n\nReference:\n\nhttps://web.archive.org/web/20100915191025/https://www.usatoday.com/weather/news/2008-06-19-socal-heat-wave_N.htm\nhttps://www.upi.com/Top_News/2008/07/06/Heat-wave-coming-to-scorched-California/13351215345710/\n\nYear 2013\n\nReference:\n\nhttps://www.nbclosangeles.com/news/local/Southern-California-Heat-Wave-Record-Hot-Temperatures-213729901.html\nhttps://web.archive.org/web/20131105060318/http://www.wunderground.com/blog/JeffMasters/comment.html?entrynum=2451\n\nYear 2016\n\nReference:\n\nhttps://www.huffpost.com/entry/record-heat-wildfires-west-us_n_57678bb4e4b015db1bc9be59?section=\nhttps://web.archive.org/web/20160622004100/https://weather.com/forecast/regional/news/dangerous-record-heat-southwest-plains\nhttps://www.accuweather.com/en/weather-news/blistering-june-heat-fades-in-southwest-after-records-highs-top-125f-phoenix-vegas-/58322509\n\n\n\n\nReferences\n\nhttps://www.wcrp-climate.org/etccdi\nhttp://etccdi.pacificclimate.org/list_27_indices.shtml\nhttps://code.mpimet.mpg.de/projects/cdo/embedded/cdo.pdf\nhttps://code.mpimet.mpg.de/projects/cdo/embedded/cdo_eca.pdf\nhttps://data-infrastructure-services.gitlab-pages.dkrz.de/tutorials-and-use-cases/use-case_climate-extremes-indices_cdo.html\nhttps://en.wikipedia.org/wiki/List_of_heat_waves\n\nCDO operators\n\nsellonlatbox - Select a longitude/latitude box. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-1760002.3.5\nmergetime - Merge datasets sorted by date and time. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-1130002.2.8\ndelete - Delete fields. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-1540002.3.1\ndel29feb - the special CDO operator to remove all the ‚Äò29th of february‚Äô. This operator is not yet documented.\naddc - Add a constant. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-3360002.7.3\nsetattribute - Set attribute. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-2380002.6.1\nydrunmean - Multi year daily running mean. https://code.mpimet.mpg.de/projects/cdo/embedded/index.html#x1-5860002.8.37\nrm - The read_method can be set to c for circular which takes into account the last time steps at the begin of the time period and vise versa. Otherwise, the first and last time steps are not used as often as the other time steps in the calculations.\neca_hwdi - Heat wave duration index. https://code.mpimet.mpg.de/projects/cdo/embedded/cdo_eca.pdf\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20031008-halo.html",
    "href": "blog/20031008-halo.html",
    "title": "Halo!",
    "section": "",
    "text": "ACC\n\n\nHari ini saya memulai sebuah blog, dan ini tulisan pertama saya.\nSaya menulisnya di Agromet Cyber Cafe, Kampus IPB Baranangsiang dan akan mencoba mengisi blog ini dengan hal yang menarik minat saya seperti: komputasi meteorologi dan terapannya untuk pertanian.\nSemoga bermanfaat!\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210618-visualize-daily-weather-forecast-from-gfs-using-google-earth-engine.html",
    "href": "blog/20210618-visualize-daily-weather-forecast-from-gfs-using-google-earth-engine.html",
    "title": "Visualize daily weather forecast from GFS using Google Earth Engine",
    "section": "",
    "text": "Since few years ago NOAA Global Forecast Data (GFS) available at Google Earth Engine (GEE), which makes it easier to access even though there are only 9 variables out of hundreds available.\nAccessing GFS data is tricky, because it can get pretty confusing with the timing. I have discussed it before on access GFS via GRIB filter and getting GFS rainfall forecast using GEE.\nLet‚Äôs start write the code. I would like to visualize all the 9-variable of GFS data using GEE.\nFirst step is to define the geographic domain and write the symbology for all variable. Each variable will have different min-max value and palette.\nNow set the date and data\nNow as the initial process when map loaded, create new variable for each band, select the bands and do filtering based on date and ‚Äúforecast_hours‚Äù. The focus is only grab 1-day forecast, so I will use forecast hours 6,12,18,24.\nI will adapt script from Gennadii Donchyts to visualize Wind Direction as a vector.\nNext script will be adding all layer to the display.\nI am planning to use Date Slider widget, so the user can easily change the date then the map will automatically change. To support that plan, I need to add Reset procedure, so every change on the data, all layers will get refresh.\nLet‚Äôs add Date Slider configuration to Map.\nAccording to the reference, Date Slider required some arguments: start, end, value, period, onChange, disabled, style\nThen the following code will follow to describe the renderDateRange function when the Date Slide onChange is active. The code is similar with initial process when map loaded above..\nNext, I would like to add Download button, so user can easily download all the variable and save it into Google Drive for whatever date they choose. The filename convention will be NameOfVariable_YYYY-MM-DD\nI would like to display all legend for all layer in a one place. To achieve the goal, I need to define some parameters: position in the panel, widget for each feature (title, unit, min-max value and a vertical ramp color as thumbnail image.\nLast configuration will be adding a panel for Legends. As I will display all the legend together, it will take up a space. So activate the panel through the button click are good idea, then I need to add additional button to hide the legend panel if not needed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLink for the full code\nNotes: The code was compiled from various source (GEE help, GEE Groups, StackExchange)\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20060310-sarjana-meteorologi.html",
    "href": "blog/20060310-sarjana-meteorologi.html",
    "title": "Sarjana Meteorologi",
    "section": "",
    "text": "Hari ini saya dinyatakan lulus program sarjana Meteorologi - FMIPA IPB, setelah tadi siang saya menerima Surat Keterangan Lulus (SKL). Resminya saya harus mengikuti upacara wisuda yang akan diselenggarakan dibulan Juni mendatang.\nSemester 7 dan 8 saya jalani dengan cukup tantangan, mulai dari:\n\n1 Maret 2005. Seminar Usul Penelitian\n11 Januari 2006. Seminar Hasil Penelitian\n8 Maret 2006. Sidang tertutup/ujian skripsi selama 4 jam, dengan 3+1 orang penguji: (i) Pak Idung Risdiyanto, (ii) Pak Rokhis Khomarudin, (iii) Pak Sobry Effendi dan (iv) Ibu Laras Tursilowati.\n\nOya yang lebih asik lagi, Senin depan 13 Maret 2006 saya akan mulai bekerja sebagai asisten peneliti di Lembaga Penerbangan dan Antariksa Nasional (LAPAN). Tugas pertama saya adalah memanfaatkan luaran hasil penelitian saya untuk menyempurnakan proses akuisisi data dan perhitungan model kebakaran hutan di LAPAN.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230616-regression-analysis-with-dummy-variables.html",
    "href": "blog/20230616-regression-analysis-with-dummy-variables.html",
    "title": "Regression analysis with dummy variables",
    "section": "",
    "text": "This exercise aims to determine the best reduced model (RM) in regression analysis with dummy variables from annual rainfall data and altitude data in three different regions. This will result in a new regression equation capable of describing the relationship between altitude and rainfall in these three regions.\n\nSummary\nThe dummy variables constructed in this article are based on regional location, specifically regions 1, 2, and 3. The initial analysis entailed the representation of data for each region through scatter plots. Thereafter, a regression analysis with all parameters was conducted to derive the Full Model (FM). Subsequently, the scatter plot patterns for each region were examined, and regression equation models with identical intercepts or slopes were identified. The objective was to generate simpler regression models or equations (Reduced Models - RM) from the dummy variables constructed based on regional location. Upon obtaining several RMs, all were statistically tested using the F-test to ascertain their similarity to the FM. It was also necessary to compute and analyze the Mallows‚Äôs Cp value for all RMs to determine the optimal RM. A good Reduced Model is one that is similar or identical to the Full Model. The F-test performed in this report was designed to determine whether the RM is similar or identical to the FM. The hypotheses for the F-test were as follows.\nH0: FM = RM\nH1: FM ‚â† RM\nThe null hypothesis is refuted in instances where the observed F-value surpasses the F-table value. This suggests that, as of yet, there‚Äôs insufficient robust evidence to proclaim that the Reduced Model (RM) bears resemblance to the Full Model (FM) (Kutner et al., 2005). Within the conducted F-test analysis, a confidence interval of 95% is employed. The observed F-value can be calculated using the following formulation (Kutner et al., 2005):\n\\[F_{\\text{Observed}} = \\frac{(SSR_{FM} - SSR_{RM})/(df_{R,FM} - df_{R,RM})}{SSE_{FM}/df_{E,FM}}\\]\nThe F-table value is derived from the F-distribution with the calculated degrees of freedom\n\\[F_{\\text{table}} = F(df_{R,FM} - df_{R,RM}, df_{E,FM})\\]\nThe RM is considered as efficient or akin to the FM if the Mallows‚Äôs Cp value is equal to or less than the total number of parameters (\\(C_{P,\\text{Mallows}} \\leq p\\)) (Mallows, 1973). The Cp value is determined using the equation:\n\\[C_{P,\\text{Mallows}} = p + \\frac{(S^2 - \\sigma^2)(n - p)}{\\sigma^2}\\]\nwhere \\(p\\) represents the number of parameters utilized in the RM, \\(n\\) denotes the total number of observations within the model (\\(n=45\\)), \\(S^2\\) is the variance of the RM, and \\(\\sigma^2\\) is the variance of the FM.\n\n\nData\nTotal rainfall in different altitude and region. The data available in csv format with columns: altitude; rainfall; region\nThe data for this analysis is available from this link: https://drive.google.com/file/d/1v3CGHBykg3UUqjKS3oyy8rIGsogN1DY5/view?usp=sharing\n\n\nImplementation\nIn the implementation phase of this analysis, we utilized Python and the library to develop dummy variables regression\n\nPlot the input data\nThe code presented aims to investigate the relationship between rainfall and altitude across different regions. The dataset, obtained from a CSV file, contains information on rainfall and altitude for various regions. The code utilizes the pandas library to read the data and matplotlib and seaborn libraries for data visualization.\nTo begin, unique regions in the dataset are identified. A dictionary, regression_params, is created to store the coefficients of the regression equations for each region. Subsequently, a scatter plot is generated for each region, where altitude is plotted on the x-axis and rainfall on the y-axis. This is achieved using the sns.scatterplot function from the seaborn library.\nA linear regression model is then fitted to the data for each region using the LinearRegression class from the sklearn.linear_model module. The model is trained with altitude as the predictor variable (X) and rainfall as the target variable (y). The slope and intercept coefficients of the regression equation are obtained from the fitted model.\nThe regression coefficients are stored in the regression_params dictionary, associating them with their respective regions. Additionally, the regression equation is displayed on the plot for each region using the ax.text function. A regression line is drawn on the plot using the ax.plot function to visualize the relationship between altitude and rainfall.\nThe resulting plot showcases the rainfall-altitude relationship for different regions, with each region‚Äôs data points, regression line, and equation displayed. The plot is saved as an image file, and the figure is displayed for further examination.\nFinally, the regression parameters for each region are printed to provide insights into the specific regression equations obtained. The slope and intercept values are extracted from the regression_params dictionary and displayed for each region.\n [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú1920‚Äù] For region 1, the regression equation is y = 2.43x + 996.53 For region 2, the regression equation is y = 1.99x + 1096.34 For region 3, the regression equation is y = 0.70x + 623.63 [/caption]\nThe provided code snippet focuses on data preprocessing and feature creation based on the information in a CSV file. It employs the pandas library for data manipulation and transformation.\nInitially, the CSV file is read into a DataFrame using the pd.read_csv function, with the resulting DataFrame stored as df.\nNext, several new columns are created based on the region column. These new columns serve as indicator variables to represent different regions in the dataset. Specifically, columns I1, I2, and I3 are generated using logical comparisons to check if the region value matches the respective region number. The astype(int) method is then applied to convert the resulting Boolean values to integers.\nSimilarly, additional columns H1, H2, and H3 are created by multiplying the altitude column with the corresponding indicator variables (I1, I2, and I3). This results in the creation of separate altitude columns for each region, where the altitude values are present only for the respective region and are set to zero for other regions.\nFollowing this, combinations of indicator variables are generated to represent different combinations of regions. Columns I12, I13, I23, and I123 are created using logical comparisons to check if the region value matches the respective region combination. Column I123 is assigned a constant value of 1 since it represents the inclusion of all regions.\nSimilarly, new altitude columns H12, H13, H23, and H123 are created by multiplying the altitude column with the respective combination indicator variables. These columns enable the representation of altitude values for specific region combinations.\nLastly, the modified DataFrame is saved as a new CSV file using the to_csv function, with the file path specified and the separator set to ;. The resulting DataFrame is displayed using the df.head() method to show the first few rows of the transformed dataset.\nIn summary, this code segment demonstrates a data preprocessing step where new columns are created to represent regions and region combinations based on the original data. These transformations facilitate subsequent analysis and modeling tasks by providing a more informative and structured dataset.\n\n\n\nFull model, avoid dummy trap\nThe Full Model regression equation doesn‚Äôt include I3 because of a technique used in regression analysis known as dummy coding. When we have a categorical variable with k levels (in this case, region with 3 levels), we need to create k-1 dummy variables to represent it in the regression model.\nThe reason for using k-1 dummy variables instead of k is to avoid the dummy variable trap, which is a scenario in which the independent variables are multicollinear. In other words, one variable can be predicted perfectly from the others.\nIn our case, I1, I2, and I3 represent the three regions. If we included all three in our model, we would have perfect multicollinearity because I3 can be perfectly predicted from I1 and I2 (if I1 = 0 and I2 = 0, then I3 has to be 1). This would make the model‚Äôs estimates unstable and uninterpretable.\nBy leaving out I3, we are implicitly choosing region 3 as the reference category. The coefficients for I1 and I2 then represent the difference in the outcome between regions 1 and 3, and regions 2 and 3, respectively.\nIf we want to make comparisons between regions 1 and 2, we can either change the reference category (by including I3 and leaving out I1 or I2 instead), or compute the difference between the I1 and I2 coefficients.\n\n\n\nReduced Model\nNext, we are talking about creating Reduced Models (RMs) from a Full Model (FM) with dummy variables representing regions and altitude variables interacted with these region dummies. The Full Model (FM) in this context is:\nFM: y123 = a1 I1 + a2 I2 + a3 I3 + b1 H1 + b2 H2 + b3 H3\nwhere:\n\ny123 represents rainfall\nI1, I2, I3 are dummy variables for regions 1, 2, and 3, respectively\nH1, H2, H3 are altitude variables interacted with the respective region dummies\n\nBased on this FM, we can derive 5 different Reduced Models (RMs):\n\nRM1: Common slope across region 1 and region 2: y123 = a12 I12 + b1H1 + b2H2 + a3 I3 + b3H3\nRM2: Common intercept and slope across all regions: y123 = a123 I123 + b123 H123\nRM3: Common intercept and slope across region 1 and region 3, different slope for region 2: y123 = a13 I13 + b13H13 + a2I2 + b2H2\nRM4: Common slope across region 1 and region 2, different slope for region 3: y123 = a12 I12 + b12 H12 + a3I3 + b3H3\nRM5: Common intercept across all regions, common slope across region 1 and region 2, different slope for region 3: y123 = a1 I1 + a2 I2 + a3 I3 + b12 H12 + b3 H3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCp Mallows\nNext, a summary table was created to provide a succinct overview of both the Full Model (FM) and each Reduced Model (RM). This table is essential as it encapsulates vital statistical information about each model. This summary table consists of five columns: P, S, œÉ, n, and C_P_Mallow, and six rows corresponding to FM‚Äô,RM1,RM2‚Äô, RM3, RM4, and RM5.\nThe P denotes the number of parameters used in each model, S indicates the standard deviation of the residuals, œÉ represents the standard deviation of residuals for the full model, n signifies the number of observations, and C_P_Mallow represents the value of Mallow‚Äôs C_P statistic.\nIn the process of determining the effectiveness of the Reduced Models in relation to the Full Model, the Mallow‚Äôs \\(C_P\\) statistic plays a crucial role. According to Mallows (1973), a Reduced Model can be considered comparable to the Full Model if the Mallow‚Äôs \\(C_P\\) value is less than or equal to the number of parameters (\\(C_{P,\\text{Mallow}} \\leq p\\)). This statistic is calculated using the formula:\n\\[C_{P,\\text{Mallow}} = p + \\frac{(S^2 - \\sigma^2)(n - p)}{\\sigma^2}\\]\nIn this context, \\(p\\) corresponds to the number of parameters used in the Reduced Model, \\(n\\) denotes the total data observations used in the model (in this case, \\(n=45\\)), \\(S^2\\) is the variance of the Reduced Model, and \\(\\sigma^2\\) is the variance of the Full Model. By making use of this computation, we were able to evaluate the efficiency of each Reduced Model in comparison to the Full Model, aiding in the effective and accurate analysis of our data set.\n\n\n\nPlot Cp Mallows\nBased on the provided results from the code, we can create a plot to visualize the CP Mallow statistic. The x-axis of the plot will represent the number of predictors (P), while the y-axis will represent the CP Mallow values.\nTo begin, we will draw a cross line starting from the point (1, 1) and extending to the point (n, n), where ‚Äòn‚Äô represents the total number of observations. This line will serve as a reference and help us identify the region of interest.\nNext, we will plot the CP values on the y-axis corresponding to the respective number of predictors (P) on the x-axis. Each point on the plot will represent a reduced model, with the CP value indicating its performance compared to the full model.\nTo highlight the specific point that satisfies the given criteria - the lowest number of predictors (P) and falls either above or below the cross line - we can customize the marker style or color for that point. This will make it visually distinct from the other points on the plot.\nBy examining the plot, we can easily identify the reduced model that strikes a balance between simplicity (fewer predictors) and predictive power (CP Mallow value). The highlighted point will represent the optimal reduced model that meets these criteria.\nThis plot provides a visual representation of the CP Mallow statistic, allowing us to compare the performance of different reduced models and select the most appropriate one based on the desired balance between complexity and prediction accuracy.\n\nThe execution of linear regression analysis across the three designated regions indicated a correlation between the augmentation of annual precipitation and escalating altitude. The reduced models (RM) that demonstrated a close correspondence to the full model (FM) were RM2 and RM4. However, the models that exhibited remarkable efficacy were RM2. The characteristics embodied by the first and second regions can be postulated to bear similarities, while they exhibit discernible divergence from the attributes of the third region\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20240125-maximizing-thinkpad-t14-gen-2-amd.html",
    "href": "blog/20240125-maximizing-thinkpad-t14-gen-2-amd.html",
    "title": "Maximizing Thinkpad T14 Gen 2 AMD",
    "section": "",
    "text": "I bought a Thinkpad T14 Gen 2 AMD (released on August 2022) end of December 2023, it‚Äôs second hand with mint condition and standard specification (AMD Ryzen‚Ñ¢ 7 PRO 5850U Processor with Radeon Graphics, 256GB nvme and 16GB RAM, FHD 14‚Äù).\nThis is my second-owned Thinkpad after the T480, and same as before I also did some upgrades on my T14. Here‚Äôs the list of the components:\n\nT14 Gen 2: 14‚Ä≥, 3840√ó2160, IPS, 500 nits, 100% Adobe RGB, Anti-glare from https://www.myfixguide.com/store/screen-for-thinkpad-t14/\n40pin UHD cable from https://www.myfixguide.com/store/lcd-cable-for-t14-gen2/\n4TB NVMe SSD from https://www.crucial.com/ssd/p3-plus/CT4000P3PSSD8\n32GB RAM DDR4 SODIMM 3200MHz from https://www.corsair.com/us/en/p/memory/cmsx32gx4m1a3200c22/corsair-high-performance-vengeance-memory-kit-cmsx32gx4m1a3200c22\nWWAN card and the antenna from https://thinkparts.com/products/-4970\n\nI did all the installation myself by following the awesome guideline and use Pro Tech Toolkit https://www.ifixit.com/Store/Tools/Pro-Tech-Toolkit/IF145-307 from IFIXIT.\nSee some of the picture below:\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3333‚Äù] Remove the old screen and install the WWAN antenna [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3333‚Äù] Install the new screen [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3333‚Äù] Put the memory in the free slot [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3333‚Äù] Replace the old SSD with new one, and put the WWAN card in place and antenna cable too [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú4032‚Äù] This is the machine with upgrades [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3024‚Äù] Look, the Cellular connection is available now [/caption] [caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú3024‚Äù] The new 4K screen is on and I just need to install the bezel üòé [/caption]\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20081130-paddys-growth-and-development-model-and-economic-value-of-farming-system.html",
    "href": "blog/20081130-paddys-growth-and-development-model-and-economic-value-of-farming-system.html",
    "title": "Paddy‚Äôs growth and development model, and economic value of farming system",
    "section": "",
    "text": "Agriculture is one of the important sectors in Indonesia so that the various efforts related to development in agriculture continues to be done, especially paddy. This study aims to create algorithms and design of programs for simulating the growth and development of paddy and presented spatially. The approach of this study were divided into three parts. First is to compile a database of weather and economic calculations are used as input application system, the second is to integrate numerical and spatial models. And the third is to develop Graphical User Interface (GUI). Numerical model developed is the growth, development and economic.\nGrowth model simulates the flow of biomass to leaves, stems, roots and seeds as well as the loss of respiration. Added total biomass is a function of efficiency on the use and amount of solar radiation intercepted.\nDevelopment model describes the development phase of each plant phenological events suspected based on the concept of heat units. Based on the concept of heat units, plant development phase will change from one phase to the next phase if the plant has reached a certain accumulation of heat units.\nEconomic analysis was done by seeing Benefit Cost Ratio (B/C), activities that benefit the farm system will have a B/C &gt; 1.0, and the Break Event Point (BEP). For the government and related agencies, the resulting output is expected to be useful for the determination of agricultural development policy. As for farmers, is expected to help the timing of planting in order to obtain optimum production and profit.\nAbout the model\nShierary-Rice\nShierary-Rice crop simulation model was first developed by Handoko (1998), the goal is to estimate the potential production of paddy in Indonesia. The development of crop simulation model with daily resolution, especially for paddy can not be separated from the state of the weather, because these elements provide a major influence on plant growth and development phase that requires the input of daily weather data including maximum and minimum temperature, humidity, solar radiation, wind speed (Handoko 1994).\nThe use of Shierary-Rice model usually intended more paddy which made the source of water from rain or irrigation. This model consists of four main sub-sub-model development models, sub models of growth, water balance sub-model and sub models of nitrogen availability. In this study, Shierary-Rice simulation model was adopted and developed from a single point into the region and the results are presented spatially.\nGrowth model\nGrowth model simulates the flow of biomass to leaves, stems, roots and seeds as well as the loss of respiration. Model calculates the increment of biomass growth based on solar radiation interception and crop water availability. Solar radiation interception (Qint) calculated from data of solar radiation (Qs) and leaf area index (LAI). The amount of biomass of photosynthesis is shared between vegetative and generative organs (seeds) based on the phase of plant development. Added total biomass is a function of the efficiency of the use of solar radiation and the amount of solar radiation is intercepted. Some of these biomass will be used in the process of respiration. Respiration rate is calculated based on the weight of each plant organ (roots, stems, leaves and seeds) and temperature are represented in the form of temperature quotient.\nDevelopment model\nModel describing the development phase of development of plants from each of the phenological events suspected based on the concept of heat units. Based on this concept, the rate of plant growth occurs when the average daily temperature exceeds the base temperature. Base temperature is the minimum temperature required by a plant to grow to accumulate heat units. Base temperature for each plant will be different, for paddy to be used in the model used base temperature of 15 ¬∞C. Based on the concept of heat units, plant development phase will change from one phase to the next phase if the plant has reached a certain accumulation of heat units. For example, for certain varieties of paddy will reach the phase of maximum shoots after accumulating heat units of 750 HU.\nIn general, the paddy phenological events began to be deployed to harvest given the scale of 0-1. The scale is divided into four events, namely seedling (s = 0.0), planting (0.25), maximum shoots (0.50), flowering (s = 0.75) and physiologically mature (s = 1.0).\nEconomic value model\nThis model was calculated on the basis of survey data needs and cost per hectare of paddy crop. Economic analysis of paddy farming is done by seeing Benefit Cost Ratio (B/C), activities that benefit the farm system will have a B/C &gt; 1.0, and the Break Event Point (BEP).\n\nB/C = (Planting area * Production * grain prices)/Cost\nBEP = Cost/(Planting area * Production)\n\nApplication\nThis sub-model presents an estimate of¬†potential production at various planting times. Estimation on potential production is based on weather data, nitrogen treatment and paddy varieties. Weather data and nitrogen treatment are related to the water requirements and nutrients of paddy in each phase of its development. Information on potential production estimation can be visaulised spatially and in one location (point). One cell on the map represents 100 hectares of paddy fields. In a one-point simulation, the output is an estimate of potential production at various planting date and the optimum planting time. The sub-model of economic value presents profit estimation based on cost component information and estimated potential production, both for one planting period and each phase of its development. Cost components can be modified according to conditions and prevailing prices.\nThe results of this simulation can be useful for the government in making policies and programs related to the planting schedule and is expected to be used by farmers to find out the optimum planting time and estimate the amount of benefits that might be obtained.\nUser Interface\nThe user interface was built as a software application package with the Visual Basic programming language and supported with a number of library files and ActiveX from other software. To display the results of the model, a map is built by adding MapObjects to the project component in Visual Basic. MapObject has several features such as zoom in, zoom out and display the coordinates of the map. The program offer two-methods to calculate the growth and development models, one point and regional.\nAt one point calculation options, there is a choice of simulation to determine the optimum planting time and simulation of growth, development, groundwater balance and paddy production and its economic value. The required weather data input is made in * .csv format and the simulation can be carried out by modifying the amount of nitrogen treatment, planting date, paddy varieties and types of irrigated or rainfed paddy fields.\n\n\n\n\n\n\n\n\n\n\n\nAfter the simulation model is run, information on the length of development for each phase is presented on a specific page. The periods of each phase vary widely and are determined based on rainfall conditions and other climate data. In addition to seeing the simulation results for each phase of development, it can also be seen directly the magnitude of the potential results in general. On the other page, optimum planting time simulation information can be obtained by estimating the production potential and economic value of the farm.\n\n\n\n\n\n\n\n\n\n\n\nThe magnitude of the component cost of farming has been made default when the simulation is run. If you want to make modifications to the cost components can be done on a separate page.\n \nIn addition to simulations for one point, simulation can also be done for the region. Area simulations are carried out only on locations with paddy land cover, so previously the map that would be used as well as administrative also land cover maps. As an example of regional simulation, for the January 1 planting scenario (representing rainy season conditions) with 250 kg nitrogen treatment and rainfed paddy conditions and Ciliwung paddy varieties, most of the Karawang region in the northern part is only likely to produce 1-2 tons / ha. While in the southern part of Karawang, the yield potential is higher at more than 6 tons/ha. Whereas for the July 1 planting scenario (representing dry season conditions) with the same treatment, most of the Karawang region only has the potential to produce 2-3 tons/ha, only a small portion in the southern region has the potential to produce more than 6 tons.\n \nThe agricultural sector has an important role in Indonesia. Therefore, businesses that support agricultural development are needed. The paddy growth and development simulation model presented spatially is very useful for the government in making program policies for certain areas, looking at the distribution of potential yields, and determining priority target areas. In addition, this model is expected to be useful for farmers to determine planting strategies such as the selection of varieties, determining the optimum planting time and capital preparation based on the estimated cost components and estimated profits obtained.\nReference\nBinh ND, Murty VVN, Hoan DX. 1994.¬†Evaluation of the possibility for¬†rainfed agriculture using a soil moisture simulation model.¬†Agric.Water.Manage, 26, 187-199.\nBrisson N, Bernard S, Patrick B. 1992.¬†Agrometeorological Soil Water Balance for Crop Simulation Models.¬†Agric.For.Meteorol, 59, 267-287.\nChang J. 1974.¬†Climate and Agriculture: an Ecological Survey.Third Edition. University of Hawaii. ALDINE Publishing Company. Chicago.\nDoraiswamy DC, Thomson DR. 1982.¬†A crop moisture stress index for large areas and its application in the prediction of spring wheat phenology.¬†Agric.Meteorol, 27, 1-15.\nHandoko, Risdiyanto, 1. dan Sugiarto, Y. 1998.¬†Model simulasi tanman padi Shierary-Rice untuk estimasi produksi potensial padi di Indonesia.¬†Makalah pada Lokakarya Sistem Pemantauan dan Prediksi Produksi Padi di Indonesia, BPPT. Jakarta, 22 Juli 1998.\nHandoko, I.¬†¬†1994.¬†¬†Dasar Penyusunan dan Aplikasi Model Simulasi Komputer untuk Pertanian. Jurusan GEOMET FMIPA IPB.¬†¬†Bogor.\nHillel, D. 1972.¬†The Field Water Balanced and Water Use Efficiency.¬†In: D. Hillel (ed.) Optimizing the Soil Physical Environment Toward Greater Crop Yields.¬†Academic Press. New York.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20180410-telegram-group-for-gis-user-in-indonesia.html",
    "href": "blog/20180410-telegram-group-for-gis-user-in-indonesia.html",
    "title": "Telegram group for GIS user in Indonesia",
    "section": "",
    "text": "A group of friends and me run a Telegram group discussion about GIS and Remote Sensing application, currently focusing on Indonesia context and has been participated by more than 900 GIS practitioners (as of 10 April 2018).\nAs of now, there has been 31 sessions conducted. Presentations were delivered voluntary by a number of participants, covering the topic of SDG application, spatial model, web-map, UAV, etc. Feel free to join at this link.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20200309-72-hours-assessment-workshop-in-vanuatu.html",
    "href": "blog/20200309-72-hours-assessment-workshop-in-vanuatu.html",
    "title": "72-hours assessment workshop in Vanuatu",
    "section": "",
    "text": "Trip to Port Vila - Vanuatu last week was my first trip to Pacific Islands. 10 hours flight in total from Jakarta with stop in Sydney, Australia.\nWith colleague from Fiji, I run a workshop and stakeholder meeting to identify key food security and demographic vulnerability indicators for the country, and conduct the 72-hour assessment tool workshop with the National Disaster Management Office (NDMO) and the Food Security and Livelihood Cluster (FSLC) members. Very happy to see all the participant from Government agencies, International NGO and others are excited to follow the workshop and learn on how QGIS could be use during emergency situation.\nPort Vila can be categorised as a safe city, low crime rate, you can staying up late at night near the beach or inside the bar, and¬†residents who are friendly to foreigners. Was a pleasant experience.\nIn the weekend, I went to Blue Lagoon, but unfortunately I can not visit the very famous Mount Yasur in Tanna Island,¬†is an easily accessible active volcano, and is a major Vanuatu tourist attraction. Maybe next time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20210119-setup-your-machine-to-be-gis-ready.html",
    "href": "blog/20210119-setup-your-machine-to-be-gis-ready.html",
    "title": "Setup your machine to be GIS ready!",
    "section": "",
    "text": "Since 2009, I started to use Macbook Pro with OS X Snow Leopard along with Windows XP installed via Parallels Desktop to support my work as Earth Observation and Climate Analyst. I tried a lot to install and configure various geo tools and their dependencies in my Mac.\nThere are many paths to installing and configuring software, not all of them complementary. The following guidelines reflect current technology stack (hardware and software). Tested using Macbook Pro 15-inch 2018, 2.9 GHz 6-Core Intel Core i9 and 32 GB 2400 MHz DDR4, running on macOS Big Sur 11.1 and Windows Server 2019 via Parallels Desktop 16.1.2."
  },
  {
    "objectID": "blog/20210119-setup-your-machine-to-be-gis-ready.html#macos-spatial-stack",
    "href": "blog/20210119-setup-your-machine-to-be-gis-ready.html#macos-spatial-stack",
    "title": "Setup your machine to be GIS ready!",
    "section": "macOS Spatial Stack",
    "text": "macOS Spatial Stack\n\nText Editor\nI prefer to use Sublime Text as my text editor, because it rich of features, light and powerful. You can download from the website: https://www.sublimetext.com/3\n\nInstall subl Terminal extension for Sublime Text 3, so you can call it via Terminal. Type below code in your Terminal\n\nsudo ln -s ‚Äú/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl‚Äù /usr/local/bin/subl\n\n\nBeside Sublime Text, there are many competitors which have similar feature, Give it try Visual Studio Code and Atom.\n\n\nCommand Line Tools for Xcode\nInstall Command Line Tools (CLT) for Xcode, paste below code in your terminal and follow the process.\n\nxcode-select ‚Äìinstall\n\nOr you can download it from Apple Developer website (you must have Apple Developer account): https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_for_Xcode_12.3/Command_Line_Tools_for_Xcode_12.3.dmg\n\n\nHomebrew\nNext, install Homebrew. As stated in their website, Homebrew installs the stuff you need that Apple (or your Linux system) didn‚Äôt. Paste below code in your terminal and follow the process.\n\n/bin/bash -c ‚Äú$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)‚Äù\n\n\nInstall Homebrew formula\nTo install a formula, we just need to type brew install formulaname in Terminal, then Homebrew will do the job. I will install various tools to support our works. For example a famous download manager called WGET, below is the example:\n\nbrew install wget\n\n\n\nInstall Homebrew geospatial and climate library\nTo work on geospatial and climate related activities, we need various library below.\nGeospatial: gdal, json-c, libgeotiff, libpng, libspatialite, libtiff, libxml2, openjpeg, proj, sqlite\nClimate: hdf4, hdf5, netcdf, cdo, nco, ncl\nI just type below code in Terminal, then everything will install automatically. But if you missed some formula, you can easily install it.\n\nbrew install libgeotiff\nbrew install netcdf\n\nand so on.\nGDAL/OGR, CDO, NCO and NCL also available as Homebrew formula, but I will install it via Anaconda.\n\n\n\nQGIS\nQGIS installer for macOS available from the official website: https://qgis.org/downloads/macos/qgis-macos-pr.dmg\n\n\nPostgreSQL\nI prefer to use Postgres.app because it‚Äôs the easiest way to get started with PostgreSQL + PostGIS on the Mac. You can download latest software via this link: https://postgresapp.com/downloads.html\nInstalling a graphic client like pgAdmin will make a perfect combination.\n\n\nR and RStudio\nI used R to do statistical computing, the installer available from this link: https://cloud.r-project.org/bin/macosx/R-4.0.3.pkg\nThen installing the graphic client RStudio also important if you are typical GUI person. RStudio can be download from this link: https://download1.rstudio.org/desktop/macos/RStudio-1.3.1093.dmg\n\n\nPanoply\nPanoply is my favorite apps for plots geo-referenced and other arrays from netCDF, HDF, GRIB, and other datasets. You can download the latest software from this link https://www.giss.nasa.gov/tools/panoply/download/PanoplyMacOS-4.12.2.dmg\n\n\nAnaconda\nI am aware macOS already shipped with their own python, now I would like to install python again using Anaconda. The reason? I agree with most of answer in Reddit forum:\n\nUser level install of the version of python you want\nAble to install/update packages completely independent of system libraries or admin privileges\nconda tool installs binary packages, rather than requiring compile resources like pip - again, handy if you have limited privileges for installing necessary libraries.\nMore or less eliminates the headaches of trying to figure out which version/release of package X is compatible with which version/release of package Y, both of which are required for the install of package Z\nComes either in full-meal-deal version, with numpy, scipy, PyQt, spyder IDE, etc. or in minimal / alacarte version (miniconda) where you can install what you want, when you need it\nNo risk of messing up required system libraries\n\nSo, I choose Anaconda to support my works on earth observation and scientific data computing using python. You can find the latest installer via this link https://repo.anaconda.com/archive/Anaconda3-2020.11-MacOSX-x86_64.pkg\nAfter you install Anaconda, I assume located in ~/opt/anaconda3 You need to configure the specific environment for GIS and climate works.\n\nWhen you open Terminal after install Anaconda, it will activating the base environment by default. To prevent that, type below command\n\nconda config ‚Äìset auto_activate_base false\n\nCreate conda environment called gis, when conda asks you to proceed, type y.\n\nconda create ‚Äìname gis\n\nActivate the environment\n\nconda activate gis to deactivate environment, type conda deactivate\n\nInstall python, numpy, scipy, matplotlib and jupyter via Conda-Forge channel\n\nconda install -c conda-forge python numpy scipy matplotlib jupyter\n\nInstall GIS (gdal, pandas, geopandas, rasterio, xarray, rioxarray, cartopy) and Climate (cdo, nco, ncl, metpy) package.\n\nconda install -c conda-forge gdal pandas geopandas rasterio xarray rioxarray cartopy\nconda install -c conda-forge cdo nco ncl metpy\n\nPlease continue if you need more package.\n\n\nPython IDE\nI used PyCharm from Jetbrain as my IDE. You can download the latest installer for community edition via this link https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=mac&code=PCC\nConfiguring a conda environment in PyCharm\n\nIn the Settings/Preferences dialog (‚åò,), select Project:  | Project Interpreter.\nAlternatively, from the Welcome screen, select Configure.\nSelect Preferences.\nSelect Project Interpreter.\nIn the Project Interpreter page, click the gear icon and select Add.\nIn the left-hand pane of the Add Python Interpreter dialog box, select Conda Environment. The following actions depend on whether the conda environment existed before.\n\n\n\n\nConfigure your .bash_profile or .zshrc\nApart from having a home directory to create and store files, users need an environment that gives them access to the tools and resources. When a user logs in to a system, the user‚Äôs work environment is determined by the initialization files. These initialization files are defined by the user‚Äôs startup shell, which can vary depending on the release. The default initialization files in your home directory enable you to customize your working environment.\nOpen .bash_profile or .zshrc using Sublime Text. Both files are located in your home folder ~/. Type in your Terminal\n\nsubl ~/.bash_profile\nsubl ~/.zshrc\n\nBoth file is a personal initialization file for configuring the user environment. The file is defined in your home directory and can be used for the following:\n\nModifying your working environment by setting custom environment variables and terminal settings\nInstructing the system to initiate applications\n\nBelow is example of my .zshrc file"
  },
  {
    "objectID": "blog/20210119-setup-your-machine-to-be-gis-ready.html#windows-spatial-stack",
    "href": "blog/20210119-setup-your-machine-to-be-gis-ready.html#windows-spatial-stack",
    "title": "Setup your machine to be GIS ready!",
    "section": "Windows Spatial Stack",
    "text": "Windows Spatial Stack\nCurrently I used Windows Server 2019 running on Parallels Desktop 16. If you use Windows 10 Pro, I suggest you install Windows Subsystem for Linux https://docs.microsoft.com/en-us/windows/wsl/install-win10\nIt will make your life easier when you try to run unix-based code/apps.\n\nText Editor\nAs follow in macOS Spatial Stack, I prefer to use Sublime Text as my text editor, because it rich of features, light and powerful. You can download from the website: https://www.sublimetext.com/3\n\n\nArcGIS Desktop and ArcGIS Pro\nI assume you have installed ArcGIS Desktop and ArcGIS Pro in your Windows. If you don‚Äôt have it, you can request a free 21-day trial of ESRI product via this link https://www.esri.com/en-us/arcgis/products/arcgis-online/trial or buy Personal Use version via ESRI store https://www.esri.com/en-id/store\nAfter you got the license via email, save it to your local drive. The license will be like 371xxxxx_v9.elf9 for hardware key file or\n000F206F635Av9.txt for license key file or EVA123456789 for Authorization number or ArcGISProAdvance_SingleUse_123456.prvc for provisiong text file.\nThe license will allow you to pre-populate authorization numbers in the Software Authorization Wizard instead of manually entering the information;\n\nSave the license file to your machine and don‚Äôt share it with anyone. In case it is used for software authorization on another machine, any previous authorization will be invalidated;\nLaunch the Software Authorization Wizard by clicking ‚ÄúAuthorize Now‚Äù in the ArcGIS Administrator for ArcGIS Desktop and ArcGIS Engine, or the License Server Administrator for License Manager installations. For ArcGIS Server products, select ‚ÄúAuthorize‚Äù in the respective post-installer;\nSelect the option ‚ÄúI have received an authorization file from Esri and am now ready to finish the authorization process;\nBrowse to the saved provisioning file; or enter the Authorization number.\nSelect ‚ÄùAuthorize with Esri now over the Internet.‚Äù Select the website or email option if your machine does not have an Internet connection;\nReview the pre-populated authorization information and enter any additional information required;\nOn the subsequent panels, you will see the authorization numbers entered for the core product as well as any extensions if applicable;\nPress ‚ÄòNext‚Äô to complete the authorization process.\n\nNow, lets tune to make ArcGIS Desktop run faster, follow this step: https://github.com/ncss-tech/geo-pit/wiki/How-to-Make-ESRI-Desktop-ArcMap-Faster\n\n\nAnaconda\nYou can install the latest Anaconda for Windows via this link https://repo.anaconda.com/archive/Anaconda3-2020.11-Windows-x86_64.exe\nTo avoid breaking ArcGIS (or other software), uncheck the checkboxes (a) make Anaconda the default Python and (b) add Anaconda‚Äôs Python to the PATH. I will install Anaconda3 at C:/Anaconda3\nThen you can follow step-by-step above in macOS Spatial Stack section on creating environment, activate and deactivate, install package via channel.\n\nWorkflow to set up Anaconda with ArcGIS Pro 2.7\n\nCreate an Anaconda environment that is compatible with ArcGIS Pro\n\nCopy the folder arcgispro-py3 from C:Filesand paste to C:\nRename the copied folder arcgispro-py3 in C:to arcgispro\n\nTest the virtual environment\n\nAt the Anaconda Command Prompt, type: conda activate arcgispro\nType: conda list. You can see the list of packages installed\n\nAdd more packages\n\nLet‚Äôs add the Python Spatial Analysis Library (pysal) module.\nType the following command at the Anaconda Prompt: conda install pysal\n\nConfigure ArcGIS to see Anaconda and vice versa\n\nAnaconda Python to ArcPy\n\nEdit the ArcGISPro.pth (path) file within ‚ÄúC:-packages‚Äù.\nChange the relative ArcPy path to C:Files\nChange the relative ArcToolBox path to C:Files\n\nArcpy to Anaconda Python\n\nCreate a zconda.pth (path) file with the content ‚ÄúC:-packages‚Äù in it.\nThen Copy zconda.pth to C:Files-py3-packages\n\nTesting in ArcGIS Pro\n\nStart ArcGIS Pro, open the Python window\ntype ‚Äúimport pysal‚Äù\ntype ‚Äúpysal.‚Äù A popup menu with a list of pysal-provided functions is a pretty good sign the install succeeded.\n\nTesting in PyCharm\n\nStart PyCharm, in File‚Ä¶, choose Project then Project Interpreter\nIgnore the drop down list for Project Interpreter, and click the cog button to Add Local, and in the file browser pick C:.exe\nTo run your script, right click it in the Project window, and choose either Run or Debug\nRestart PyCharm for the Python Console to use the arcpro environment.\n\n\n\nSource: https://gis.stackexchange.com/a/202704\n\n\n\nR\nYou can download R for windows via this link https://cloud.r-project.org/bin/windows/base/R-4.0.3-win.exe\nThen follow with install R bridge for ArcGIS https://r.esri.com to combine the power of ArcGIS and R to solve your spatial problems. The installation procedure, check the Github pages https://github.com/R-ArcGIS/r-bridge-install"
  },
  {
    "objectID": "blog/20230128-history-of-climate-modeling.html",
    "href": "blog/20230128-history-of-climate-modeling.html",
    "title": "History of climate modeling",
    "section": "",
    "text": "Image source: https://nps.edu/-/nps-researchers-partner-on-next-generation-climate-model\nThe history of climate modeling dates back to the late 19th and early 20th centuries (Edwards, P.N. 2011), with some of the earliest models being developed in the 1870s (Uppenbrink, J. 1996). The main focus of these models was the study of atmospheric processes and the calculation of future climate changes (Arrhenius, S. 1896). With the advent of advanced computing technology (Ruttimann, J. 2006) in the mid-20th century, climate models improved significantly and could be used to predict the effects of various changes in the Earth‚Äôs climate. Today, climate models are used to simulate both past (Otto-Bliesner, et.al. 2006) and future (Fick, S.E. and R.J. Hijmans, 2017) climate scenarios, as well as to understand the effects of global warming. The role of climate models in understanding climate change is becoming increasingly important.\nClimate models are mathematical representations of Earth‚Äôs climate system, used to simulate and predict the evolution of climate over time. They are used to study the behavior of the climate system and its components, as well as to improve predictions of future climate states. One of the most common types of climate model is the General Circulation Model (GCM), originally created by scientists (Manabe, S. and Bryan, K. 1969) at the Geophysical Fluid Dynamics Laboratory (GFDL), which simulates the global circulation of the atmosphere, ocean, land surface, and sea ice. GCMs are typically used to simulate medium- to long-term changes in climate due to natural or human-caused forcing (McGuffie, K. and Henderson-Sellers, A. 2005). An extension of GCMs is Earth System Models (ESMs) which typically include additional components representing the interaction of the atmosphere, ocean, land surface, and sea-ice with the biosphere and cryosphere (Scholze, M., et.al. 2012). These models allow for a more complete representation of the Earth system, including feedback between components (Sokolov, A. et.al. 2018). ESMs are used for climate projections, including the assessment of future climate change and its impacts on various sectors of society (Heavens, N.G. et.al.2013). Another subset of GCMs and ESMs is Regional Climate Models (RCMs) which are used to simulate the climate of a smaller region. They can simulate the effects of small-scale features, such as mountains and coastlines, more realistically than global models (Wang, Y. et.al. 2004). RCMs are used to simulate the effects of climate change on a local level, such as changes to precipitation and temperature, as well as on regional climate extremes (Tapiador, F.J. et.al. 2020).\nTo understand the complexity of the climate system, all components of the climate system including atmosphere, oceans, land surface, and ice are modeled (Gettelman, A. and Rood, R.B., 2016, pp 13-22). The atmosphere is a complex system of gasses, radiation, and air particles, driven by energy from the sun. Atmospheric processes include convection, advection, radiation, and condensation, all of which are important in determining weather patterns (Gettelman, A. and Rood, R.B., 2016, pp 71-76). The oceans are a key component of the global climate system, covering more than 70% of the Earth‚Äôs surface. Oceanic processes such as upwelling and downwelling, evaporation, and convection all play an important role in regulating climate. The oceans store and transport heat energy, modulating global temperatures and driving weather patterns (Gettelman, A. and Rood, R.B., 2016, pp 87-88). The land surface includes the terrestrial biosphere and the topography of the land. Terrestrial processes such as photosynthesis, respiration, and evaporation are important in regulating climate variability¬† (Gettelman, A. and Rood, R.B., 2016, pp 109-111). Land cover changes, such as urbanization and deforestation, can impact weather patterns and climate. Ice within the climate system acts as an important regulator, acting to reflect incoming solar radiation back into space. Ice sheets, glaciers, and sea ice are responsible for the formation of the Earth‚Äôs albedo, which is an important factor in determining the global energy balance (Gettelman, A. and Rood, R.B., 2016, pp 101). Changes in ice cover can have profound effects on the climate system.\nClimate models are a vital tool for understanding and predicting the Earth‚Äôs climate. They are computer simulations that simulate the dynamics of the atmosphere and its interactions with the ocean, land surface, and other components of the climate system (Tehrani, M.J., et.al. 2022). The processes that are simulated in a climate model include radiation, precipitation, and circulation.\nRadiation, the transfer of energy from the Sun to the Earth, is the primary source of energy for the Earth‚Äôs climate and is responsible for determining the temperature of the atmosphere. The radiation that is transferred to the Earth‚Äôs surface and atmosphere is called shortwave radiation (Yang, Q. et.al 2020), and it is mainly composed of visible light and infrared radiation. Precipitation, the process by which water is transferred from the atmosphere to the land and ocean surface, is essential to many ecosystems and processes such as erosion, sedimentation, and the hydrological cycle (Tapiador, F.J. et.al. 2017). Circulation, the transfer of heat and moisture from the ocean and atmosphere to higher latitudes, is responsible for the formation of the Earth‚Äôs major climate zones and the transport of heat and moisture around the planet. It is also responsible for the development of storm systems and the formation of large-scale climate features such as El Ni√±o and La Ni√±a (Behera, S.K. et.al. 2021).\nOne of the main uses of climate models is to project future climate change and the increase in rainfall erosivity wil drive high erosion rates (Panagos, P., et.al. 2022). By assessing current and past conditions, such as greenhouse gas concentrations, temperatures, and ocean currents, climate models predict how the climate may change over time. This can help inform policy decisions regarding climate change mitigation and adaptation (Marzi, S. et.al. 2021; Lindbergh, S. et.al. 2022; Xing, Q. et.al. 2022). Another application of climate models is to understand past climate change. By running simulations using past data, such as atmospheric concentrations and temperatures, scientists can better understand how climate has changed over time (Li, Y. et.al. 2018; Razjigaeva, N.G. et.al. 2020). This information can then be used to better understand the current climate, as well as inform climate change mitigation strategies.\nAnother example is the study of past climate events such as droughts (Gupta, A. S. et.al. 2011), heatwaves (Trancoso, R. et.al. 2020), and floods (Degeai, J.P. et.al. 2022). By analyzing past climate data, scientists can understand the causes of these events and how they are related to changes in atmospheric and oceanic circulation patterns. This information can be used to develop early warning systems for future climate events, as well as to inform policy decisions related to disaster risk management (Coughlan de Perez, E. et.al. 2022; Li, D. et.al. 2021). Climate models can also be used to reconstruct past climate conditions by using various data sources such as ice cores, tree rings, and sediment cores. This allows scientists to understand the past climate conditions, as well as to study the impact of past climate change on the environment, such as on vegetation (Li, P. et.al. 2021), animals (Gulland, F.M. et.al. 2022), and human societies (Rivera-Collazo, 2022).\nIn summary, climate models provide a powerful tool for understanding past and future climate changes. Their ability to simulate the Earth‚Äôs climate system and its various components allows scientists to assess the impact of human activities and natural phenomena on the climate. The main uses of climate models include projecting future climate change and understanding past climate change. However, there are several major challenges and limitations associated with climate modeling (CCSP, 2008; Oluwagbemi, O.O. et.al. 2022), including the complexity of the Earth‚Äôs climate system, the need for high-performance computing, the dependence on accurate data, and the difficulty of interpreting results. Despite these challenges, climate models are essential in informing policy decisions regarding climate change mitigation and adaptation (IPCC, 2007). The Intergovernmental Panel on Climate Change (IPCC) report, which utilizes numerous climate simulations (Flato, G.J. et.al. 2013) is one of the most influential and widely recognized climate models. It serves as a useful tool for assessing the effectiveness of policies and strategies to reduce the impacts of climate change, and for making informed decisions about proposed approaches.\nReferences\n\nEdwards, P.N. (2011). History of climate modeling. WIREs Climate Change 2, 128‚Äì139. John Wiley & Sons, Ltd.¬†https://doi.org/10.1002/wcc.95\nUppenbrink, J. (1996). Arrhenius and global warming. Science, 272:1122. https://doi.org/10.1126/science.272.5265.1122\nArrhenius, S. (1896). On the Influence of Carbonic Acid in the Air upon the Temperature of the Ground. Philos Mag J Sci. 41:237-276. https://doi.org/10.1080/14786449608620846\nRuttimann, J. (2006). Milestones in scientific computing. Nature 440, 399‚Äì402. https://doi.org/10.1038/440399a\nOtto-Bliesner, B.L., Marshall, S,J., Overpeck, J.T., Miller, G.H., Hu, A. (2006). Simulating Arctic Climate Warmth and Icefield Retreat in the Last Interglaciation. Science, 311:5768. https://doi.org/10.1126/science.1120808\nFick, S.E. and R.J. Hijmans, (2017). WorldClim 2: new 1km spatial resolution climate surfaces for global land areas. Int J Climatol 37 (12): 4302-4315. https://doi.org/10.1002/joc.5086\nManabe, S., Bryan, K. (1969). Climate Calculation with a combined ocean-atmosphere model. J. Atmos. Sci., 26(4), 786‚Äì789, https://doi.org/10.1175/1520-0469(1969)026%3C0786:CCWACO%3E2.0.CO;2\nMcGuffie, K., Henderson-Sellers, A. (2005). A Climate Modelling Primer: A History of and Introduction to Climate Models. John Wiley & Sons. 253pp. https://doi.org/10.1002/0470857617.ch2\nScholze, M., Allen, J., Collins, W., Cornell, S., Huntingford, C., Joshi, M.M, Lowe, J.A, Smith, R.S., Wild, O. (2012). Earth system models: A tool to understand changes in the Earth system. In S. Cornell, I. Prentice, J. House, & C. Downy (Eds.), Understanding the Earth System: Global Change Science for Application (pp.¬†129-159). Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511921155.008\nSokolov, A., Kicklighter, D., Schlosser, C. A., Wang, C., Monier, E., Brown-Steiner, B., et al.¬†(2018). Description and evaluation of the MIT Earth system model (MESM). AGU Journal of Advances in Modeling Earth Systems, 10(8), 1759‚Äì1789. https://doi.org/10.1029/2018MS001277\nHeavens, N. G., Ward, D. S. & Natalie, M. M. (2013). Studying and Projecting Climate Change with Earth System Models. Nature Education Knowledge 4(5):4. https://www.nature.com/scitable/knowledge/library/studying-and-projecting-climate-change-with-earth-103087065/\nWang, Y., Leun, L. R., McGregor, J. L., Lee, D.-K., Wang, W.-C., Ding, Y., Kimura, F. (2004). Regional Climate Modeling: Progress, Challenges, and Prospects. Journal of the Meteorological Society of Japan. Ser. II, 82(6), 1599‚Äì1628. https://doi.org/10.2151/jmsj.82.1599\nTapiador, F.J., Navarro, A., Moreno, R., S√°nchez, J.L.,¬† Garcia-Ortega, E. (2020). Regional climate models: 30 years of dynamical downscaling. Atmos. Res., 235, Article 104785. https://doi.org/10.1016/j.atmosres.2019.104785\nGettelman, A., Rood, R.B. (2016). Components of the Climate System. In: Demystifying Climate Models. Earth Systems Data and Models, vol 2. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-662-48959-8_2\nTehrani, M.J., Bozorg-Haddad, O., Pingale, S.M., Achite, M., Singh, V.P. (2022). Introduction to Key Features of Climate Models. In: Bozorg-Haddad, O. (eds) Climate Change in Sustainable Water Resources Management. Springer Water. Springer, Singapore. https://doi.org/10.1007/978-981-19-1898-8_6\nYang, Q., Zhang, F., Zhang, H., Wang, Z., Iwabuchi, H., Li, J. (2020). Impact of Œ¥-Four-Stream Radiative Transfer Scheme on global climate model simulation. Journal of Quantitative Spectroscopy and Radiative Transfer. 243:106800. https://doi.org/10.1016/j.jqsrt.2019.106800\nTapiador, F.J., Navarro, A., Levizzani, V.,Garc√≠a-Ortega, E., Huffman, G.J., Kidd, C., Kucera, P.A., Kummerow, C.D., Masunaga, H., Petersen, W.A., Roca, R., Sanchez, J.-L., Tao, W.-K., Turk, F.J. (2017). Global precipitation measurements for validating climate models. Atmos. Res. 197, pp.¬†1-20, https://doi.org/10.1016/j.atmosres.2017.06.021\nBehera, S. K., Doi, T., and Luo, J.-J. (2021). Air‚Äìsea interaction in tropical Pacific: the dynamics of El Ni√±o/Southern Oscillation. In: Tropical and Extratropical Air-Sea Interactions (Elsevier). p.¬†61‚Äì92. https://doi.org/10.1016/B978-0-12-818156-0.00005-8\nPanagos, P., Borrelli, P., Matthews, F., Liakos, L., Bezak, N., Diodato, N., Ballabio, C. (2022). Global rainfall erosivity projections for 2050 and 2070. J. Hydrol. 610, 127865. https://doi.org/10.1016/j.jhydrol.2022.127865\nMarzi, S., Mysiak, J., Essenfelder, A.H., Pal, J.S., Vernaccini, L., Mistry, M.N., Alfieri, L., Poljansek, K., Marin-Ferrer, M., Vousdoukas, M. (2021). Assessing future vulnerability and risk of humanitarian crises using climate change and population projections within the INFORM framework. Global Environmental Change. 71:102393. https://doi.org/10.1016/j.gloenvcha.2021.102393\nLindbergh, S., Ju, Y., He, Y., Radke, J., Rakas, J. (2022). Cross-sectoral and multiscalar exposure assessment to advance climate adaptation policy: The case of future coastal flooding of California‚Äôs airports. Climate Risk Management. 38:100462. https://doi.org/10.1016/j.crm.2022.100462\nXing, Q.; Sun, Z.; Tao, Y.; Shang, J.; Miao, S.; Xiao, C.; Zheng, C. (2022). Projections of future temperature-related cardiovascular mortality under climate change, urbanization and population aging in Beijing, China. Environ. Int. 163, 107231. https://doi.org/10.1016/j.envint.2022.107231\nLi, Y., Liu, Y., Ye, W., Xu, L., Zhu, G., et al.¬†(2018). A new assessment of modern climate change, china‚Äîan approach based on paleo-climate. Earth-Science Rev.¬†177, 458‚Äì477. https://doi.org/10.1016/j.earscirev.2017.12.017\nRazjigaeva, N.G., Grebennikova, T., Ganzey, L., Ponomarev, V., Gorbunov, A., Klimin, M., Arslanov, K., Maksimov, F., Petrov, A. (2020). Recurrence of extreme floods in south Sakhalin Island as evidence of paleo-typhoon variability in North-Western Pacific since 6.6 ka. Palaeogeogr. Palaeoclimatol. Palaeoecol. 556:109901. https://doi.org/10.1016/j.palaeo.2020.109901\nGupta, A. S., Jain, S., Kim, J.S. (2011). Past climate, future perspective: an exploratory analysis using climate proxies and drought risk assessment to inform water resources management and policy in Maine, USA. J. Environ. Manage. 92:3, pp.¬†941-947. https://doi.org/10.1016/j.jenvman.2010.10.054\nTrancoso, R., Syktus, J., Toombs, N., Ahrens, D., Wong, K.K.-H., Pozza, R.D. (2020). Heatwaves intensification in Australia: a consistent trajectory across past, present and future. Sci. Total Environ. 742:140521. https://doi.org/10.1016/j.scitotenv.2020.140521\nDegeai, J.P., Blanchemanche, P., Tavenne, L., Tillier, M., Bohbot, H., Devillers, B., Dezileau, L. (2022). River flooding on the French Mediterranean coast and its relation to climate and land use change over the past two millennia. CATENA. 219:106623. https://doi.org/10.1016/j.catena.2022.106623\nCoughlan de Perez, E., Harrison, L., Berse, K., Easton-Calabria, E., Marunye, J., Marake,¬† M., Murshed, S.B., Shampa, Zauisomue, E.H. (2022). Adapting to climate change through anticipatory action: The potential use of weather-based early warnings. Weather and Climate Extremes. 38:100508. https://doi.org/10.1016/j.wace.2022.100508\nLi, D., Fang, Z. N., & Bedient, P. B. (2021). Flood early warning systems under changing climate and extreme events. In Climate change and extreme events (pp.¬†83‚Äì 103). Elsevier. https://doi.org/10.1016/B978-0-12-822700-8.00002-0\nLi, P., Liu, Z., Zhou, X., Xie, B., Li, Z., Luo, Y., Zhu, Q., Peng, C. (2021). Combined control of multiple extreme climate stressors on autumn vegetation phenology on the Tibetan Plateau under past and future climate change. Agr. Forest Meteorol. 308‚Äì309:108571. https://doi.org/10.1016/j.agrformet.2021.108571\nGulland, F.M.; Baker, J.D.; Howe, M.; LaBrecque, E.; Leach, L.; Moore, S.E.; Reeves, R.R.; Thomas, P.O. (2022). A Review of Climate Change Effects on Marine Mammals in United States Waters: Past Predictions, Observed Impacts, Current Research and Conservation Imperatives. Clim. Change Ecol. 3:100054. https://doi.org/10.1016/j.ecochg.2022.100054\nRivera-Collazo, I. (2022). Environment, climate and people: Exploring human responses to climate change. J. Anthropol. Archaeol. 68:101460. https://doi.org/10.1016/j.jaa.2022.101460\n[CCSP] Climate Change Science Program. (2008): Climate Models: An Assessment of Strengths and Limitations. A Report by the U.S. Climate Change Science Program and the Subcommittee on Global Change Research [Bader D.C., C. Covey, W.J. Gutowski Jr., I.M. Held, K.E. Kunkel, R.L. Miller, R.T. Tokmakian and M.H. Zhang (Authors)]. Department of Energy, Office of Biological and Environmental Research, Washington, D.C., USA, 124 pp.¬†https://toolkit.climate.gov/reports/climate-models-assessment-strengths-and-limitations\nOluwagbemi, O.O.; Hamutoko, J.T.; Fotso-Nguemo, T.C.; Lokonon, B.O.K.; Emebo, O.; Kirsten, K.L. (2022). Towards Resolving Challenges Associated with Climate Change Modelling in Africa. Appl. Sci. 12, 7107. https://doi.org/10.3390/app12147107\n[IPCC] Intergovernmental Panel on Climate Change (2007): Summary for Policymakers. In: Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change [Solomon, S., D. Qin, M. Manning, Z. Chen, M. Marquis, K.B. Averyt, M.Tignor and H.L. Miller (eds.)]. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA. https://www.ipcc.ch/si../assets/image-blog/uploads/2018/05/ar4_wg1_full_report-1.pdf\nFlato, G., Marotzke, J., Abiodun, B., Braconnot, P., Chou, S.C., Collins, W., Cox, P., Driouech, F., Emori, S., Eyring, V., Forest, C., Gleckler, P., Guilyardi, E., Jakob, C., Kattsov, V., Reason, C., Rummukainen, M. (2013). Evaluation of Climate Models. In: Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Stocker, T.F., Qin, D., Plattner, G.-K., Tignor, M., Allen, S.K., Boschung, J., Nauels, A., Xia, Y., Bex, V., Midgley, P.M. (eds.)]. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA. https://www.ipcc.ch/si../assets/image-blog/uploads/2018/02/WG1AR5_Chapter09_FINAL.pdf\n\nWeb Resources\n\nhttps://glossary.ametsoc.org/wiki/Welcome\nhttps://www.carbonbrief.org/timeline-history-climate-modelling/\nhttps://climate.mit.edu/explainers/climate-models\nhttps://en.wikipedia.org/wiki/Climate_model\nhttps://earthobservatory.nasa.gov/blogs/earthmatters/2017/04/05/a-climate-model-for-the-history-books/\nhttps://net-zero.blog/book-blog/a-short-history-of-climate-models\nhttps://www.sciencedirect.com/topics/earth-and-planetary-sciences/general-circulation-model\nhttps://www.energy.gov/science/doe-explainsearth-system-and-climate-models\nhttps://e3sm.org/\nhttps://climate.nasa.gov/news/2943/study-confirms-climate-models-are-getting-future-warming-projections-right/\nhttps://news.climate.columbia.edu/2018/05/18/climate-models-accuracy/\nhttps://www.gfdl.noaa.gov/climate-modeling/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20230702-fourier-regression-model-to-generate-monthly-to-daily-temperature-data.html",
    "href": "blog/20230702-fourier-regression-model-to-generate-monthly-to-daily-temperature-data.html",
    "title": "Fourier regression model to generate monthly to daily temperature data",
    "section": "",
    "text": "1 Introduction\nIn the sphere of meteorology, the significance of statistical models in comprehending and forecasting diverse weather patterns is incontestable. Within this context, the Fourier regression model has emerged as a formidable asset, specifically in generating daily time series from monthly temperature data (Wilks, 1998). The model lays a robust foundation for simulating temperature patterns, yielding crucial insights that are indispensable for weather prediction, climate change studies, and managing water resources.\nThe Fourier regression model has been proven to be a highly effective tool for generating daily time series from monthly temperature data, enhancing our understanding and prediction capabilities in weather forecasting, climate change studies, and water resource management. This model‚Äôs unique ability to incorporate historical context allows it to capture intricate dependencies and transitions in temperature data, which are crucial in understanding temperature patterns.\nBy applying Fourier series, it is possible to reduce the number of parameters involved in the process, thereby simplifying complex calculations and making the model more efficient. Moreover, the Fourier regression model can seamlessly replace missing values and handle anomalies, which are often challenges in data analysis. This enables more accurate simulations and predictions, making it a vital tool in fields such as agriculture and urban planning.\nThe Fourier regression model‚Äôs success in generating daily time series from monthly temperature data not only contributes to our understanding of weather patterns but also provides practical solutions for real-world challenges, making it a powerful instrument in various domains.\n\n\n2 Data\nOver the past three decades, Bogor‚Äôs climate has remained relatively consistent. The city experiences an average annual temperature of around 26 ¬∞Celsius. The temperature varies little throughout the year, with the warmest month averaging around 27 ¬∞Celsius and the coolest month averaging around 25 ¬∞Celsius.\nDaily temperature data of Bogor Climatological Station from 1984-2021 were used in this analysis, downloaded from BMKG Data Online in *.xlsx format. The file then manipulated by remove the logo and unnecessary text, aggregated into monthly, leaving only two columns, namely date in column A and temperature in column B for the header with the format extending downwards, and save as *.csv format.\nThe final input file is accessible via this link: https://drive.google.com/file/d/1vKT5ekDnqahkG6um5wIm-ZfhExqZTAm8/view?usp=sharing\n\n\n3 Methods\nThis exercise focuses on the Fourier regression model as a tool for generating daily temperature data from monthly time series (Boer, 1999).\nFourier series can also be employed to generate other climate data. McCaskill (1990a) utilized Fourier series regression, incorporating rainfall events to generate pan evaporation data, maximum and minimum air temperature, and daily radiation intensity (P(i)).\n\nwhere \\(f\\) represents a rain function, \\(R(i+j)\\) is a rain event on day \\((i+j)\\), and \\(c_j\\), \\(l\\), and \\(n\\) are determined through regression analysis. In the context of Australia, the incorporation of rain events in the Fourier series function did not exert a significant impact, although it substantially reduced the error level of the estimated value (McCaskill, 1990a).\nIn the above equation 1, the effect of rainfall events is assumed to be additive. However, for certain regions, this rainfall event impact could be multiplicative.\nIn many cases, climate data is generally presented as monthly data, making analysis requiring daily data difficult to execute. Fourier series regression can also be used to generate daily climate data from average monthly climate data (Epstein, 1991). The equation is written as follows:\n\nwhere \\(t' = \\frac{2\\pi t}{12}\\), and \\(t\\) is the month. This equation assumes an equal number of days in each month, which is not the case in reality. Therefore, to adjust it, the value of \\(t\\) in the above equation is changed as the \\(m\\)-th day for the \\(T\\)-th month so that the value \\(t = (T-0.5)+\\frac{(m-0.5)}{D}\\), where \\(D\\) is the number of days in month \\(T\\). The use of equation 2 to create fitting lines for daily data is highly effective. The fitting lines composed from daily data and those derived from monthly data almost overlap.\nFor simulation purposes, an error component, \\(e(i)\\), which has a normal distribution with a mean of 0 and a variance of \\(s^2\\) is typically included. Thus, the data series generated by each simulation will differ but still reflect the seasonal diversity of data. Errors in climate data simulation models often autocorrelate. Therefore, the error component can be modeled using a k-th order autocorrelation function (Wannacott and Wannacott, 1987), which is:\n\nwhere \\(r\\) is the correlation value and \\(w(i)\\) is the random error (white noise). The simplest autocorrelation error function linearly connects the error on day \\(i\\) with the error on day \\(i-1\\) plus the random error on day \\(i\\) (first-order autocorrelation function), namely:\n\nTherefore, if the value of \\(r\\) is positive, the error on day \\(i\\) tends to increase if the error on the previous day was high, and vice versa. Practically, the value of \\(r\\) is always less than one, but its magnitude is unknown.\n\n\n4 Implementation\nIn the implementation phase of this analysis, we utilized Python and the Pandas, Numpy and Matplotlib library to develop a Fourier regression model to generate daily time series from monthly temperature data.\n\n4.1 How-to?\nThe step-by-step guide for the model is readily accessible in Google Colab, an ideal platform for data analysis and machine learning. This comprehensive how-to guide explains the entire process, starting with reshaping the data to ensure compatibility with the model, and aggregate calculation from daily to monthly, assigning monthly data across the corresponding days of the month, Fourier Series Modeling and Coefficient Extraction, Temperature estimation using Fourier Coefficients, Autocorrelated Error Calculation and Final estimates adjusted temperature..\n4.1.1 Reshape the data\nThe first step in our analysis involves pre-processing and reshaping the data to fit the requirements of the subsequent statistical modeling. Our raw temperature data, originally in a CSV file, consists of daily temperature readings recorded over several years. In this data, dates are represented in a ‚ÄòYYYY-MM-DD‚Äô format. However, for our analysis, we require the ‚Äòday of the year‚Äô and the ‚Äòyear‚Äô as separate variables.\nWe start by loading the data into a Pandas DataFrame. Next, we convert the ‚Äòdate‚Äô column into a datetime format using the pd.to_datetime() function, which facilitates date-specific manipulations. This allows us to extract the ‚Äòday of the year‚Äô and the ‚Äòyear‚Äô information from each date and store these in new columns titled ‚Äòdayofyear‚Äô and ‚Äòyear‚Äô, respectively.\nSince we have multiple temperature readings per day, we average these readings for each day of the year across all years. We do this by grouping the data by ‚Äòdayofyear‚Äô and ‚Äòyear‚Äô, and then calculating the mean temperature for each group using the groupby() and mean() functions.\nHowever, this leaves us with a long format DataFrame, where each row represents a day of a particular year. For easier visualization and modeling, we convert this into a wide format DataFrame, where each column represents a year and each row represents a day of the year. This transformation is performed using the unstack() function.\nLastly, we reset the DataFrame index for neatness and compatibility with future operations. The resulting DataFrame is saved into a new CSV file. This reshaping of data forms the foundation for our Fourier regression model and helps ensure accuracy and efficiency in the subsequent analysis.\nAbove code will produce output previews like below.\n\nTable 1. Reshape data from long to wide format\n4.1.2 Daily to Monthly\nIn addition to the daily analysis, we decided to explore the temperature trends on a monthly basis. The process for reshaping the data for monthly temperature averages mirrors the daily approach.\nFirst, we load the raw temperature data from a CSV file into a Pandas DataFrame and convert the ‚Äòdate‚Äô column to a datetime format. With the datetime format, we‚Äôre able to extract the ‚Äòmonth‚Äô and ‚Äòyear‚Äô from each date, creating new columns for each.\nAs with the daily analysis, we handle multiple temperature readings per day by averaging these for each month of each year. We achieve this by grouping the data by ‚Äòmonth‚Äô and ‚Äòyear‚Äô, then calculating the mean temperature for each group.\nTo facilitate further analysis and visualization, we convert this long format DataFrame to a wide format DataFrame, with each column representing a year and each row representing a month. This is done using the unstack() function.\nAfter resetting the DataFrame index for better data structure, we save the resulting DataFrame into a new CSV file. This CSV file contains average monthly temperatures over the years and will be useful for understanding broader temperature trends and providing context to our Fourier regression model.\nAbove code will produce output previews like below.\n\nTable 2. Monthly average of temperature\n4.1.3 Assigning monthly data into across the corresponding days of the month\nIn order to prepare our dataset for Fourier regression modeling, we need to map the average monthly temperature values to their corresponding days of the year. This step is crucial as it enables the creation of a continuous time series from the previously calculated monthly averages.\nWe begin this process by defining the number of days in each month, differentiating between leap and non-leap years. Then, we create a new DataFrame, dayofyear_df, with a ‚Äòdayofyear‚Äô column that sequentially enumerates each day of the year from 1 to 366. A binary ‚Äòleap‚Äô column is also added to indicate if the day corresponds to a leap year.\nTo map the ‚Äòdayofyear‚Äô to the corresponding month, we create a ‚Äòmonth‚Äô column using np.repeat() to repeat the month index according to the number of days in each month. This column is then adjusted for non-leap years.\nThe average monthly temperatures, stored in monthly_avg_df, are merged with the dayofyear_df DataFrame, repeating each monthly average across the corresponding days of the month. As a result, we obtain a DataFrame with daily granularity, which contains the corresponding average monthly temperature for each day.\nWe then handle the 366th day of non-leap years, setting the temperature to NaN, as it doesn‚Äôt exist in those years.\nFinally, we remove the unnecessary ‚Äòmonth‚Äô and ‚Äòleap‚Äô columns, reset the index, and save this DataFrame into a new CSV file. This final, reshaped DataFrame serves as our input for the Fourier regression modeling, enabling us to predict temperatures at a daily level from average monthly temperatures.\nAbove code will produce output previews like below.\n\nTable 3. Assigning monthly data into daily\n4.1.4 Fourier series modeling and coefficient extraction\nThe next step in the analysis process involves fitting a Fourier series to our daily temperature data. The Fourier series is a mathematical tool used for analyzing periodic functions, making it suitable for modeling periodic patterns in weather data like temperature.\nTo begin, we first load the reshaped DataFrame containing the daily average temperatures. Next, we define a Fourier function, specifying the form it should take. The function is expressed in terms of trigonometric terms (cosine and sine functions) and includes coefficients that we aim to estimate (a0, a1, b1, a2, b2).\nTo perform this estimation, we iterate over each year in the DataFrame. For each year, we calculate new variables ‚ÄòT‚Äô, ‚Äòm‚Äô, ‚ÄòD‚Äô, and ‚Äòt‚Äô. These variables represent respectively the month, the day of the month, the number of days in the month, and a transformed time index (where each month is considered as a unit time interval). We exclude data points with NaN or infinite values.\nWe then utilize the curve_fit function from the scipy.optimize module to fit the Fourier function to the temperature data for each year. This function returns the optimal values for the coefficients a0, a1, b1, a2, and b2 that best fit the data.\nIn cases where there‚Äôs insufficient data to fit the Fourier series, we handle the errors and assign NaN values to the coefficients for that year.\nOnce we obtain the coefficients for each year, we save this data into a new CSV file. This file will then be used to generate our Fourier regression model and perform temperature estimation. The generated Fourier coefficients provide insights into the amplitude and phase of the cyclical patterns in the temperature data.\nAbove code will produce output previews like below.\n\nTable 4. Fourier coefficient\n4.1.5 Temperature estimation using Fourier coefficient\nHaving determined the coefficients of the Fourier series for each year, we can now use these coefficients to generate temperature estimates. This step entails constructing a time series model for the daily temperatures based on the Fourier series.\nWe start by loading the DataFrame that contains the Fourier coefficients for each year. These coefficients were calculated in the previous step and are used to define the form of the Fourier series for each year.\nOur next task is to create a new DataFrame, ‚Äòtemp_estimates‚Äô, to store our estimated temperatures. This DataFrame is initially populated with a ‚Äòdayofyear‚Äô column, containing each day of the year (from 1 to 367).\nWe then iterate over each year in our coefficients DataFrame. For each year, we create a separate DataFrame ‚Äòyear_df‚Äô and calculate the transformed time index ‚Äòt‚Äô just as we did when fitting the Fourier series. This time index is used as the input to our Fourier function.\nNext, we use our Fourier function, along with the coefficients for the current year, to calculate the estimated temperature for each day of that year. These estimated temperatures are then added as a new column in the ‚Äòyear_df‚Äô DataFrame, with the column name being the current year.\nWe repeat this process for all years in our dataset, merging the temperature estimates for each year into the ‚Äòtemp_estimates‚Äô DataFrame.\nFinally, we save these temperature estimates to a new CSV file. The end result of this process is a DataFrame that provides a day-by-day estimate of the temperature for each year based on the Fourier regression model. These estimates serve as the basis for our subsequent analysis and allow us to visualize and quantify the cyclical patterns present in the temperature data.\nAbove code will produce output previews like below.\n\nTable 5. Temperature estimates\n4.1.6 Autocorrelated error calculation\nThis code calculates the autocorrelated error between the observed temperature and the estimated temperature from the Fourier model for each year, and stores the errors in a dataframe.\nFirstly, we load the wide-format data and the estimated temperature data. Then, we specify an autocorrelation factor (r), which is a parameter that describes the correlation between values of the error at different points in time.\nWe loop over each year from 1986 to 2022, and for each year we:\nCalculate the difference between the observed and estimated temperatures to get the error.\nGenerate a sequence of random numbers from a normal distribution, called white noise.\nCompute the autocorrelated error. The error for the first day is simply the white noise, and for each subsequent day, the error is the autocorrelation factor multiplied by the previous day‚Äôs error, plus the white noise for that day.\nFinally, we create a DataFrame from the dictionary of autocorrelated errors, and save it to a CSV file.\nThis autocorrelated error represents the error in our model‚Äôs estimate that cannot be explained by the model itself, but rather depends on previous errors. This could be due to factors that we did not include in our model, such as atmospheric conditions or climate change. By including this autocorrelation in our analysis, we can better understand and model these unexplained variations in temperature.\nAbove code will produce output previews like below.\n\nTable 6. Autocorrelated error\n4.1.7 Final estimates adjusted temperature\nWe integrated the autocorrelated error into our temperature estimates to generate a more refined model of temperature estimation. With this data in place, we transformed our wide-format data into a long-format data frame. Each row of this data frame represented a specific day from a specific year, containing information on the date, observed temperature, estimated temperature, error, and the adjusted estimated temperature (estimate + error).\nThis transformed format provided us with a holistic and granular view of our data, suitable for subsequent detailed analyses. Once the transformation was complete, the data was saved into a CSV file, enabling easy access for further research or data visualization tasks.\nAbove code will produce output previews like below.\n\nTable 7. Final adjusted temperature\n\n\n4.2 Jupyter Notebook\nThe provided code shows the process on how we can use monthly temperature data to generate daily time series temperature using Fourier regression models. Here‚Äôs a summary of what each part of the code does: https://gist.github.com/bennyistanto/a9e6045a78b230dbd5c443a0e0e4fa41\n\n\n\n5 Results\nThe graphical visualization of the estimated daily temperature against the observed temperature provided a robust means of evaluating the efficacy of the Fourier model across the study period (1986-2021) at the Bogor Climatological Station. The estimated temperature, generated from the Fourier model, was superimposed onto a scatter plot of the observed temperatures. The latter were smoothed using the nonparametric LOESS technique to discern major trends within the data.\nEach subplot delineated a separate year‚Äôs worth of data, allowing for an insightful year-to-year examination of the model‚Äôs performance. The observed temperatures were presented as scatter points, with the LOESS smoothing line capturing the general pattern of the temperature across different days of the year.\nThe comparison between the observed temperature trends and the estimates from the Fourier model revealed a substantial degree of congruence, indicating the model‚Äôs reliability in predicting daily temperature patterns. The Fourier model demonstrated a commendable ability to generate daily temperature estimates from monthly data. This affirms the model‚Äôs utility in climatological studies, particularly when granular daily data are not readily available.\nAbove code will produce a chart visualization below\n\nPicture 1. Observed temperature vs Estimated temperature, year-by-year\nIn this exercise, we implemented a method to compute autocorrelated errors between observed and estimated temperatures from 1986 to 2022. We began by loading our dataset, after which we defined an autocorrelation factor, a parameter that reveals the correlation between different points in time within the error series.\nEach year‚Äôs temperature discrepancy was calculated and white noise, a random sequence derived from a normal distribution, was added. We then introduced the autocorrelation factor into the errors, with the first day‚Äôs error being only the white noise. The error for subsequent days factored in both the white noise and a portion of the previous day‚Äôs error, weighted by the autocorrelation factor.\nPost computation, we organized the autocorrelated errors into a dictionary, subsequently transforming it into a DataFrame for further analysis. This dataset of autocorrelated errors presents the unexplained variance within our model, potentially stemming from unaccounted factors such as climate changes or certain atmospheric conditions.\nFinally, we integrated these autocorrelated errors with our estimated temperatures, resulting in an adjusted and more refined temperature prediction. This revised dataset was then visualized in a time series plot, allowing for a comparative analysis between observed and adjusted estimated temperatures. The plot revealed a clear upward trend in the temperature at the Bogor Climatological Station from 1986 to 2021. Notably, the application of autocorrelated errors offered an excellent fit to the observed temperatures, thus affirming the effectiveness of our model.\nAbove code will produce a chart visualization below\n\nPicture 2. Observed temperature vs Estimated temperature with error\nThe Fourier model‚Äôs estimates were further scrutinized by integrating autocorrelated errors into the calculations. This facilitated the generation of a modified temperature prediction that comprised the original estimate and the error. Upon examination, the plots vividly displayed a comprehensive juxtaposition of this adjusted forecast against the observed data for each year from 1986 to 2021.\nNotably, the error-adjusted estimates, visualized through LOESS-smoothed lines, revealed minor disparities compared to the original model predictions. These charts underscored the pertinence of accommodating inherent model errors and substantiated the robustness of the Fourier model‚Äôs initial estimates. The insights derived from this comparison can guide further refinements to the model for superior accuracy in future temperature estimates.\nAbove code will produce a chart visualization below\n\nPicture 3. Observed temperature vs Estimated temperature with error, year-by-year\n\n\n6 Conclusion\nThe application of the Fourier regression model for generating daily time series data from monthly temperature observations has demonstrated considerable efficacy in climatological studies. This modeling approach provides a mathematically rigorous way to interpolate intra-monthly fluctuations, leveraging periodicity inherent in annual temperature patterns. The model is thus capable of filling data gaps and offering granular insights into day-to-day temperature variations, a granularity that monthly data alone cannot provide.\nNotably, the model‚Äôs provision for the autocorrelation of errors adds an additional layer of realism to the estimations, acknowledging the dependence of errors on preceding values. This factor makes the model responsive to the serial correlation often seen in climatic data, enhancing its predictive capabilities.\nIn conclusion, Fourier regression modeling serves as an invaluable tool for climatologists, offering an effective means of generating daily time series data from sparse or aggregated observations. Through its utilization, it is possible to acquire more detailed insights into temperature dynamics, paving the way for refined climate studies, policy formulation, and mitigation strategies against climatic anomalies. The model‚Äôs robustness, flexibility, and accommodating nature towards error correlation further enhance its applicability, making it a staple in the data-driven examination of climate patterns.\n\n\n7 References\nEpstein, E.S. 1991. On Obtaining daily climatological values from monthly means. J. Climate 4:465-368. https://doi.org/10.1175/1520-0442(1991)004%3C0365:OODCVF%3E2.0.CO;2\nBoer, R., Notodipuro, K.A., Las, I. 1999. Prediction of Daily Rainfall Characteristics from Monthly Climate Indices. RUT-IV report. National Research Council, Indonesia.\nCasta√±eda-Miranda, A., Icaza-Herrera, M. de, & Casta√±o, V. M. (2019). Meteorological Temperature and Humidity Prediction from Fourier-Statistical Analysis of Hourly Data. Advances in Meteorology, 2019, 1‚Äì13. https://doi.org/10.1155/2019/4164097\nHern√°ndez-Bedolla, J.; Solera, A.; Paredes-Arquiola, J.; Sanchez-Quispe, S.T.; Dom√≠nguez-S√°nchez, C. A Continuous Multisite Multivariate Generator for Daily Temperature Conditioned by Precipitation Occurrence. Water 2022, 14, 3494. https://doi.org/10.3390/w14213494\nMcCaskill, M.R. 1990. An efficient method for generation of full climatological records from daily rainfall. Australian Journal of Agricultural Research 41, 595-602. https://doi.org/10.1071/AR9900595\nParra-Plazas, J., Gaona-Garcia, P. & Plazas-Nossa, L. Time series outlier removal and imputing methods based on Colombian weather stations data. Environ Sci Pollut Res 30, 72319‚Äì72335 (2023). https://doi.org/10.1007/s11356-023-27176-x\nSrikanthan, R., & McMahon, T. A. (2001). Stochastic generation of annual, monthly and daily climate data: A review. Hydrology and Earth System Sciences Discussions, 5(4), 653-670. https://doi.org/10.5194/hess-5-653-2001\nStern, R. D., & Coe, R. (1984). A model fitting analysis of daily rainfall data. Journal of the Royal Statistical Society. Series A (General), 147(1), 1-34. https://doi.org/10.2307/2981736\nWannacott, T.H. abd R.J. Wannacott. 1987. Regression: A second course in statistics. Robert E. Krieger Publishing, Co.¬†Florida.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20100420-west-sumatra-earthquake-response-2009.html",
    "href": "blog/20100420-west-sumatra-earthquake-response-2009.html",
    "title": "West Sumatra Earthquake Response 2009",
    "section": "",
    "text": "Almost 5-months I have been working for UNOCHA as GIS Officer to support earthquake response that happen in West Sumatra, 30 Sep 2009. I was bored in the first two-months, because UNOCHA‚Äôs works mostly on coordination and I feel my excellent skills on GIS not much used.\n\nOne of example work, I did an assessment to IDP camps, then update the map. Easy peasy and nothing special.\n\nThen I realized, coordinating lots of humanitarian actors are challenging, especially when you asked them to provide data and update on their works. Some are voluntary and proactive sending the data, but most of them are lazy. üòÇ\nI have been requested by Head of UNOCHA in Jakarta to produce an ATLAS related to the response, similar with what UNOCHA did for Yogyakarta Earthquake in 2006. Challenging work (GIS+Design+Visualization+Coordination) has started, with team - I try to approach the cluster lead and member to get update about their work and communicate about our plan to create an ATLAS, so far the progress is quite good.\nHere‚Äôs some of the draft.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20130402-mbtiles-map.html",
    "href": "blog/20130402-mbtiles-map.html",
    "title": "MBTiles map",
    "section": "",
    "text": "After few months playing around with TileMill and TileStream, I have created few MBTiles map using data from my previous and current project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20110210-back-to-aceh-again.html",
    "href": "blog/20110210-back-to-aceh-again.html",
    "title": "Back to Aceh (again)",
    "section": "",
    "text": "Starting from 1 February 2011, I am back to Aceh again. I got GIS consultant position at the International Labour Organization (ILO) to support a project on ‚ÄúCreating Jobs: Capacity Building for Local Resources-Based Road Works in Selected District in NAD and Nias‚Äù.\nThis project is to support the Aceh Government in improving the livelihoods of the Acehnese people by enhancing the capacity of local governments to generate maximum employment opportunities through investments in rural infrastructure, particularly the road sector.\nThe objective of the project is to provide technical assistance (TA) to address the issues of asset maintenance and mainstreaming Local Resource-based (LRB) approaches. The implementation of this proposed exit strategy is limited to the Aceh cluster of the project. One of the key activities of TA is in the development and implementation of a Geographic Information System (GIS) as a key supporting tool in decision-making and planning of investments in road infrastructure.\nI am based at Dinas Bina Marga and Cipta Karya office in Sigli, Pidie district and frequently travelling to Bireun.\n \n\n\n\n Back to top"
  },
  {
    "objectID": "blog/20060720-how-to-calculate-the-fire-weather-index-in-indonesia.html",
    "href": "blog/20060720-how-to-calculate-the-fire-weather-index-in-indonesia.html",
    "title": "How to calculate the Fire Weather Index in Indonesia",
    "section": "",
    "text": "FWI Excel program add-in is a quick and easy way to calculate the fire weather index into a table in Microsoft Excel, hereinafter known as XLFWI add-ins. XLFWI add-in consists of functions worksheets for six index of fire weather index (FFMC, DMC, DC, BUI, ISI and FWI). Besides the function of this worksheet can be applied directly into the weather data that is stored in Microsoft Excel.\nCalculation system is designed to easily and quickly used by the user, or can also be modified in accordance with the application. XLFWI add-ins can be used for operational calculations FWI index on a single weather station, as FWI and training systems can be combined with other manufacturing excel display, such as charts and basic data analysis.\nDownload FWI Excel add-in here.\nIn addition to the Excel add-ins, as part of my final research project I developed a program that can run directly to calculate the FWI. The program is written in BASIC language, and some components have been adjusted for the FWI calculations in Indonesia.\nDownload FWI Calculator here (32-bit Windows program)\n[caption id=‚Äú‚Äù align=‚Äúalignnone‚Äù width=‚Äú2308‚Äù] FWI Kalkulator [/caption]\n\n\n\n Back to top"
  },
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Works",
    "section": "",
    "text": "Explore my portfolio of work in climate science, GIS, and data visualization."
  },
  {
    "objectID": "works.html#overview",
    "href": "works.html#overview",
    "title": "Works",
    "section": "",
    "text": "Explore my portfolio of work in climate science, GIS, and data visualization."
  },
  {
    "objectID": "works.html#featured-sections",
    "href": "works.html#featured-sections",
    "title": "Works",
    "section": "Featured Sections",
    "text": "Featured Sections\n\n\nExperiences\nProfessional experiences and career highlights in climate science and geospatial analysis.\n\n\nProjects\nKey projects demonstrating expertise in earth observation and data science.\n\n\nConsulting\nConsulting services and collaborative engagements.\n\n\nMaps & Infographics\nVisual storytelling through maps and data visualizations.\n\n\n\nExplore each section to learn more about my work."
  }
]