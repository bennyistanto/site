{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ff5924-ab60-4967-9fa0-7d1062f77d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Scanning hierarchy from C:\\Users\\benny\\Downloads\\Squarespace-Wordpress-Export-01-31-2026.xml...\n",
      "2. Extracting content...\n",
      "Saved: site\\index.qmd [page]\n",
      "Saved: site\\blog\\20031008-halo.qmd [post]\n",
      "Saved: site\\blog\\20031016-menduga-distribusi-ukuran-butir-hujan.qmd [post]\n",
      "Saved: site\\blog\\20031028-pengukuran-suhu.qmd [post]\n",
      "Saved: site\\blog\\20031120-makalah-dasar-agronomi.qmd [post]\n",
      "Saved: site\\blog\\20031125-mekanika-gerak-dalam-dua-dimensi.qmd [post]\n",
      "Saved: site\\blog\\20031210-anak-umptn-memang-beda.qmd [post]\n",
      "Saved: site\\blog\\20040103-menghitung-radiasi-matahari.qmd [post]\n",
      "Saved: site\\blog\\20040427-model-pendugaan-biomassa-tanaman-padi.qmd [post]\n",
      "Saved: site\\blog\\20040522-pendugaan-deret-hari-kering.qmd [post]\n",
      "Saved: site\\blog\\20040708-model-biomassa-dan-neraca-air.qmd [post]\n",
      "Saved: site\\blog\\20050108-satelit-geostasioner.qmd [post]\n",
      "Saved: site\\blog\\20050327-termodinamika-dalam-pertanian-rumahkaca.qmd [post]\n",
      "Saved: site\\blog\\20050610-magang-di-tisda-bppt.qmd [post]\n",
      "Saved: site\\blog\\20060310-sarjana-meteorologi.qmd [post]\n",
      "Saved: site\\blog\\20060623-multi-person-decision-making.qmd [post]\n",
      "Saved: site\\blog\\20060720-how-to-calculate-the-fire-weather-index-in-indonesia.qmd [post]\n",
      "Saved: site\\blog\\20060812-numerical-random-search.qmd [post]\n",
      "Saved: site\\blog\\20061109-sistem-peringkat-bahaya-kebakaran.qmd [post]\n",
      "Saved: site\\blog\\20061221-jacub-rais-award.qmd [post]\n",
      "Saved: site\\blog\\20070210-automatic-coordinate-register-system.qmd [post]\n",
      "Saved: site\\blog\\20071215-gis-for-immunization-activities.qmd [post]\n",
      "Saved: site\\blog\\20081109-model-simulasi-penyakit-busuk-daun-tanaman-kentang.qmd [post]\n",
      "Saved: site\\blog\\20081130-paddys-growth-and-development-model-and-economic-value-of-farming-system.qmd [post]\n",
      "Saved: site\\blog\\20081225-potential-soil-loss.qmd [post]\n",
      "Saved: site\\blog\\20091005-landmark-survey-in-tls.qmd [post]\n",
      "Saved: site\\blog\\20091121-webmap-interface.qmd [post]\n",
      "Saved: site\\blog\\20100420-west-sumatra-earthquake-response-2009.qmd [post]\n",
      "Saved: site\\blog\\20101101-2010-mentawai-tsunami-and-earthquake.qmd [post]\n",
      "Saved: site\\blog\\20101215-using-google-to-disseminate-information-during-2010-tsunami-in-mentawai-islands.qmd [post]\n",
      "Saved: site\\blog\\20110210-back-to-aceh-again.qmd [post]\n",
      "Saved: site\\blog\\20110428-ilo-basic-operational-gis-for-road-assessment-part-1.qmd [post]\n",
      "Saved: site\\blog\\20110501-ilo-basic-operational-gis-for-road-assessment-part-2.qmd [post]\n",
      "Saved: site\\blog\\20110601-ilo-basic-operational-gis-for-road-assessment-part-3.qmd [post]\n",
      "Saved: site\\blog\\20110810-first-try-using-tilemill-and-tilestream.qmd [post]\n",
      "Saved: site\\blog\\20120304-banjir-tangse-di-akhir-februari-2012.qmd [post]\n",
      "Saved: site\\blog\\20120611-first-day-at-wfp.qmd [post]\n",
      "Saved: site\\blog\\20120612-menggunakan-fungsi-arcpy-melalui-python-non-arcgis.qmd [post]\n",
      "Saved: site\\blog\\20130402-mbtiles-map.qmd [post]\n",
      "Saved: site\\blog\\20130415-cloudless-satellite-image.qmd [post]\n",
      "Saved: site\\blog\\20130911-menginstall-gdalogr-for-python-di-windows.qmd [post]\n",
      "Saved: site\\blog\\20140915-kelas-inspirasi-bogor-2.qmd [post]\n",
      "Saved: site\\blog\\20141216-osm-use-case-market-accessibility.qmd [post]\n",
      "Saved: site\\blog\\20150801-2015-esri-user-conference.qmd [post]\n",
      "Saved: site\\blog\\20160402-so-fun-aerial-bold.qmd [post]\n",
      "Saved: site\\blog\\20160510-2016-gis-for-sustainable-world-conference.qmd [post]\n",
      "Saved: site\\blog\\20160810-how-long-will-i-live.qmd [post]\n",
      "Saved: site\\blog\\20160927-satellite-based-monitoring-of-growing-season.qmd [post]\n",
      "Saved: site\\blog\\20161205-standing-crowd-density-2-desember-2016.qmd [post]\n",
      "Saved: site\\blog\\20170516-list-of-free-satellite-based-products-and-geospatial-data-on-internet.qmd [post]\n",
      "Saved: site\\blog\\20170802-blending-satellite-precipitation-and-gauge-observations.qmd [post]\n",
      "Saved: site\\blog\\20171124-openstreetmap-geoweek-2017.qmd [post]\n",
      "Saved: site\\blog\\20171213-openstreetmap-mapathon-in-atambua.qmd [post]\n",
      "Saved: site\\blog\\20180206-vampire-won-2017-innovation-challenge.qmd [post]\n",
      "Saved: site\\blog\\20180402-climate-expertise-in-the-humanitarian-field.qmd [post]\n",
      "Saved: site\\blog\\20180410-telegram-group-for-gis-user-in-indonesia.qmd [post]\n",
      "Saved: site\\blog\\20180429-visiting-rohingya-refugees-in-coxs-bazaar.qmd [post]\n",
      "Saved: site\\blog\\20180606-food-accessibility-for-rohingya-refugees.qmd [post]\n",
      "Saved: site\\blog\\20180911-dssat-training-in-chiang-mai-2018.qmd [post]\n",
      "Saved: site\\blog\\20190126-extreme-winter-in-mongolia.qmd [post]\n",
      "Saved: site\\blog\\20190422-surface-buffer.qmd [post]\n",
      "Saved: site\\blog\\20190531-summer-in-mongolia.qmd [post]\n",
      "Saved: site\\blog\\20190708-pixel-wise-regression-between-rainfall-and-sea-surface-temperature.qmd [post]\n",
      "Saved: site\\blog\\20190727-technical-engagement-workshop-in-beijing.qmd [post]\n",
      "Saved: site\\blog\\20190914-innovation-in-humanitarian-crisis-meeting.qmd [post]\n",
      "Saved: site\\blog\\20191102-sf-innovation-bootcamp-2019.qmd [post]\n",
      "Saved: site\\blog\\20191108-global-gis-workshop-2019.qmd [post]\n",
      "Saved: site\\blog\\20191109-visit-to-dlr-and-wfp-innovation-accelerator.qmd [post]\n",
      "Saved: site\\blog\\20191225-dry-and-start-of-rainy-season-in-timor-leste-20192020.qmd [post]\n",
      "Saved: site\\blog\\20200102-jakarta-flood-2020.qmd [post]\n",
      "Saved: site\\blog\\20200206-paddy-harvesting-and-planting-area-2019.qmd [post]\n",
      "Saved: site\\blog\\20200309-72-hours-assessment-workshop-in-vanuatu.qmd [post]\n",
      "Saved: site\\blog\\20200311-covid-19-and-wfh.qmd [post]\n",
      "Saved: site\\blog\\20200409-historical-flood-occurrence.qmd [post]\n",
      "Saved: site\\blog\\20200423-vulnerable-groups-in-covid-19.qmd [post]\n",
      "Saved: site\\blog\\20200505-30-minutes-rainfall-and-landslide-research.qmd [post]\n",
      "Saved: site\\blog\\20200630-monthly-weather-data-from-era5-and-fldas.qmd [post]\n",
      "Saved: site\\blog\\20200706-calculate-spi-using-imerg-data.qmd [post]\n",
      "Saved: site\\blog\\20200710-calculate-spi-using-chirps-data.qmd [post]\n",
      "Saved: site\\blog\\20200721-number-of-dry-and-wet-spell.qmd [post]\n",
      "Saved: site\\blog\\20200829-kriging-and-idw-interpolation-in-gee.qmd [post]\n",
      "Saved: site\\blog\\20201008-la-nina-and-indonesia-context.qmd [post]\n",
      "Saved: site\\blog\\20201101-30-day-map-challenge.qmd [post]\n",
      "Saved: site\\blog\\20201109-a-reason-for-being.qmd [post]\n",
      "Saved: site\\blog\\20201210-geotiff-to-netcdf-file-with-time-dimension-enabled-and-cf-compliant.qmd [post]\n",
      "Saved: site\\blog\\20210119-setup-your-machine-to-be-gis-ready.qmd [post]\n",
      "Saved: site\\blog\\20210125-calculate-spi-using-monthly-rainfall-data-in-geotiff-format.qmd [post]\n",
      "Saved: site\\blog\\20210311-today-a-year-ago.qmd [post]\n",
      "Saved: site\\blog\\20210413-how-to-get-daily-rainfall-forecast-data-from-gfs-part1.qmd [post]\n",
      "Saved: site\\blog\\20210416-how-to-get-daily-rainfall-forecast-data-from-gfs-part-2.qmd [post]\n",
      "Saved: site\\blog\\20210525-hp-clj-pro-mfp-m181fw-supply-error-message.qmd [post]\n",
      "Saved: site\\blog\\20210611-ninth-year-and-the-2020-nobel-peace-prize-laureate.qmd [post]\n",
      "Saved: site\\blog\\20210618-visualize-daily-weather-forecast-from-gfs-using-google-earth-engine.qmd [post]\n",
      "Saved: site\\blog\\20210820-farewell-wfp.qmd [post]\n",
      "Saved: site\\blog\\20210824-qgis-auto-mapping.qmd [post]\n",
      "Saved: site\\blog\\20210830-upgrade-mbp-mc724-early-2011.qmd [post]\n",
      "Saved: site\\blog\\20210901-batch-download-30-minutes-rainfall-data-from-multiple-coordinates-and-dates.qmd [post]\n",
      "Saved: site\\blog\\20211114-terraclimate-data-and-standardized-precipitation-evapotranspiration-index-spei.qmd [post]\n",
      "Saved: site\\blog\\20211211-mean-annual-temperature-and-number-of-hot-days-in-a-year.qmd [post]\n",
      "Saved: site\\blog\\20220102-hows-2021-rainfall-in-indonesia.qmd [post]\n",
      "Saved: site\\blog\\20220217-modis-lst-explorer.qmd [post]\n",
      "Saved: site\\blog\\20220319-batch-task-execution-in-google-earth-engine-code-editor.qmd [post]\n",
      "Saved: site\\blog\\20220501-maximizing-thinkpad-t480.qmd [post]\n",
      "Saved: site\\blog\\20220803-heat-wave-duration-index.qmd [post]\n",
      "Saved: site\\blog\\20230128-history-of-climate-modeling.qmd [post]\n",
      "Saved: site\\blog\\20230131-descriptive-statistics-analysis-using-climate-data.qmd [post]\n",
      "Saved: site\\blog\\20230210-install-the-wrf-model-in-wsl2.qmd [post]\n",
      "Saved: site\\blog\\20230215-sbr-market-day-2023-when-creativity-and-demon-slayer-colors-led-to-success.qmd [post]\n",
      "Saved: site\\blog\\20230228-unit-hydrographs.qmd [post]\n",
      "Saved: site\\blog\\20230306-pycpt-config-and-notebook.qmd [post]\n",
      "Saved: site\\blog\\20230307-experimental-climatological-rainfall-zone.qmd [post]\n",
      "Saved: site\\blog\\20230315-pycpt-for-subseasonal-forecasts-in-indonesia.qmd [post]\n",
      "Saved: site\\blog\\20230329-monitoring-dry-and-wet-season-in-south-sudan.qmd [post]\n",
      "Saved: site\\blog\\20230401-visualising-the-wrf-output.qmd [post]\n",
      "Saved: site\\blog\\20230415-fuzzy-inference-system-fis-for-flood-risk-assessment.qmd [post]\n",
      "Saved: site\\blog\\20230525-impact-of-climate-change-in-cities.qmd [post]\n",
      "Saved: site\\blog\\20230526-second-order-markov-chain-model-to-generate-time-series-of-occurrence-and-rainfall.qmd [post]\n",
      "Saved: site\\blog\\20230614-sentinel-1-modified-radar-vegetation-index.qmd [post]\n",
      "Saved: site\\blog\\20230616-regression-analysis-with-dummy-variables.qmd [post]\n",
      "Saved: site\\blog\\20230702-fourier-regression-model-to-generate-monthly-to-daily-temperature-data.qmd [post]\n",
      "Saved: site\\blog\\20230811-spi-based-drought-characteristics.qmd [post]\n",
      "Saved: site\\blog\\20230822-parsing-bmkgs-daily-climate-data.qmd [post]\n",
      "Saved: site\\blog\\20230824-monthly-mosaic-of-modified-radar-vegetation-index.qmd [post]\n",
      "Saved: site\\blog\\20230825-a-certified-gisp.qmd [post]\n",
      "Saved: site\\blog\\20231015-hourly-humidity-data.qmd [post]\n",
      "Saved: site\\blog\\20240103-firmware-upgrade-on-thuraya-satsleeve-for-iphone.qmd [post]\n",
      "Saved: site\\blog\\20240124-drought-propagation.qmd [post]\n",
      "Saved: site\\blog\\20240125-maximizing-thinkpad-t14-gen-2-amd.qmd [post]\n",
      "Saved: site\\blog\\20240416-utilizing-cuda.qmd [post]\n",
      "Saved: site\\blog\\20240503-skip-pearson-fitting-on-climate-indices-python-package.qmd [post]\n",
      "Saved: site\\blog\\20240523-xkcd-style-for-lseqm-illustration.qmd [post]\n",
      "Saved: site\\blog\\20240525-xkcd-style-for-country-map.qmd [post]\n",
      "Saved: site\\blog\\20250228-word-clock.qmd [post]\n",
      "Saved: site\\about\\index.qmd [page]\n",
      "Saved: site\\experiences\\index.qmd [page]\n",
      "Saved: site\\consulting\\index.qmd [page]\n",
      "Saved: site\\maps-and-infographics\\index.qmd [page]\n",
      "Saved: site\\vitae\\index.qmd [page]\n",
      "\n",
      "Extraction complete! 138 files created.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_XML_FILE = r'C:\\Users\\benny\\Downloads\\Squarespace-Wordpress-Export-01-31-2026.xml'\n",
    "OUTPUT_DIR = 'site'\n",
    "\n",
    "# Namespaces found in your XML\n",
    "NAMESPACES = {\n",
    "    'wp': 'http://wordpress.org/export/1.2/',\n",
    "    'content': 'http://purl.org/rss/1.0/modules/content/',\n",
    "    'excerpt': 'http://wordpress.org/export/1.2/excerpt/',\n",
    "    'dc': 'http://purl.org/dc/elements/1.1/'\n",
    "}\n",
    "\n",
    "def get_text(element, tag, ns=None):\n",
    "    \"\"\"Helper to safely extract text from XML elements.\"\"\"\n",
    "    try:\n",
    "        found = element.find(tag, namespaces=ns) if ns else element.find(tag)\n",
    "        return found.text if found is not None else \"\"\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def clean_content(html_content):\n",
    "    \"\"\"\n",
    "    Cleans Squarespace HTML bloat and converts to Markdown.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Unwrap Squarespace specific layout divs\n",
    "    for div in soup.find_all(\"div\", class_=\"sqs-html-content\"):\n",
    "        div.unwrap()\n",
    "\n",
    "    # Convert to Markdown\n",
    "    markdown_text = md(str(soup), heading_style=\"ATX\")\n",
    "    \n",
    "    # Clean up excessive newlines\n",
    "    markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
    "    \n",
    "    return markdown_text.strip()\n",
    "\n",
    "def format_yaml(title, date, categories, tags, author, summary=\"\"):\n",
    "    \"\"\"Creates the Quarto YAML front matter.\"\"\"\n",
    "    yaml = \"---\\n\"\n",
    "    yaml += f'title: \"{title}\"\\n'\n",
    "    if author:\n",
    "        yaml += f'author: \"{author}\"\\n'\n",
    "    if date:\n",
    "        try:\n",
    "            dt_obj = datetime.strptime(date, '%a, %d %b %Y %H:%M:%S %z')\n",
    "            clean_date = dt_obj.strftime('%Y-%m-%d')\n",
    "            yaml += f'date: \"{clean_date}\"\\n'\n",
    "        except ValueError:\n",
    "            pass \n",
    "            \n",
    "    if categories or tags:\n",
    "        yaml += \"categories:\\n\"\n",
    "        all_tags = set(categories + tags)\n",
    "        for tag in all_tags:\n",
    "            yaml += f'  - \"{tag}\"\\n'\n",
    "    \n",
    "    if summary:\n",
    "        # Clean summary for YAML safety\n",
    "        summary = summary.replace('\"', \"'\").replace('\\n', ' ')\n",
    "        yaml += f'description: \"{summary}\"\\n'\n",
    "            \n",
    "    yaml += \"---\\n\\n\"\n",
    "    return yaml\n",
    "\n",
    "def get_parent_slug(items_dict, parent_id):\n",
    "    \"\"\"Recursively finds the parent slug path.\"\"\"\n",
    "    if parent_id == '0' or parent_id not in items_dict:\n",
    "        return \"\"\n",
    "    \n",
    "    parent_item = items_dict[parent_id]\n",
    "    parent_slug = parent_item['slug']\n",
    "    grandparent_id = parent_item['parent_id']\n",
    "    \n",
    "    grandparent_slug = get_parent_slug(items_dict, grandparent_id)\n",
    "    \n",
    "    if grandparent_slug:\n",
    "        return os.path.join(grandparent_slug, parent_slug)\n",
    "    return parent_slug\n",
    "\n",
    "def process_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    channel = root.find('channel')\n",
    "    \n",
    "    # Dictionary to store page hierarchy info: {post_id: {'slug': slug, 'parent_id': parent_id}}\n",
    "    pages_map = {}\n",
    "\n",
    "    print(f\"1. Scanning hierarchy from {file_path}...\")\n",
    "    \n",
    "    # First Pass: Build Hierarchy Map\n",
    "    for item in channel.findall('item'):\n",
    "        post_id = get_text(item, 'wp:post_id', NAMESPACES)\n",
    "        post_name = get_text(item, 'wp:post_name', NAMESPACES)\n",
    "        post_parent = get_text(item, 'wp:post_parent', NAMESPACES)\n",
    "        post_type = get_text(item, 'wp:post_type', NAMESPACES)\n",
    "        \n",
    "        if post_type != 'attachment':\n",
    "            pages_map[post_id] = {'slug': post_name, 'parent_id': post_parent}\n",
    "\n",
    "    print(\"2. Extracting content...\")\n",
    "\n",
    "    # Second Pass: Extract and Write\n",
    "    count = 0\n",
    "    for item in channel.findall('item'):\n",
    "        # Metadata\n",
    "        title = get_text(item, 'title')\n",
    "        post_name = get_text(item, 'wp:post_name', NAMESPACES)\n",
    "        post_type = get_text(item, 'wp:post_type', NAMESPACES)\n",
    "        post_id = get_text(item, 'wp:post_id', NAMESPACES)\n",
    "        parent_id = get_text(item, 'wp:post_parent', NAMESPACES)\n",
    "        status = get_text(item, 'wp:status', NAMESPACES)\n",
    "        pub_date = get_text(item, 'pubDate')\n",
    "        author = get_text(item, 'dc:creator', NAMESPACES)\n",
    "        excerpt = get_text(item, 'excerpt:encoded', NAMESPACES)\n",
    "        \n",
    "        # --- FILTERS ---\n",
    "        # Skip attachments, navigation menu items, and trashed/draft posts\n",
    "        if post_type in ['attachment', 'nav_menu_item'] or status != 'publish':\n",
    "            continue\n",
    "\n",
    "        # Extract Categories and Tags\n",
    "        categories = []\n",
    "        tags = []\n",
    "        for cat in item.findall('category'):\n",
    "            domain = cat.get('domain')\n",
    "            if domain == 'category':\n",
    "                categories.append(cat.text)\n",
    "            elif domain == 'post_tag':\n",
    "                tags.append(cat.text)\n",
    "\n",
    "        # Process Content\n",
    "        raw_content = get_text(item, 'content:encoded', NAMESPACES)\n",
    "        clean_body = clean_content(raw_content)\n",
    "        final_content = format_yaml(title, pub_date, categories, tags, author, excerpt) + clean_body\n",
    "\n",
    "        # Determine Output Path\n",
    "        target_dir = \"\"\n",
    "        filename = \"index.qmd\"\n",
    "\n",
    "        if post_type == 'post':\n",
    "            # BLOG POSTS -> site/blog/[slug].qmd\n",
    "            target_dir = os.path.join(OUTPUT_DIR, 'blog')\n",
    "            filename = f\"{post_name}.qmd\"\n",
    "        \n",
    "        else:\n",
    "            # PAGES, PROJECTS, CSR, etc.\n",
    "            # Calculate full directory path based on hierarchy\n",
    "            parent_path = get_parent_slug(pages_map, parent_id)\n",
    "            \n",
    "            if parent_path:\n",
    "                # e.g. site/works/projects/index.qmd\n",
    "                target_dir = os.path.join(OUTPUT_DIR, parent_path, post_name)\n",
    "            else:\n",
    "                # Top level pages\n",
    "                if post_name == 'home':\n",
    "                    target_dir = OUTPUT_DIR # site/index.qmd\n",
    "                elif post_name == 'blog':\n",
    "                    target_dir = os.path.join(OUTPUT_DIR, 'blog') # site/blog/index.qmd\n",
    "                else:\n",
    "                    target_dir = os.path.join(OUTPUT_DIR, post_name)\n",
    "\n",
    "        # Create Directory and Write\n",
    "        if target_dir and not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "            \n",
    "        full_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        with open(full_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_content)\n",
    "        \n",
    "        print(f\"Saved: {full_path} [{post_type}]\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"\\nExtraction complete! {count} files created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_xml(INPUT_XML_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5486134-0f67-4f1f-9a19-07a08816fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "INPUT_XML_FILE = r\"C:\\Users\\benny\\Downloads\\Squarespace-Wordpress-Export-01-31-2026.xml\"\n",
    "OUTPUT_DIR     = r\"C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\"   # or just \"site\" for relative\n",
    "BASE_SITE_URL  = \"https://benny.istan.to\"  # used to resolve /s/... or /... links\n",
    "\n",
    "PRESERVE_HIERARCHY = True     # nest pages by wp:post_parent\n",
    "OVERWRITE          = False    # overwrite existing .qmd and manifest files\n",
    "ALLOW_BLOG_PAGE_OVERWRITE = False  # if a PAGE slug == \"blog\", write to pages/blog/ instead of blog/index.qmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452ec60a-35e6-4a52-99ba-254a6be246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "NAMESPACES = {\n",
    "    \"wp\": \"http://wordpress.org/export/1.2/\",\n",
    "    \"content\": \"http://purl.org/rss/1.0/modules/content/\",\n",
    "    \"excerpt\": \"http://wordpress.org/export/1.2/excerpt/\",\n",
    "    \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "}\n",
    "\n",
    "RAW_URL_RE = re.compile(r\"https?://[^\\s\\\"\\'<>]+\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def get_text(element: ET.Element, tag: str, ns: dict | None = None) -> str:\n",
    "    try:\n",
    "        found = element.find(tag, namespaces=ns) if ns else element.find(tag)\n",
    "        return (found.text or \"\").strip() if found is not None else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_rfc822_to_iso(pub_date: str) -> str:\n",
    "    if not pub_date:\n",
    "        return \"\"\n",
    "    try:\n",
    "        dt_obj = datetime.strptime(pub_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "        return dt_obj.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def yaml_escape(s: str) -> str:\n",
    "    return (s or \"\").replace('\"', \"'\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "\n",
    "def format_yaml(title: str, date_rfc822: str, categories: list[str], tags: list[str],\n",
    "                author: str, summary: str = \"\") -> str:\n",
    "    title = title or \"Untitled\"\n",
    "    out = [\"---\", f'title: \"{yaml_escape(title)}\"']\n",
    "\n",
    "    if author:\n",
    "        out.append(f'author: \"{yaml_escape(author)}\"')\n",
    "\n",
    "    date_iso = parse_rfc822_to_iso(date_rfc822)\n",
    "    if date_iso:\n",
    "        out.append(f'date: \"{date_iso}\"')\n",
    "\n",
    "    merged = sorted({t for t in (categories or []) + (tags or []) if t})\n",
    "    if merged:\n",
    "        out.append(\"categories:\")\n",
    "        for t in merged:\n",
    "            out.append(f'  - \"{yaml_escape(t)}\"')\n",
    "\n",
    "    if summary:\n",
    "        out.append(f'description: \"{yaml_escape(summary)}\"')\n",
    "\n",
    "    out.append(\"---\\n\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def pick_best_from_srcset(srcset: str) -> str:\n",
    "    if not srcset:\n",
    "        return \"\"\n",
    "    best_url, best_w = \"\", -1\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        tokens = part.split()\n",
    "        url = tokens[0].strip()\n",
    "        w = 0\n",
    "        if len(tokens) > 1 and tokens[1].lower().endswith(\"w\"):\n",
    "            try:\n",
    "                w = int(tokens[1][:-1])\n",
    "            except Exception:\n",
    "                w = 0\n",
    "        if w >= best_w:\n",
    "            best_w = w\n",
    "            best_url = url\n",
    "    return best_url\n",
    "\n",
    "\n",
    "def normalize_url(u: str, base_site_url: str) -> str:\n",
    "    u = (u or \"\").strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = urljoin(base_site_url.rstrip(\"/\") + \"/\", u.lstrip(\"/\"))\n",
    "    return u\n",
    "\n",
    "\n",
    "def normalize_media_attributes(soup: BeautifulSoup, base_site_url: str) -> None:\n",
    "    # Images: promote real URL into src and drop srcset\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        cand = \"\"\n",
    "        for attr in (\"data-src\", \"data-image\", \"data-original\"):\n",
    "            if img.get(attr):\n",
    "                cand = img.get(attr)\n",
    "                break\n",
    "        if not cand and img.get(\"srcset\"):\n",
    "            cand = pick_best_from_srcset(img.get(\"srcset\"))\n",
    "        if not cand and img.get(\"src\"):\n",
    "            cand = img.get(\"src\")\n",
    "\n",
    "        cand = normalize_url(cand, base_site_url)\n",
    "        if cand:\n",
    "            img[\"src\"] = cand\n",
    "        if img.get(\"srcset\"):\n",
    "            del img[\"srcset\"]\n",
    "\n",
    "    # Links\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = normalize_url(a.get(\"href\", \"\"), base_site_url)\n",
    "        if href:\n",
    "            a[\"href\"] = href\n",
    "\n",
    "    # Other common media\n",
    "    for tag_name, attr in [(\"source\", \"src\"), (\"video\", \"src\"), (\"audio\", \"src\"), (\"iframe\", \"src\")]:\n",
    "        for t in soup.find_all(tag_name):\n",
    "            v = normalize_url(t.get(attr, \"\"), base_site_url)\n",
    "            if v:\n",
    "                t[attr] = v\n",
    "\n",
    "\n",
    "def clean_content_to_markdown(html_content: str, base_site_url: str) -> str:\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # Unwrap common Squarespace wrappers\n",
    "    for div in soup.find_all(\"div\", class_=\"sqs-html-content\"):\n",
    "        div.unwrap()\n",
    "\n",
    "    # Critical: normalize <img> / links BEFORE markdownify\n",
    "    normalize_media_attributes(soup, base_site_url)\n",
    "\n",
    "    markdown_text = md(str(soup), heading_style=\"ATX\", bullets=\"-\")\n",
    "    markdown_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_text).strip()\n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "def extract_asset_urls_from_raw_html(html_content: str, base_site_url: str) -> set[str]:\n",
    "    urls: set[str] = set()\n",
    "    if not html_content:\n",
    "        return urls\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    attrs = [\"src\", \"href\", \"data-src\", \"data-image\", \"data-original\", \"poster\"]\n",
    "    for tag in soup.find_all(True):\n",
    "        for a in attrs:\n",
    "            if tag.get(a):\n",
    "                urls.add(normalize_url(tag.get(a), base_site_url))\n",
    "\n",
    "    # srcset candidates\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img.get(\"srcset\"):\n",
    "            for part in img[\"srcset\"].split(\",\"):\n",
    "                part = part.strip()\n",
    "                if part:\n",
    "                    urls.add(normalize_url(part.split()[0].strip(), base_site_url))\n",
    "\n",
    "    # raw URL fallback\n",
    "    for u in RAW_URL_RE.findall(html_content):\n",
    "        urls.add(normalize_url(u, base_site_url))\n",
    "\n",
    "    return {u for u in urls if u}\n",
    "\n",
    "\n",
    "def write_blog_index(out_root: str, overwrite: bool = False) -> None:\n",
    "    blog_dir = os.path.join(out_root, \"blog\")\n",
    "    os.makedirs(blog_dir, exist_ok=True)\n",
    "    path = os.path.join(blog_dir, \"index.qmd\")\n",
    "    if os.path.exists(path) and not overwrite:\n",
    "        return\n",
    "    content = \"\"\"---\n",
    "title: \"Blog\"\n",
    "listing:\n",
    "  contents: .\n",
    "  sort: \"date desc\"\n",
    "  type: default\n",
    "  categories: true\n",
    "  page-size: 20\n",
    "  exclude: \"index.qmd\"\n",
    "---\n",
    "\n",
    "Welcome to the blog.\n",
    "\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def build_pages_map(channel: ET.Element) -> dict[str, dict]:\n",
    "    pages_map: dict[str, dict] = {}\n",
    "    for item in channel.findall(\"item\"):\n",
    "        post_type = get_text(item, \"wp:post_type\", NAMESPACES)\n",
    "        if post_type == \"attachment\":\n",
    "            continue\n",
    "        post_id = get_text(item, \"wp:post_id\", NAMESPACES)\n",
    "        slug = get_text(item, \"wp:post_name\", NAMESPACES)\n",
    "        parent_id = get_text(item, \"wp:post_parent\", NAMESPACES)\n",
    "        if post_id:\n",
    "            pages_map[post_id] = {\"slug\": slug, \"parent_id\": parent_id}\n",
    "    return pages_map\n",
    "\n",
    "\n",
    "def get_parent_slug_path(pages_map: dict[str, dict], parent_id: str) -> str:\n",
    "    if not parent_id or parent_id == \"0\" or parent_id not in pages_map:\n",
    "        return \"\"\n",
    "    parent = pages_map[parent_id]\n",
    "    grand = get_parent_slug_path(pages_map, parent[\"parent_id\"])\n",
    "    return os.path.join(grand, parent[\"slug\"]) if grand else parent[\"slug\"]\n",
    "\n",
    "\n",
    "def convert_wxr_to_quarto(\n",
    "    input_xml: str,\n",
    "    out_root: str,\n",
    "    base_site_url: str,\n",
    "    preserve_hierarchy: bool = True,\n",
    "    overwrite: bool = False,\n",
    "    allow_blog_page_overwrite: bool = False\n",
    "):\n",
    "    if not os.path.exists(input_xml):\n",
    "        raise FileNotFoundError(f\"XML not found: {input_xml}\")\n",
    "\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "    manifest_txt = os.path.join(out_root, \"_asset_manifest.txt\")\n",
    "    manifest_csv = os.path.join(out_root, \"_asset_manifest.csv\")\n",
    "\n",
    "    if overwrite:\n",
    "        for p in (manifest_txt, manifest_csv):\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    write_blog_index(out_root, overwrite=overwrite)\n",
    "\n",
    "    tree = ET.parse(input_xml)\n",
    "    root = tree.getroot()\n",
    "    channel = root.find(\"channel\")\n",
    "    if channel is None:\n",
    "        raise ValueError(\"Invalid WXR: missing <channel>\")\n",
    "\n",
    "    pages_map = build_pages_map(channel)\n",
    "\n",
    "    all_assets: set[str] = set()\n",
    "    manifest_rows: dict[str, list[str]] = {}  # url -> row\n",
    "    count_written = 0\n",
    "\n",
    "    for item in channel.findall(\"item\"):\n",
    "        post_type = get_text(item, \"wp:post_type\", NAMESPACES)\n",
    "        status = get_text(item, \"wp:status\", NAMESPACES)\n",
    "\n",
    "        if status != \"publish\":\n",
    "            continue\n",
    "        if post_type in {\"attachment\", \"nav_menu_item\"}:\n",
    "            continue\n",
    "\n",
    "        title = get_text(item, \"title\")\n",
    "        post_name = get_text(item, \"wp:post_name\", NAMESPACES)\n",
    "        post_id = get_text(item, \"wp:post_id\", NAMESPACES)\n",
    "        parent_id = get_text(item, \"wp:post_parent\", NAMESPACES)\n",
    "        pub_date = get_text(item, \"pubDate\")\n",
    "        author = get_text(item, \"dc:creator\", NAMESPACES)\n",
    "        excerpt = get_text(item, \"excerpt:encoded\", NAMESPACES)\n",
    "\n",
    "        categories, tags = [], []\n",
    "        for cat in item.findall(\"category\"):\n",
    "            domain = (cat.get(\"domain\") or \"\").strip()\n",
    "            txt = (cat.text or \"\").strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            if domain == \"category\":\n",
    "                categories.append(txt)\n",
    "            elif domain == \"post_tag\":\n",
    "                tags.append(txt)\n",
    "\n",
    "        raw_html = get_text(item, \"content:encoded\", NAMESPACES)\n",
    "\n",
    "        # ---- Asset manifest extraction ----\n",
    "        asset_urls = extract_asset_urls_from_raw_html(raw_html, base_site_url)\n",
    "\n",
    "        attach_url = get_text(item, \"wp:attachment_url\", NAMESPACES)\n",
    "        if attach_url:\n",
    "            asset_urls.add(normalize_url(attach_url, base_site_url))\n",
    "\n",
    "        for u in asset_urls:\n",
    "            if u and u not in manifest_rows:\n",
    "                all_assets.add(u)\n",
    "                manifest_rows[u] = [u, post_type, post_name or \"\", post_id or \"\"]\n",
    "\n",
    "        # ---- HTML -> Markdown ----\n",
    "        body_md = clean_content_to_markdown(raw_html, base_site_url)\n",
    "        front = format_yaml(title, pub_date, categories, tags, author, excerpt)\n",
    "        final_content = front + body_md + \"\\n\"\n",
    "\n",
    "        # ---- Output path ----\n",
    "        filename = \"index.qmd\"\n",
    "\n",
    "        if post_type == \"post\":\n",
    "            target_dir = os.path.join(out_root, \"blog\")\n",
    "            filename = f\"{post_name}.qmd\" if post_name else \"untitled.qmd\"\n",
    "        else:\n",
    "            if post_name == \"home\":\n",
    "                target_dir = out_root\n",
    "            elif post_name == \"blog\":\n",
    "                target_dir = os.path.join(out_root, \"blog\") if allow_blog_page_overwrite else os.path.join(out_root, \"pages\", \"blog\")\n",
    "            else:\n",
    "                if preserve_hierarchy:\n",
    "                    parent_path = get_parent_slug_path(pages_map, parent_id)\n",
    "                    target_dir = os.path.join(out_root, parent_path, post_name) if parent_path else os.path.join(out_root, post_name)\n",
    "                else:\n",
    "                    target_dir = os.path.join(out_root, post_name)\n",
    "\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        full_path = os.path.join(target_dir, filename)\n",
    "\n",
    "        if (not overwrite) and os.path.exists(full_path):\n",
    "            base, ext = os.path.splitext(full_path)\n",
    "            i = 2\n",
    "            while os.path.exists(f\"{base}-{i}{ext}\"):\n",
    "                i += 1\n",
    "            full_path = f\"{base}-{i}{ext}\"\n",
    "\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "            f.write(final_content)\n",
    "\n",
    "        count_written += 1\n",
    "\n",
    "    # ---- Write manifest files ----\n",
    "    with open(manifest_txt, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        for u in sorted(all_assets):\n",
    "            f.write(u + \"\\n\")\n",
    "\n",
    "    with open(manifest_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"url\", \"post_type\", \"slug\", \"post_id\"])\n",
    "        for u in sorted(manifest_rows.keys()):\n",
    "            w.writerow(manifest_rows[u])\n",
    "\n",
    "    print(f\"Extraction complete! {count_written} QMD files created.\")\n",
    "    print(f\"Asset manifest: {manifest_txt} ({len(all_assets)} unique URLs)\")\n",
    "    return count_written, len(all_assets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7c33d1-06e1-4268-93b9-ccbd5f010f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete! 138 QMD files created.\n",
      "Asset manifest: C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\\_asset_manifest.txt (1969 unique URLs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138, 1969)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_qmd, count_assets = convert_wxr_to_quarto(\n",
    "    input_xml=INPUT_XML_FILE,\n",
    "    out_root=OUTPUT_DIR,\n",
    "    base_site_url=BASE_SITE_URL,\n",
    "    preserve_hierarchy=PRESERVE_HIERARCHY,\n",
    "    overwrite=OVERWRITE,\n",
    "    allow_blog_page_overwrite=ALLOW_BLOG_PAGE_OVERWRITE,\n",
    ")\n",
    "\n",
    "count_qmd, count_assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c56b0-b6a5-4fb7-9d42-50af0382ae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
