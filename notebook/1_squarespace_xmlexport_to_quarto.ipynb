{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5486134-0f67-4f1f-9a19-07a08816fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "INPUT_XML_FILE = r\"C:\\Users\\benny\\Downloads\\Squarespace-Wordpress-Export-01-31-2026.xml\"\n",
    "OUTPUT_DIR     = r\"C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\"   # or just \"site\" for relative\n",
    "BASE_SITE_URL  = \"https://benny.istan.to\"  # used to resolve /s/... or /... links\n",
    "\n",
    "PRESERVE_HIERARCHY = True     # nest pages by wp:post_parent\n",
    "OVERWRITE          = False    # overwrite existing .qmd and manifest files\n",
    "ALLOW_BLOG_PAGE_OVERWRITE = False  # if a PAGE slug == \"blog\", write to pages/blog/ instead of blog/index.qmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452ec60a-35e6-4a52-99ba-254a6be246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "NAMESPACES = {\n",
    "    \"wp\": \"http://wordpress.org/export/1.2/\",\n",
    "    \"content\": \"http://purl.org/rss/1.0/modules/content/\",\n",
    "    \"excerpt\": \"http://wordpress.org/export/1.2/excerpt/\",\n",
    "    \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "}\n",
    "\n",
    "RAW_URL_RE = re.compile(r\"https?://[^\\s\\\"\\'<>]+\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def get_text(element: ET.Element, tag: str, ns: dict | None = None) -> str:\n",
    "    try:\n",
    "        found = element.find(tag, namespaces=ns) if ns else element.find(tag)\n",
    "        return (found.text or \"\").strip() if found is not None else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_rfc822_to_iso(pub_date: str) -> str:\n",
    "    if not pub_date:\n",
    "        return \"\"\n",
    "    try:\n",
    "        dt_obj = datetime.strptime(pub_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "        return dt_obj.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def yaml_escape(s: str) -> str:\n",
    "    return (s or \"\").replace('\"', \"'\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "\n",
    "def format_yaml(title: str, date_rfc822: str, categories: list[str], tags: list[str],\n",
    "                author: str, summary: str = \"\") -> str:\n",
    "    title = title or \"Untitled\"\n",
    "    out = [\"---\", f'title: \"{yaml_escape(title)}\"']\n",
    "\n",
    "    if author:\n",
    "        out.append(f'author: \"{yaml_escape(author)}\"')\n",
    "\n",
    "    date_iso = parse_rfc822_to_iso(date_rfc822)\n",
    "    if date_iso:\n",
    "        out.append(f'date: \"{date_iso}\"')\n",
    "\n",
    "    merged = sorted({t for t in (categories or []) + (tags or []) if t})\n",
    "    if merged:\n",
    "        out.append(\"categories:\")\n",
    "        for t in merged:\n",
    "            out.append(f'  - \"{yaml_escape(t)}\"')\n",
    "\n",
    "    if summary:\n",
    "        out.append(f'description: \"{yaml_escape(summary)}\"')\n",
    "\n",
    "    out.append(\"---\\n\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def pick_best_from_srcset(srcset: str) -> str:\n",
    "    if not srcset:\n",
    "        return \"\"\n",
    "    best_url, best_w = \"\", -1\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        tokens = part.split()\n",
    "        url = tokens[0].strip()\n",
    "        w = 0\n",
    "        if len(tokens) > 1 and tokens[1].lower().endswith(\"w\"):\n",
    "            try:\n",
    "                w = int(tokens[1][:-1])\n",
    "            except Exception:\n",
    "                w = 0\n",
    "        if w >= best_w:\n",
    "            best_w = w\n",
    "            best_url = url\n",
    "    return best_url\n",
    "\n",
    "\n",
    "def normalize_url(u: str, base_site_url: str) -> str:\n",
    "    u = (u or \"\").strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = urljoin(base_site_url.rstrip(\"/\") + \"/\", u.lstrip(\"/\"))\n",
    "    return u\n",
    "\n",
    "\n",
    "def normalize_media_attributes(soup: BeautifulSoup, base_site_url: str) -> None:\n",
    "    # Images: promote real URL into src and drop srcset\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        cand = \"\"\n",
    "        for attr in (\"data-src\", \"data-image\", \"data-original\"):\n",
    "            if img.get(attr):\n",
    "                cand = img.get(attr)\n",
    "                break\n",
    "        if not cand and img.get(\"srcset\"):\n",
    "            cand = pick_best_from_srcset(img.get(\"srcset\"))\n",
    "        if not cand and img.get(\"src\"):\n",
    "            cand = img.get(\"src\")\n",
    "\n",
    "        cand = normalize_url(cand, base_site_url)\n",
    "        if cand:\n",
    "            img[\"src\"] = cand\n",
    "        if img.get(\"srcset\"):\n",
    "            del img[\"srcset\"]\n",
    "\n",
    "    # Links\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = normalize_url(a.get(\"href\", \"\"), base_site_url)\n",
    "        if href:\n",
    "            a[\"href\"] = href\n",
    "\n",
    "    # Other common media\n",
    "    for tag_name, attr in [(\"source\", \"src\"), (\"video\", \"src\"), (\"audio\", \"src\"), (\"iframe\", \"src\")]:\n",
    "        for t in soup.find_all(tag_name):\n",
    "            v = normalize_url(t.get(attr, \"\"), base_site_url)\n",
    "            if v:\n",
    "                t[attr] = v\n",
    "\n",
    "\n",
    "def clean_content_to_markdown(html_content: str, base_site_url: str) -> str:\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # Unwrap common Squarespace wrappers\n",
    "    for div in soup.find_all(\"div\", class_=\"sqs-html-content\"):\n",
    "        div.unwrap()\n",
    "\n",
    "    # Critical: normalize <img> / links BEFORE markdownify\n",
    "    normalize_media_attributes(soup, base_site_url)\n",
    "\n",
    "    markdown_text = md(str(soup), heading_style=\"ATX\", bullets=\"-\")\n",
    "    markdown_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_text).strip()\n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "def extract_asset_urls_from_raw_html(html_content: str, base_site_url: str) -> set[str]:\n",
    "    urls: set[str] = set()\n",
    "    if not html_content:\n",
    "        return urls\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    attrs = [\"src\", \"href\", \"data-src\", \"data-image\", \"data-original\", \"poster\"]\n",
    "    for tag in soup.find_all(True):\n",
    "        for a in attrs:\n",
    "            if tag.get(a):\n",
    "                urls.add(normalize_url(tag.get(a), base_site_url))\n",
    "\n",
    "    # srcset candidates\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img.get(\"srcset\"):\n",
    "            for part in img[\"srcset\"].split(\",\"):\n",
    "                part = part.strip()\n",
    "                if part:\n",
    "                    urls.add(normalize_url(part.split()[0].strip(), base_site_url))\n",
    "\n",
    "    # raw URL fallback\n",
    "    for u in RAW_URL_RE.findall(html_content):\n",
    "        urls.add(normalize_url(u, base_site_url))\n",
    "\n",
    "    return {u for u in urls if u}\n",
    "\n",
    "\n",
    "def write_blog_index(out_root: str, overwrite: bool = False) -> None:\n",
    "    blog_dir = os.path.join(out_root, \"blog\")\n",
    "    os.makedirs(blog_dir, exist_ok=True)\n",
    "    path = os.path.join(blog_dir, \"index.qmd\")\n",
    "    if os.path.exists(path) and not overwrite:\n",
    "        return\n",
    "    content = \"\"\"---\n",
    "title: \"Blog\"\n",
    "listing:\n",
    "  contents: .\n",
    "  sort: \"date desc\"\n",
    "  type: default\n",
    "  categories: true\n",
    "  page-size: 20\n",
    "  exclude: \"index.qmd\"\n",
    "---\n",
    "\n",
    "Welcome to the blog.\n",
    "\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def build_pages_map(channel: ET.Element) -> dict[str, dict]:\n",
    "    pages_map: dict[str, dict] = {}\n",
    "    for item in channel.findall(\"item\"):\n",
    "        post_type = get_text(item, \"wp:post_type\", NAMESPACES)\n",
    "        if post_type == \"attachment\":\n",
    "            continue\n",
    "        post_id = get_text(item, \"wp:post_id\", NAMESPACES)\n",
    "        slug = get_text(item, \"wp:post_name\", NAMESPACES)\n",
    "        parent_id = get_text(item, \"wp:post_parent\", NAMESPACES)\n",
    "        if post_id:\n",
    "            pages_map[post_id] = {\"slug\": slug, \"parent_id\": parent_id}\n",
    "    return pages_map\n",
    "\n",
    "\n",
    "def get_parent_slug_path(pages_map: dict[str, dict], parent_id: str) -> str:\n",
    "    if not parent_id or parent_id == \"0\" or parent_id not in pages_map:\n",
    "        return \"\"\n",
    "    parent = pages_map[parent_id]\n",
    "    grand = get_parent_slug_path(pages_map, parent[\"parent_id\"])\n",
    "    return os.path.join(grand, parent[\"slug\"]) if grand else parent[\"slug\"]\n",
    "\n",
    "\n",
    "def convert_wxr_to_quarto(\n",
    "    input_xml: str,\n",
    "    out_root: str,\n",
    "    base_site_url: str,\n",
    "    preserve_hierarchy: bool = True,\n",
    "    overwrite: bool = False,\n",
    "    allow_blog_page_overwrite: bool = False\n",
    "):\n",
    "    if not os.path.exists(input_xml):\n",
    "        raise FileNotFoundError(f\"XML not found: {input_xml}\")\n",
    "\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "    manifest_txt = os.path.join(out_root, \"_asset_manifest.txt\")\n",
    "    manifest_csv = os.path.join(out_root, \"_asset_manifest.csv\")\n",
    "\n",
    "    if overwrite:\n",
    "        for p in (manifest_txt, manifest_csv):\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    write_blog_index(out_root, overwrite=overwrite)\n",
    "\n",
    "    tree = ET.parse(input_xml)\n",
    "    root = tree.getroot()\n",
    "    channel = root.find(\"channel\")\n",
    "    if channel is None:\n",
    "        raise ValueError(\"Invalid WXR: missing <channel>\")\n",
    "\n",
    "    pages_map = build_pages_map(channel)\n",
    "\n",
    "    all_assets: set[str] = set()\n",
    "    manifest_rows: dict[str, list[str]] = {}  # url -> row\n",
    "    count_written = 0\n",
    "\n",
    "    for item in channel.findall(\"item\"):\n",
    "        post_type = get_text(item, \"wp:post_type\", NAMESPACES)\n",
    "        status = get_text(item, \"wp:status\", NAMESPACES)\n",
    "\n",
    "        if status != \"publish\":\n",
    "            continue\n",
    "        if post_type in {\"attachment\", \"nav_menu_item\"}:\n",
    "            continue\n",
    "\n",
    "        title = get_text(item, \"title\")\n",
    "        post_name = get_text(item, \"wp:post_name\", NAMESPACES)\n",
    "        post_id = get_text(item, \"wp:post_id\", NAMESPACES)\n",
    "        parent_id = get_text(item, \"wp:post_parent\", NAMESPACES)\n",
    "        pub_date = get_text(item, \"pubDate\")\n",
    "        author = get_text(item, \"dc:creator\", NAMESPACES)\n",
    "        excerpt = get_text(item, \"excerpt:encoded\", NAMESPACES)\n",
    "\n",
    "        categories, tags = [], []\n",
    "        for cat in item.findall(\"category\"):\n",
    "            domain = (cat.get(\"domain\") or \"\").strip()\n",
    "            txt = (cat.text or \"\").strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            if domain == \"category\":\n",
    "                categories.append(txt)\n",
    "            elif domain == \"post_tag\":\n",
    "                tags.append(txt)\n",
    "\n",
    "        raw_html = get_text(item, \"content:encoded\", NAMESPACES)\n",
    "\n",
    "        # ---- Asset manifest extraction ----\n",
    "        asset_urls = extract_asset_urls_from_raw_html(raw_html, base_site_url)\n",
    "\n",
    "        attach_url = get_text(item, \"wp:attachment_url\", NAMESPACES)\n",
    "        if attach_url:\n",
    "            asset_urls.add(normalize_url(attach_url, base_site_url))\n",
    "\n",
    "        for u in asset_urls:\n",
    "            if u and u not in manifest_rows:\n",
    "                all_assets.add(u)\n",
    "                manifest_rows[u] = [u, post_type, post_name or \"\", post_id or \"\"]\n",
    "\n",
    "        # ---- HTML -> Markdown ----\n",
    "        body_md = clean_content_to_markdown(raw_html, base_site_url)\n",
    "        front = format_yaml(title, pub_date, categories, tags, author, excerpt)\n",
    "        final_content = front + body_md + \"\\n\"\n",
    "\n",
    "        # ---- Output path ----\n",
    "        filename = \"index.qmd\"\n",
    "\n",
    "        if post_type == \"post\":\n",
    "            target_dir = os.path.join(out_root, \"blog\")\n",
    "            filename = f\"{post_name}.qmd\" if post_name else \"untitled.qmd\"\n",
    "        else:\n",
    "            if post_name == \"home\":\n",
    "                target_dir = out_root\n",
    "            elif post_name == \"blog\":\n",
    "                target_dir = os.path.join(out_root, \"blog\") if allow_blog_page_overwrite else os.path.join(out_root, \"pages\", \"blog\")\n",
    "            else:\n",
    "                if preserve_hierarchy:\n",
    "                    parent_path = get_parent_slug_path(pages_map, parent_id)\n",
    "                    target_dir = os.path.join(out_root, parent_path, post_name) if parent_path else os.path.join(out_root, post_name)\n",
    "                else:\n",
    "                    target_dir = os.path.join(out_root, post_name)\n",
    "\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        full_path = os.path.join(target_dir, filename)\n",
    "\n",
    "        if (not overwrite) and os.path.exists(full_path):\n",
    "            base, ext = os.path.splitext(full_path)\n",
    "            i = 2\n",
    "            while os.path.exists(f\"{base}-{i}{ext}\"):\n",
    "                i += 1\n",
    "            full_path = f\"{base}-{i}{ext}\"\n",
    "\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "            f.write(final_content)\n",
    "\n",
    "        count_written += 1\n",
    "\n",
    "    # ---- Write manifest files ----\n",
    "    with open(manifest_txt, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        for u in sorted(all_assets):\n",
    "            f.write(u + \"\\n\")\n",
    "\n",
    "    with open(manifest_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"url\", \"post_type\", \"slug\", \"post_id\"])\n",
    "        for u in sorted(manifest_rows.keys()):\n",
    "            w.writerow(manifest_rows[u])\n",
    "\n",
    "    print(f\"Extraction complete! {count_written} QMD files created.\")\n",
    "    print(f\"Asset manifest: {manifest_txt} ({len(all_assets)} unique URLs)\")\n",
    "    return count_written, len(all_assets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7c33d1-06e1-4268-93b9-ccbd5f010f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete! 138 QMD files created.\n",
      "Asset manifest: C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\\_asset_manifest.txt (1969 unique URLs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138, 1969)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_qmd, count_assets = convert_wxr_to_quarto(\n",
    "    input_xml=INPUT_XML_FILE,\n",
    "    out_root=OUTPUT_DIR,\n",
    "    base_site_url=BASE_SITE_URL,\n",
    "    preserve_hierarchy=PRESERVE_HIERARCHY,\n",
    "    overwrite=OVERWRITE,\n",
    "    allow_blog_page_overwrite=ALLOW_BLOG_PAGE_OVERWRITE,\n",
    ")\n",
    "\n",
    "count_qmd, count_assets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c56b0-b6a5-4fb7-9d42-50af0382ae7d",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
