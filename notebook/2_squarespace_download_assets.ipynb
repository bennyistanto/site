{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f858737-2f7d-40d3-9c27-99eadd876a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE_DIR = r\"C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\"   # where your .qmd are\n",
    "ASSETS_DIR = os.path.join(SITE_DIR, \"assets\")\n",
    "MANIFEST_PATH = os.path.join(SITE_DIR, \"_asset_manifest.txt\")\n",
    "\n",
    "BASE_SITE_URL = \"https://benny.istan.to\"  # for /s/... links if any\n",
    "DELAY_SECONDS = 1.0\n",
    "OVERWRITE_EXISTING = False  # if True, redownload even if file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cc79d9-1dc7-4ae7-952e-a7dd29ea0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, time, html, mimetypes\n",
    "from urllib.parse import urlparse, unquote, parse_qs, urljoin\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Domains that usually host actual files\n",
    "ASSET_HOSTS = {\n",
    "    \"images.squarespace-cdn.com\",\n",
    "    \"static1.squarespace.com\",\n",
    "    \"images.squarespace.com\",\n",
    "    \"static.squarespace.com\",\n",
    "}\n",
    "\n",
    "# Your site domain CAN be kept, but only for /s/ file URLs\n",
    "SITE_HOST_ALLOW = {\"benny.istan.to\", \"bennyistanto.squarespace.com\"}\n",
    "\n",
    "DOWNLOADABLE_EXTS = {\n",
    "    \".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".svg\",\n",
    "    \".pdf\", \".zip\", \".mp4\", \".mov\", \".mp3\", \".wav\",\n",
    "    \".doc\", \".docx\", \".ppt\", \".pptx\", \".xls\", \".xlsx\",\n",
    "}\n",
    "\n",
    "FENCED_CODE_RE = re.compile(r\"```.*?```\", re.DOTALL)\n",
    "MD_LINK_RE = re.compile(r\"!\\[[^\\]]*\\]\\(([^)]+)\\)|\\[[^\\]]*\\]\\(([^)]+)\\)\")\n",
    "HTML_ATTR_RE = re.compile(r\"\"\"(?:src|href|data-src|data-image|data-original|poster)\\s*=\\s*[\"']([^\"']+)[\"']\"\"\", re.IGNORECASE)\n",
    "SRCSET_RE = re.compile(r\"\"\"srcset\\s*=\\s*[\"']([^\"']+)[\"']\"\"\", re.IGNORECASE)\n",
    "CSS_URL_RE = re.compile(r\"\"\"url\\(\\s*['\"]?([^'\")]+)['\"]?\\s*\\)\"\"\", re.IGNORECASE)\n",
    "RAW_URL_RE = re.compile(r\"\"\"https?://[^\\s\"'<>()]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "ASSET_ID_RE = re.compile(r\"(\\d{13}-[A-Z0-9]{8,})\")\n",
    "\n",
    "\n",
    "def create_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "    retry = Retry(\n",
    "        total=6,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "\n",
    "def strip_trailing_punct(u: str) -> str:\n",
    "    return u.rstrip(').,;:\\'\"!?]>')\n",
    "\n",
    "def normalize_url(u: str, base_site_url: str) -> str:\n",
    "    u = (u or \"\").strip().strip(\"<>\")\n",
    "    u = html.unescape(u)\n",
    "    u = strip_trailing_punct(u)\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = urljoin(base_site_url.rstrip(\"/\") + \"/\", u.lstrip(\"/\"))\n",
    "    return u\n",
    "\n",
    "\n",
    "def looks_like_downloadable_file(u: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if URL is likely a file asset (not a page).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        host = p.netloc.lower()\n",
    "        path = p.path or \"\"\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "        # 1) Asset CDN hosts: require a file extension OR content-type query\n",
    "        if host in ASSET_HOSTS:\n",
    "            if ext in DOWNLOADABLE_EXTS:\n",
    "                return True\n",
    "            q = parse_qs(p.query)\n",
    "            if \"content-type\" in q:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        # 2) Your own domain: only download if it's a Squarespace file link (/s/...) AND looks like a file\n",
    "        if host in SITE_HOST_ALLOW:\n",
    "            if path.startswith(\"/s/\") and (ext in DOWNLOADABLE_EXTS or ext):\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def asset_key(u: str) -> str:\n",
    "    p = urlparse(u)\n",
    "    return f\"{p.netloc.lower()}{p.path}\"\n",
    "\n",
    "\n",
    "def best_quality_score(u: str, width_hint: int = 0) -> int:\n",
    "    score = width_hint or 0\n",
    "    try:\n",
    "        q = parse_qs(urlparse(u).query)\n",
    "        fmt = (q.get(\"format\", [\"\"])[0] or \"\").lower()\n",
    "        m = re.search(r\"(\\d{3,5})w\", fmt)\n",
    "        if m:\n",
    "            score = max(score, int(m.group(1)))\n",
    "        if fmt in {\"original\", \"raw\"}:\n",
    "            score = max(score, 99999)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return score\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    name = unquote(name).replace(\" \", \"_\")\n",
    "    name = re.sub(r\"[^A-Za-z0-9._-]+\", \"\", name)\n",
    "    name = re.sub(r\"_{2,}\", \"_\", name).strip(\"._-\")\n",
    "    return name or \"asset\"\n",
    "\n",
    "\n",
    "def fix_extension(ext: str) -> str:\n",
    "    if not ext:\n",
    "        return \".jpg\"\n",
    "    ext = ext.lower()\n",
    "    if ext in [\".jpe\", \".jpeg\"]:\n",
    "        return \".jpg\"\n",
    "    return ext\n",
    "\n",
    "\n",
    "def choose_base_name(final_url: str) -> str:\n",
    "    p = urlparse(final_url)\n",
    "    m = ASSET_ID_RE.search(p.path or \"\")\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    base = os.path.basename(p.path) or \"asset\"\n",
    "    stem = os.path.splitext(base)[0] or \"asset\"\n",
    "    return safe_filename(stem)\n",
    "\n",
    "\n",
    "def extract_urls_from_text(text: str, base_site_url: str):\n",
    "    text = FENCED_CODE_RE.sub(\"\", text or \"\")\n",
    "    found = []\n",
    "\n",
    "    # Markdown links/images\n",
    "    for m in MD_LINK_RE.findall(text):\n",
    "        for raw in m:\n",
    "            if raw:\n",
    "                raw = raw.strip().strip('\"').strip(\"'\")\n",
    "                norm = normalize_url(raw, base_site_url)\n",
    "                found.append((raw, norm, 0))\n",
    "\n",
    "    # HTML attributes\n",
    "    for raw in HTML_ATTR_RE.findall(text):\n",
    "        raw = raw.strip()\n",
    "        norm = normalize_url(raw, base_site_url)\n",
    "        found.append((raw, norm, 0))\n",
    "\n",
    "    # srcset\n",
    "    for srcset in SRCSET_RE.findall(text):\n",
    "        parts = [p.strip() for p in srcset.split(\",\") if p.strip()]\n",
    "        for part in parts:\n",
    "            tokens = part.split()\n",
    "            raw = tokens[0].strip()\n",
    "            width_hint = 0\n",
    "            if len(tokens) > 1:\n",
    "                m = re.match(r\"(\\d{2,5})w\", tokens[1].lower())\n",
    "                if m:\n",
    "                    width_hint = int(m.group(1))\n",
    "            norm = normalize_url(raw, base_site_url)\n",
    "            found.append((raw, norm, width_hint))\n",
    "\n",
    "    # CSS url(...)\n",
    "    for raw in CSS_URL_RE.findall(text):\n",
    "        raw = raw.strip()\n",
    "        norm = normalize_url(raw, base_site_url)\n",
    "        found.append((raw, norm, 0))\n",
    "\n",
    "    # Raw URLs\n",
    "    for raw in RAW_URL_RE.findall(text):\n",
    "        raw = raw.strip()\n",
    "        norm = normalize_url(raw, base_site_url)\n",
    "        found.append((raw, norm, 0))\n",
    "\n",
    "    # Keep only downloadables\n",
    "    out = []\n",
    "    for raw, norm, w in found:\n",
    "        if norm and looks_like_downloadable_file(norm):\n",
    "            out.append((raw, norm, w))\n",
    "    return out\n",
    "\n",
    "\n",
    "def collect_qmd_files(site_dir: str, assets_dir: str):\n",
    "    qmds = []\n",
    "    for root, _, files in os.walk(site_dir):\n",
    "        # skip the assets dir itself\n",
    "        if os.path.abspath(root).startswith(os.path.abspath(assets_dir)):\n",
    "            continue\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".qmd\"):\n",
    "                qmds.append(os.path.join(root, fn))\n",
    "    qmds.sort()\n",
    "    return qmds\n",
    "\n",
    "\n",
    "def download_one(session, url, assets_path, delay_seconds: float, overwrite: bool):\n",
    "    try:\n",
    "        r = session.get(url, stream=True, allow_redirects=True, timeout=45)\n",
    "        if r.status_code != 200:\n",
    "            return False, None, r.url, f\"HTTP {r.status_code}\"\n",
    "\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").split(\";\")[0].strip().lower()\n",
    "        if ctype.startswith(\"text/html\"):\n",
    "            return False, None, r.url, \"Got HTML (page/blocked/redirected)\"\n",
    "\n",
    "        ext = os.path.splitext(urlparse(r.url).path)[1]\n",
    "        if not ext:\n",
    "            ext = mimetypes.guess_extension(ctype) or \"\"\n",
    "        ext = fix_extension(ext)\n",
    "\n",
    "        base = choose_base_name(r.url)\n",
    "        filename_asset = f\"{base}{ext}\"\n",
    "        save_path = os.path.join(assets_path, filename_asset)\n",
    "\n",
    "        if os.path.exists(save_path) and not overwrite:\n",
    "            return True, filename_asset, r.url, None\n",
    "\n",
    "        uniq = 1\n",
    "        final_name = filename_asset\n",
    "        while os.path.exists(save_path) and overwrite is False:\n",
    "            final_name = f\"{base}_{uniq}{ext}\"\n",
    "            save_path = os.path.join(assets_path, final_name)\n",
    "            uniq += 1\n",
    "\n",
    "        with open(save_path, \"wb\") as f_out:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 64):\n",
    "                if chunk:\n",
    "                    f_out.write(chunk)\n",
    "\n",
    "        time.sleep(delay_seconds)\n",
    "        return True, final_name, r.url, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, None, url, str(e)\n",
    "\n",
    "\n",
    "def run_download(site_dir, assets_dir, manifest_path, base_site_url, delay_seconds=1.0, overwrite=False):\n",
    "    print(f\"--- STARTING SCAN in: {site_dir} ---\")\n",
    "    os.makedirs(assets_dir, exist_ok=True)\n",
    "\n",
    "    session = create_session()\n",
    "    qmd_files = collect_qmd_files(site_dir, assets_dir)\n",
    "    print(f\"Found {len(qmd_files)} .qmd files.\")\n",
    "\n",
    "    # key -> (best_url, score)\n",
    "    best = {}\n",
    "\n",
    "    # QMD scan\n",
    "    qmd_count = 0\n",
    "    for fp in qmd_files:\n",
    "        with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        triples = extract_urls_from_text(text, base_site_url)\n",
    "        for _raw, norm, w in triples:\n",
    "            k = asset_key(norm)\n",
    "            score = best_quality_score(norm, w)\n",
    "            prev = best.get(k)\n",
    "            if prev is None or score > prev[1]:\n",
    "                best[k] = (norm, score)\n",
    "        qmd_count += len(triples)\n",
    "\n",
    "    print(f\"QMD scan: {len(best)} unique downloadable assets (from {qmd_count} URL hits).\")\n",
    "\n",
    "    # Manifest scan\n",
    "    if manifest_path and os.path.exists(manifest_path):\n",
    "        before = len(best)\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                u = normalize_url(line.strip(), base_site_url)\n",
    "                if u and looks_like_downloadable_file(u):\n",
    "                    k = asset_key(u)\n",
    "                    score = best_quality_score(u, 0)\n",
    "                    prev = best.get(k)\n",
    "                    if prev is None or score > prev[1]:\n",
    "                        best[k] = (u, score)\n",
    "        after = len(best)\n",
    "        print(f\"Manifest added: {before} -> {after} unique downloadable assets.\")\n",
    "    else:\n",
    "        print(\"Manifest not found / not used (this is why you only saw ~1013 previously).\")\n",
    "\n",
    "    keys = list(best.keys())\n",
    "    print(f\"Downloading {len(keys)} unique assets...\")\n",
    "\n",
    "    success_csv = os.path.join(assets_dir, \"_download_success.csv\")\n",
    "    failed_txt = os.path.join(assets_dir, \"_download_failed.txt\")\n",
    "    url_map = {}\n",
    "    success_rows = []\n",
    "    failed = []\n",
    "\n",
    "    for i, k in enumerate(keys, 1):\n",
    "        url = best[k][0]\n",
    "        ok, local_name, final_url, err = download_one(session, url, assets_dir, delay_seconds, overwrite)\n",
    "        if ok:\n",
    "            url_map[k] = local_name\n",
    "            success_rows.append([k, url, final_url, local_name])\n",
    "            if i <= 20 or i % 200 == 0:\n",
    "                print(f\"[{i}/{len(keys)}] OK  -> {local_name}\")\n",
    "        else:\n",
    "            failed.append(f\"{url}\\t{err}\")\n",
    "            if i <= 20 or i % 200 == 0:\n",
    "                print(f\"[{i}/{len(keys)}] FAIL -> {url} ({err})\")\n",
    "\n",
    "    with open(success_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"asset_key\", \"requested_url\", \"final_url\", \"local_filename\"])\n",
    "        w.writerows(success_rows)\n",
    "\n",
    "    if failed:\n",
    "        with open(failed_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "\n",
    "    # Rewrite QMD links\n",
    "    changed_files = 0\n",
    "    for fp in qmd_files:\n",
    "        with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        triples = extract_urls_from_text(text, base_site_url)\n",
    "        if not triples:\n",
    "            continue\n",
    "\n",
    "        qmd_dir = os.path.dirname(fp)\n",
    "        rel_assets = os.path.relpath(assets_dir, qmd_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        new_text = text\n",
    "        changed = False\n",
    "        for raw, norm, _w in triples:\n",
    "            k = asset_key(norm)\n",
    "            if k in url_map:\n",
    "                local_rel = f\"{rel_assets}/{url_map[k]}\".replace(\"\\\\\", \"/\")\n",
    "                if raw in new_text:\n",
    "                    new_text = new_text.replace(raw, local_rel)\n",
    "                    changed = True\n",
    "\n",
    "        if changed and new_text != text:\n",
    "            with open(fp, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "                f.write(new_text)\n",
    "            changed_files += 1\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"- Downloaded: {len(url_map)}\")\n",
    "    print(f\"- Failed: {len(failed)} (see {failed_txt})\" if failed else \"- Failed: 0\")\n",
    "    print(f\"- Updated QMD files: {changed_files}\")\n",
    "    print(f\"- Logs: {success_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12394013-1a43-401e-a213-2edc00a550fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING SCAN in: C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3 ---\n",
      "Found 139 .qmd files.\n",
      "QMD scan: 0 unique downloadable assets (from 0 URL hits).\n",
      "Manifest added: 0 -> 951 unique downloadable assets.\n",
      "Downloading 951 unique assets...\n",
      "[1/951] OK  -> CV_ISTANTO_Benny_20250516.pdf\n",
      "[2/951] OK  -> 15325333_10211141699402159_1841558926548787098_o_10211141699402159.jpg\n",
      "[3/951] OK  -> mmr_pheno_eos1_2022.png\n",
      "[4/951] OK  -> Screenshot2023-08-12141329.png\n",
      "[5/951] OK  -> ScreenShot2022-03-19at9.42.39PM.png\n",
      "[6/951] OK  -> ukr_pop_landscan_population_2021_wbg_A4L.png\n",
      "[7/951] OK  -> ScreenShot2022-10-18at5.00.48PM.png\n",
      "[8/951] OK  -> ScreenShot2022-10-18at4.36.45PM.png\n",
      "[9/951] OK  -> 1692902253633.jpg\n",
      "[10/951] OK  -> Screenshot2023-06-15075323.png\n",
      "[11/951] OK  -> Heatwave_CA_2016a.png\n",
      "[12/951] OK  -> IMG_2664.jpg\n",
      "[13/951] OK  -> idn_annualrain_2021.png\n",
      "[14/951] OK  -> viz4.png\n",
      "[15/951] OK  -> Screenshot2023-05-09224753.png\n",
      "[16/951] OK  -> 1589366320149-LLK82OODORI0BZMMZ0LA.png\n",
      "[17/951] OK  -> 1589366433048-6WBV4YK5RSOZ65D1FESO.png\n",
      "[18/951] OK  -> 1589372742224-GH97GF8XRZOZN88OHLG7.jpg\n",
      "[19/951] OK  -> 1589380041398-9C1XC1S200XLL9VAW4GS.jpg\n",
      "[20/951] OK  -> 1589444589243-B5OJAHGBBMUUHB6P9YQV.png\n",
      "[200/951] OK  -> 1594706735078-KW9KAXKQ3Z657U3N40Q2.jpg\n",
      "[400/951] OK  -> 1607533736617-EQFEJIOOHVTCNTS5M7IF.png\n",
      "[600/951] OK  -> 1632750134147-U3BCEGJRGJVVNK8HHXUX.png\n",
      "[800/951] OK  -> 1740332146657-KAE3E2ER5NZ8R4U3KE1U.png\n",
      "\n",
      "Done.\n",
      "- Downloaded: 951\n",
      "- Failed: 0\n",
      "- Updated QMD files: 0\n",
      "- Logs: C:\\Users\\benny\\OneDrive\\Documents\\Github\\site\\temp3\\assets\\_download_success.csv\n"
     ]
    }
   ],
   "source": [
    "run_download(\n",
    "    site_dir=SITE_DIR,\n",
    "    assets_dir=ASSETS_DIR,\n",
    "    manifest_path=MANIFEST_PATH,\n",
    "    base_site_url=BASE_SITE_URL,\n",
    "    delay_seconds=DELAY_SECONDS,\n",
    "    overwrite=OVERWRITE_EXISTING\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84ca09-3f28-4d69-84db-e11023277309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
